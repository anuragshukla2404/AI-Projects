{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The Little Book\\nof\\nDeep Learning\\nFranÃ§ois Fleuret\\n', metadata={'source': 'rag/deep learning.pdf', 'page': 0}),\n",
       " Document(page_content='FranÃ§ois Fleuret is a professor of computer sci-\\nence at the University of Geneva, Switzerland.\\nThe cover illustration is a schematic of the\\nNeocognitron by Fukushima [1980], a key an-\\ncestor of deep neural networks.\\nThis ebook is formatted to fit on a phone screen.', metadata={'source': 'rag/deep learning.pdf', 'page': 1}),\n",
       " Document(page_content='Contents\\nContents 5\\nList of figures 7\\nForeword 8\\nI Foundations 10\\n1 Machine Learning 11\\n1.1 Learning from data . . . . . . . 12\\n1.2 Basis function regression . . . . 14\\n1.3 Under and overfitting . . . . . . 16\\n1.4 Categories of models . . . . . . 18\\n2 Efficient Computation 20\\n2.1 GPUs, TPUs, and batches . . . . 21\\n2.2 Tensors . . . . . . . . . . . . . . 23\\n3 Training 25\\n3.1 Losses . . . . . . . . . . . . . . 26\\n3.2 Autoregressive models . . . . . 30\\n3.3 Gradient descent . . . . . . . . 35\\n3', metadata={'source': 'rag/deep learning.pdf', 'page': 2}),\n",
       " Document(page_content='3.4 Backpropagation . . . . . . . . 40\\n3.5 The value of depth . . . . . . . 45\\n3.6 Training protocols . . . . . . . 48\\n3.7 The benefits of scale . . . . . . 52\\nII Deep Models 57\\n4 Model Components 58\\n4.1 The notion of layer . . . . . . . 59\\n4.2 Linear layers . . . . . . . . . . . 61\\n4.3 Activation functions . . . . . . 71\\n4.4 Pooling . . . . . . . . . . . . . . 74\\n4.5 Dropout . . . . . . . . . . . . . 77\\n4.6 Normalizing layers . . . . . . . 80\\n4.7 Skip connections . . . . . . . . 84\\n4.8 Attention layers . . . . . . . . . 87\\n4.9 Token embedding . . . . . . . . 95\\n4.10 Positional encoding . . . . . . . 96\\n5 Architectures 98\\n5.1 Multi-Layer Perceptrons . . . . 99\\n5.2 Convolutional networks . . . . 101\\n5.3 Attention models . . . . . . . . 108\\nIII Applications 116\\n6 Prediction 117\\n6.1 Image denoising . . . . . . . . . 118\\n6.2 Image classification . . . . . . . 120\\n6.3 Object detection . . . . . . . . . 121\\n4', metadata={'source': 'rag/deep learning.pdf', 'page': 3}),\n",
       " Document(page_content='6.4 Semantic segmentation . . . . . 126\\n6.5 Speech recognition . . . . . . . 129\\n6.6 Text-image representations . . . 131\\n6.7 Reinforcement learning . . . . . 134\\n7 Synthesis 138\\n7.1 Text generation . . . . . . . . . 139\\n7.2 Image generation . . . . . . . . 142\\n8 The Compute Schism 146\\n8.1 Prompt Engineering . . . . . . 147\\n8.2 Quantization . . . . . . . . . . . 150\\n8.3 Adapters . . . . . . . . . . . . . 153\\n8.4 Model merging . . . . . . . . . 156\\nThe missing bits 158\\nBibliography 164\\nIndex 176\\n5', metadata={'source': 'rag/deep learning.pdf', 'page': 4}),\n",
       " Document(page_content='List of Figures\\n1.1 Kernel regression . . . . . . . . . . 14\\n1.2 Overfitting of kernel regression . . 16\\n3.1 Causal autoregressive model . . . . 32\\n3.2 Gradient descent . . . . . . . . . . . 36\\n3.3 Backpropagation . . . . . . . . . . . 40\\n3.4 Feature warping . . . . . . . . . . . 46\\n3.5 Training and validation losses . . . 49\\n3.6 Scaling laws . . . . . . . . . . . . . 53\\n3.7 Model training costs . . . . . . . . . 55\\n4.1 1D convolution . . . . . . . . . . . . 63\\n4.2 2D convolution . . . . . . . . . . . . 64\\n4.3 Stride, padding, and dilation . . . . 65\\n4.4 Receptive field . . . . . . . . . . . . 68\\n4.5 Activation functions . . . . . . . . . 72\\n4.6 Max pooling . . . . . . . . . . . . . 75\\n4.7 Dropout . . . . . . . . . . . . . . . . 78\\n4.8 Dropout 2D . . . . . . . . . . . . . . 79\\n4.9 Batch normalization . . . . . . . . . 81\\n4.10 Skip connections . . . . . . . . . . . 85\\n6', metadata={'source': 'rag/deep learning.pdf', 'page': 5}),\n",
       " Document(page_content='4.11 Attention operator interpretation . 88\\n4.12 Complete attention operator . . . . 90\\n4.13 Multi-Head Attention layer . . . . . 92\\n5.1 Multi-Layer Perceptron . . . . . . . 99\\n5.2 LeNet-like convolutional model . . 102\\n5.3 Residual block . . . . . . . . . . . . 103\\n5.4 Downscaling residual block . . . . . 104\\n5.5 ResNet-50 . . . . . . . . . . . . . . . 105\\n5.6 Transformer components . . . . . . 109\\n5.7 Transformer . . . . . . . . . . . . . 110\\n5.8 GPT model . . . . . . . . . . . . . . 112\\n5.9 ViT model . . . . . . . . . . . . . . 114\\n6.1 Convolutional object detector . . . 122\\n6.2 Object detection with SSD . . . . . 123\\n6.3 Semantic segmentation with PSP . . 127\\n6.4 CLIP zero-shot prediction . . . . . . 133\\n6.5 DQN state value evolution . . . . . 136\\n7.1 Few-shot prediction with a GPT . . 140\\n7.2 Denoising diffusion . . . . . . . . . 143\\n8.1 Chain-of-thought . . . . . . . . . . 148\\n8.2 Quantization . . . . . . . . . . . . . 151\\n7', metadata={'source': 'rag/deep learning.pdf', 'page': 6}),\n",
       " Document(page_content='Foreword\\nThe current period of progress in artificial in-\\ntelligence was triggered when Krizhevsky et al.\\n[2012] demonstrated that an artificialneuralnet-\\nwork designed twenty years earlier [LeCun et al.,\\n1989] could outperform complex state-of-the-\\nart image recognition methods by a huge mar-\\ngin, simply by being a hundred times larger and\\ntrained on a dataset similarly scaled up.\\nThis breakthrough was made possible thanks\\ntoGraph icalProcessingUnits ( GPUs), highly\\nparallel consumer-grade computing devices de-\\nveloped for real-time image synthesis and repur-\\nposed for artificial neural networks.\\nSince then, under the umbrella term of â€œ deep\\nlearn ing,â€ innovations in the structures of these\\nnetworks, the strategies to train them, and ded-\\nicated hardware have allowed for an exponen-\\ntial increase in both their size and the quantity\\nof training data they take advantage of [Sevilla\\n8', metadata={'source': 'rag/deep learning.pdf', 'page': 7}),\n",
       " Document(page_content='et al., 2022]. This has resulted in a wave of suc-\\ncessful applications across technical domains,\\nfrom computer vision and robotics to speech\\nprocessing, and since 2020 in the development\\nof Large Language Models with general proto-\\nreasoning capabilities [Chowdhery et al., 2022].\\nAlthough the bulk of deep learning is not difficult\\nto understand, it combines diverse components\\nsuch as linear algebra, calculus, probabilities, op-\\ntimization, signal processing, programming, al-\\ngorithmics, and high-performance computing,\\nmaking it complicated to learn.\\nInstead of trying to be exhaustive, this little book\\nis limited to the background necessary to under-\\nstand a few important models. This proved to\\nbe a popular approach, resulting in more than\\n500,000 downloads of the PDF file in the 12\\nmonths following its announcement on Twitter.\\nIf you did not get this book from its official URL\\nhttps://fleuret.org/public/lbdl.pdf\\nplease do so, to allow the estimation of the num-\\nber of readers.\\nFranÃ§ois Fleuret,\\nMay 19, 2024\\n9', metadata={'source': 'rag/deep learning.pdf', 'page': 8}),\n",
       " Document(page_content='Part I\\nFoundations\\n10', metadata={'source': 'rag/deep learning.pdf', 'page': 9}),\n",
       " Document(page_content='Chapter 1\\nMachine Learning\\nDeep learn ing belongs historically to the larger\\nfield of statistical machine learn ing, as it funda-\\nmentally concerns methods that are able to learn\\nrepresentations from data. The techniques in-\\nvolved come originally from artificialneuralnet-\\nworks, and the â€œdeepâ€ qualifier highlights that\\nmodels are long compositions of mappings, now\\nknown to achieve greater performance.\\nThe modularity, versatility, and scalability of\\ndeep models have resulted in a plethora of spe-\\ncific mathematical methods and software devel-\\nopment tools, establishing deep learning as a\\ndistinct and vast technical field.\\n11', metadata={'source': 'rag/deep learning.pdf', 'page': 10}),\n",
       " Document(page_content='1.1 Learning from data\\nThe simplest use case for a model trained from\\ndata is when a signal xis accessible, for instance,\\nthe picture of a license plate, from which one\\nwants to predict a quantity y, such as the string\\nof characters written on the plate.\\nIn many real-world situations where xis a high-\\ndimensional signal captured in an uncontrolled\\nenvironment, it is too complicated to come up\\nwith an analytical recipe that relates xandy.\\nWhat one can do is to collect a large train ing\\nsetð’Ÿof pairs (xn,yn), and devise a paramet-\\nricmodel f. This is a piece of computer code\\nthat incorporates train able parameterswthat\\nmodulate its behavior, and such that, with the\\nproper values wâˆ—, it is a good predictor. â€œGoodâ€\\nhere means that if an xis given to this piece\\nof code, the value Ë†y=f(x;wâˆ—)it computes is\\na good estimate of the ythat would have been\\nassociated with xin the training set had it been\\nthere.\\nThis notion of goodness is usually formalized\\nwith a lossâ„’(w)which is small when f(Â·;w)is\\ngood on ð’Ÿ. Then, train ing the model consists of\\ncomputing a value wâˆ—that minimizes â„’(wâˆ—).\\n12', metadata={'source': 'rag/deep learning.pdf', 'page': 11}),\n",
       " Document(page_content='Most of the content of this book is about the defi-\\nnition of f, which, in realistic scenarios, is a com-\\nplex combination of pre-defined sub-modules.\\nThe trainable parameters that compose ware of-\\nten called weights, by analogy with the synaptic\\nweights of biological neural networks. In addi-\\ntion to these parameters, models usually depend\\nonhyper-parameters, which are set according\\nto domain prior knowledge, best practices, or re-\\nsource constraints. They may also be optimized\\nin some way, but with techniques different from\\nthose used to optimize w.\\n13', metadata={'source': 'rag/deep learning.pdf', 'page': 12}),\n",
       " Document(page_content='1.2 Basis function regression\\nWe can illustrate the training of a model in a sim-\\nple case where xnandynare two real numbers,\\nthe loss is the mean squared error:\\nâ„’(w) =1\\nNNX\\nn=1(ynâˆ’f(xn;w))2, (1.1)\\nandf(Â·;w)is a linear combination of a pre-\\ndefined basis of functions f1,...,f K, with w=\\n(w1,...,w K):\\nf(x;w) =KX\\nk=1wkfk(x).\\nSince f(xn;w)is linear with respect to the wks\\nandâ„’(w)is quadratic with respect to f(xn;w),\\nFigure 1.1: Given a basis of functions (blue curves)\\nand a training set (black dots), we can compute an\\noptimal linear combination of the former (red curve)\\nto approximate the latter for the mean squared error.\\n14', metadata={'source': 'rag/deep learning.pdf', 'page': 13}),\n",
       " Document(page_content='the loss â„’(w)is quadratic with respect to the\\nwks, and finding wâˆ—that minimizes it boils down\\nto solving a linear system. See Figure 1.1 for an\\nexample with Gaussian kernels as fk.\\n15', metadata={'source': 'rag/deep learning.pdf', 'page': 14}),\n",
       " Document(page_content='1.3 Under and overfitting\\nA key element is the interplay between the capac-\\nity of the model, that is its flexibility and ability\\nto fit diverse data, and the amount and quality\\nof the training data. When the capacity is insuf-\\nficient, the model cannot fit the data, resulting\\nin a high error during training. This is referred\\nto as underfitting.\\nOn the contrary, when the amount of data is in-\\nsufficient, as illustrated in Figure 1.2, the model\\nwill often learn characteristics specific to the\\ntraining examples, resulting in excellent perfor-\\nmance during training, at the cost of a worse\\nFigure 1.2: If the amount of training data (black dots)\\nis small compared to the capacity of the model, the em-\\npirical performance of the fitted model during training\\n(red curve) reflects poorly its actual fit to the underly-\\ning data structure (thin black curve), and consequently\\nits usefulness for prediction.\\n16', metadata={'source': 'rag/deep learning.pdf', 'page': 15}),\n",
       " Document(page_content='fit to the global structure of the data, and poor\\nperformance on new inputs. This phenomenon\\nis referred to as overfitting.\\nSo, a large part of the art of applied machine\\nlearn ing is to design models that are not too\\nflexible yet still able to fit the data. This is done\\nby crafting the right inductivebias in a model,\\nwhich means that its structure corresponds to\\nthe underlying structure of the data at hand.\\nEven though this classical perspective is relevant\\nfor reasonably-sized deep models, things get con-\\nfusing with large ones that have a very large\\nnumber of trainable parameters and extreme ca-\\npacity yet still perform well on prediction. We\\nwill come back to this in Â§ 3.6 and Â§ 3.7.\\n17', metadata={'source': 'rag/deep learning.pdf', 'page': 16}),\n",
       " Document(page_content='1.4 Categories of models\\nWe can organize the use of machine learn ing\\nmodels into three broad categories:\\nâ€¢Regression consists of predicting a\\ncontinuous-valued vector yâˆˆRK, for instance,\\na geometrical position of an object, given an\\ninput signal X. This is a multi-dimensional\\ngeneralization of the setup we saw in Â§ 1.2. The\\ntraining set is composed of pairs of an input\\nsignal and a ground -truth value.\\nâ€¢Classification aims at predicting a value from\\na finite set {1,...,C}, for instance, the label Yof\\nan image X. As with regression, the training set\\nis composed of pairs of input signal, and ground-\\ntruth quantity, here a label from that set. The\\nstandard way of tackling this is to predict one\\nscore per potential class, such that the correct\\nclass has the maximum score.\\nâ€¢Densitymodeling has as its objective to model\\nthe probability density function of the data ÂµX\\nitself, for instance, images. In that case, the train-\\ning set is composed of values xnwithout associ-\\nated quantities to predict, and the trained model\\nshould allow for the evaluation of the probability\\ndensity function, or sampling from the distribu-\\ntion, or both.\\n18', metadata={'source': 'rag/deep learning.pdf', 'page': 17}),\n",
       " Document(page_content='Both regression and classification are generally\\nreferred to as supervised learn ing, since the\\nvalue to be predicted, which is required as a\\ntarget during training, has to be provided, for in-\\nstance, by human experts. On the contrary, den-\\nsity modeling is usually seen as unsupervised\\nlearn ing, since it is sufficient to take existing\\ndata without the need for producing an associ-\\nated ground-truth.\\nThese three categories are not disjoint; for in-\\nstance, classification can be cast as class-score\\nregression, or discrete sequence density model-\\ning as iterated classification. Furthermore, they\\ndo not cover all cases. One may want to predict\\ncompounded quantities, or multiple classes, or\\nmodel a density conditional on a signal.\\n19', metadata={'source': 'rag/deep learning.pdf', 'page': 18}),\n",
       " Document(page_content='Chapter 2\\nEfficient\\nComputation\\nFrom an implementation standpoint, deep learn-\\ning is about executing heavy computations with\\nlarge amounts of data. The Graph icalProcessing\\nUnits ( GPUs) have been instrumental in the suc-\\ncess of the field by allowing such computations\\nto be run on affordable hardware.\\nThe importance of their use, and the resulting\\ntechnical constraints on the computations that\\ncan be done efficiently, force the research in the\\nfield to constantly balance mathematical sound-\\nness and implementability of novel methods.\\n20', metadata={'source': 'rag/deep learning.pdf', 'page': 19}),\n",
       " Document(page_content='2.1 GPUs, TPUs, and batches\\nGraphical Processing Units were originally de-\\nsigned for real-time image synthesis, which re-\\nquires highly parallel architectures that happen\\nto be well suited for deep models. As their usage\\nfor AI has increased, GPUs have been equipped\\nwith dedicated tensorcores, and deep-learning\\nspecialized chips such as Googleâ€™s TensorPro-\\ncessingUnits ( TPUs) have been developed.\\nA GPU possesses several thousand parallel units\\nand its own fast memory. The limiting factor\\nis usually not the number of computing units,\\nbut the read-write operations tomem ory. The\\nslowest link is between the CPU memory and\\nthe GPU memory, and consequently one should\\navoid copying data across devices. Moreover,\\nthe structure of the GPU itself involves multiple\\nlevels of cache mem ory, which are smaller but\\nfaster, and computation should be organized to\\navoid copies between these different caches.\\nThis is achieved, in particular, by organizing the\\ncomputation in batches ofsamples that can fit\\nentirely in the GPU memory and are processed\\nin parallel. When an operator combines a sample\\nand model parameters, both have to be moved\\nto the cache memory near the actual computing\\n21', metadata={'source': 'rag/deep learning.pdf', 'page': 20}),\n",
       " Document(page_content='units. Proceeding by batches allows for copying\\nthe model parameters only once, instead of doing\\nit for each sample. In practice, a GPU processes\\na batch that fits in memory almost as quickly as\\nit would process a single sample.\\nA standard GPU has a theoretical peak perfor-\\nmance of 1013â€“1014floating-point operations\\n(FLOPs) per second, and its memory typically\\nranges from 8to80gigabytes. The standard\\nFP32 encoding of float numbers is on 32bits, but\\nempirical results show that using encoding on\\n16bits, or even less for some operands, does not\\ndegrade performance.\\nWe will come back in Â§ 3.7 to the large size of\\ndeep architectures.\\n22', metadata={'source': 'rag/deep learning.pdf', 'page': 21}),\n",
       " Document(page_content='2.2 Tensors\\nGPUs and deep learn ingframe works such as Py-\\nTorch or JAX manipulate the quantities to be\\nprocessed by organizing them as tensors, which\\nare series of scalars arranged along several dis-\\ncrete axes. They are elements of RN1Ã—Â·Â·Â·Ã— ND\\nthat generalize the notion of vector and matrix.\\nTensors are used to represent both the signals to\\nbe processed, the train able parameters of the\\nmodels, and the intermediate quantities they\\ncompute. The latter are called activations, in\\nreference to neuronal activations.\\nFor instance, a time series is naturally encoded\\nas aTÃ—Dtensor, or, for historical reasons, as a\\nDÃ—Ttensor, where Tis its duration and Dis\\nthe dimension of the feature representation at\\nevery time step, often referred to as the number\\nofchan nels. Similarly, a 2D-structured signal can\\nbe represented as a DÃ—HÃ—Wtensor, where H\\nandWare its height and width. An RGB image\\nwould correspond to D= 3, but the number of\\nchannels can grow up to several thousands in\\nlarge models.\\nAdding more dimensions allows for the represen-\\ntation of series of objects. For example, fifty RGB\\nimages of resolution 32Ã—24can be encoded as\\n23', metadata={'source': 'rag/deep learning.pdf', 'page': 22}),\n",
       " Document(page_content='a50Ã—3Ã—24Ã—32tensor.\\nDeep learning libraries provide a large number\\nof operations that encompass standard linear\\nalgebra, complex reshaping and extraction, and\\ndeep-learning specific operations, some of which\\nwe will see in Chapter 4. The implementation of\\ntensors separates the shape representation from\\nthe storage layout of the coefficients in mem-\\nory, which allows many reshaping, transposing,\\nand extraction operations to be done without\\ncoefficient copying, hence extremely rapidly.\\nIn practice, virtually any computation can be\\ndecomposed into elementary tensor operations,\\nwhich avoids non-parallel loops at the language\\nlevel and poor memory management.\\nBesides being convenient tools, tensors are\\ninstrumental in achieving computational effi-\\nciency. All the people involved in the develop-\\nment of an operational deep model, from the\\ndesigners of the drivers, libraries, and models\\nto those of the computers and chips, know that\\nthe data will be manipulated as tensors. The\\nresulting constraints on locality and block de-\\ncomposability enable all the actors in this chain\\nto come up with optimal designs.\\n24', metadata={'source': 'rag/deep learning.pdf', 'page': 23}),\n",
       " Document(page_content='Chapter 3\\nTraining\\nAs introduced in Â§ 1.1, training a model consists\\nof minimizing a loss â„’(w)which reflects the\\nperformance of the predictor f(Â·;w)on a train -\\ningsetð’Ÿ.\\nSince models are usually extremely complex, and\\ntheir performance is directly related to how well\\nthe loss is minimized, this minimization is a key\\nchallenge, which involves both computational\\nand mathematical difficulties.\\n25', metadata={'source': 'rag/deep learning.pdf', 'page': 24}),\n",
       " Document(page_content='3.1 Losses\\nThe example of the mean squared error from\\nEquation 1.1 is a standard loss for predicting a\\ncontinuous value.\\nFor density modeling, the standard loss is the\\nlikelihood of the data. If f(x;w)is to be inter-\\npreted as a normalized log-probability or log-\\ndensity, the loss is the opposite of the sum of its\\nvalues over training samples, which corresponds\\nto the likelihood of the data-set.\\nCross-entropy\\nForclassification, the usual strategy is that the\\noutput of the model is a vector with one com-\\nponent f(x;w)yper class y, interpreted as the\\nlogarithm of a non-normalized probability, or\\nlogit.\\nWith Xthe input signal and Ythe class to pre-\\ndict, we can then compute from fan estimate\\nof the posteriorprob abilities:\\nË†P(Y=y|X=x) =expf(x;w)yP\\nzexpf(x;w)z.\\nThis expression is generally called the softmax,\\nor more adequately, the softargmax, of the logits.\\n26', metadata={'source': 'rag/deep learning.pdf', 'page': 25}),\n",
       " Document(page_content='To be consistent with this interpretation, the\\nmodel should be trained to maximize the proba-\\nbility of the true classes, hence to minimize the\\ncross -entropy, expressed as:\\nâ„’ce(w) =âˆ’1\\nNNX\\nn=1logË†P(Y=yn|X=xn)\\n=1\\nNNX\\nn=1âˆ’logexpf(xn;w)ynP\\nzexpf(xn;w)z| {z }\\nLce(f(xn;w),yn).\\nContrastive loss\\nIn certain setups, even though the value to be\\npredicted is continuous, the supervision takes\\nthe form of ranking constraints. The typical do-\\nmain where this is the case is metriclearn ing,\\nwhere the objective is to learn a measure of dis-\\ntance between samples such that a sample xa\\nfrom a certain semantic class is closer to any\\nsample xbof the same class than to any sample\\nxcfrom another class. For instance, xaandxb\\ncan be two pictures of a certain person, and xca\\npicture of someone else.\\nThe standard approach for such cases is to min-\\nimize a contrastive loss, in that case, for in-\\nstance, the sum over triplets (xa,xb,xc), such\\n27', metadata={'source': 'rag/deep learning.pdf', 'page': 26}),\n",
       " Document(page_content='thatya=ybÌ¸=yc, of\\nmax(0 ,1âˆ’f(xa,xc;w)+f(xa,xb;w)).\\nThis quantity will be strictly positive unless\\nf(xa,xc;w)â‰¥1+f(xa,xb;w).\\nEngineering the loss\\nUsually, the loss minimized during training is\\nnot the actual quantity one wants to optimize\\nultimately, but a proxy for which finding the best\\nmodel parameters is easier. For instance, cross-\\nentropy is the standard loss for classification,\\neven though the actual performance measure is\\na classification error rate, because the latter has\\nno informative gradient, a key requirement as\\nwe will see in Â§ 3.3.\\nIt is also possible to add terms to the loss that\\ndepend on the trainable parameters of the model\\nthemselves to favor certain configurations.\\nThe weight decay regularization, for instance,\\nconsists of adding to the loss a term proportional\\nto the sum of the squared parameters. This can\\nbe interpreted as having a Gaussian Bayesian\\nprior on the parameters, which favors smaller\\nvalues and thereby reduces the influence of the\\ndata. This degrades performance on the train-\\n28', metadata={'source': 'rag/deep learning.pdf', 'page': 27}),\n",
       " Document(page_content='ing set, but reduces the gap between the per-\\nformance in training and that on new, unseen\\ndata.\\n29', metadata={'source': 'rag/deep learning.pdf', 'page': 28}),\n",
       " Document(page_content='3.2 Autoregressive models\\nA key class of methods, particularly for deal-\\ning with discrete sequences in natural language\\nprocessing and computer vision, are the autore-\\ngressivemodels,\\nThe chain rule for probabilities\\nSuch models put to use the chain rule from prob-\\nability theory:\\nP(X1=x1,X2=x2,...,X T=xT) =\\nP(X1=x1)\\nÃ—P(X2=x2|X1=x1)\\n...\\nÃ—P(XT=xT|X1=x1,...,X Tâˆ’1=xTâˆ’1).\\nAlthough this decomposition is valid for a ran-\\ndom sequence of any type, it is particularly effi-\\ncient when the signal of interest is a sequence\\noftokens from a finite vocabulary{1,...K}.\\nWith the convention that the additional token âˆ…\\nstands for an â€œunknownâ€ quantity, we can rep-\\nresent the event {X1=x1,...,X t=xt}as the\\nvector (x1,...,x t,âˆ…,...,âˆ…).\\n30', metadata={'source': 'rag/deep learning.pdf', 'page': 29}),\n",
       " Document(page_content='Then, a model\\nf:{âˆ…,1,...,K }Tâ†’RK\\nwhich, given such an input, computes a vector\\nltofKlogits corresponding to\\nË†P(Xt|X1=x1,...,X tâˆ’1=xtâˆ’1),\\nallows to sample one token given the previous\\nones.\\nThe chain rule ensures that by sampling Tto-\\nkensxt, one at a time given the previously sam-\\npledx1,...,x tâˆ’1, we get a sequence that follows\\nthe joint distribution. This is an autoregressive\\ngenerative model.\\nTraining such a model can be done by minimiz-\\ning the sum across training sequences and time\\nsteps of the cross -entropy loss\\nLce\\x00\\nf(x1,...,x tâˆ’1,âˆ…,...,âˆ…;w),xt\\x01\\n,\\nwhich is formally equivalent to maximizing the\\nlikelihood of the true xts.\\nThe value that is classically monitored is not the\\ncross-entropy itself, but the perplexity, which is\\ndefined as the exponential of the cross-entropy.\\nIt corresponds to the number of values of a uni-\\nform distribution with the same entropy, which\\nis generally more interpretable.\\n31', metadata={'source': 'rag/deep learning.pdf', 'page': 30}),\n",
       " Document(page_content='x1x2 ... xTâˆ’2xTâˆ’1l1 l2 l3 ... lTâˆ’1 lT\\nf\\nFigure 3.1: An autoregressive model f, iscausal if a\\ntime step xtof the input sequence modulates the pre-\\ndicted logits lsonly if s > t , as depicted by the blue\\narrows. This allows computing the distributions at all\\nthe time steps in one pass during training. During sam-\\npling, however, the ltandxtare computed sequentially,\\nthe latter sampled with the former, as depicted by the\\nred arrows.\\nCausal models\\nThe training procedure we just described re-\\nquires a different input for each t, and the bulk\\nof the computation done for t < tâ€²is repeated for\\ntâ€². This is extremely inefficient since Tis often\\nof the order of hundreds or thousands.\\nThe standard strategy to address this issue is to\\ndesign a model fthat predicts all the vectors of\\nlogits l1,...,l Tat once, that is:\\nf:{1,...,K }Tâ†’RTÃ—K,\\n32', metadata={'source': 'rag/deep learning.pdf', 'page': 31}),\n",
       " Document(page_content='but with a computational structure such that the\\ncomputed logits ltforxtdepend only on the\\ninput values x1,...,x tâˆ’1.\\nSuch a model is called causal, since it corre-\\nsponds, in the case of temporal series, to not\\nletting the future influence the past, as illustrated\\nin Figure 3.1.\\nThe consequence is that the output at every posi-\\ntion is the one that would be obtained if the input\\nwere only available up to before that position.\\nDuring training, it allows one to compute the\\noutput for a full sequence and to maximize the\\npredicted probabilities of all the tokens of that\\nsame sequence, which again boils down to mini-\\nmizing the sum of the per-token cross-entropy.\\nNote that, for the sake of simplicity, we have\\ndefined fas operating on sequences of a fixed\\nlength T. However, models used in practice,\\nsuch as the transformers we will see in Â§ 5.3, are\\nable to process sequences of arbitrary length.\\nTokenizer\\nOne important technical detail when dealing\\nwith natural languages is that the representation\\nas tokens can be done in multiple ways, ranging\\nfrom the finest granularity of individual symbols\\n33', metadata={'source': 'rag/deep learning.pdf', 'page': 32}),\n",
       " Document(page_content='to entire words. The conversion to and from the\\ntoken representation is carried out by a separate\\nalgorithm called a tokenizer.\\nA standard method is the Byte Pair Encoding\\n(BPE) [Sennrich et al., 2015] that constructs to-\\nkens by hierarchically merging groups of char-\\nacters, trying to get tokens that represent frag-\\nments of words of various lengths but of similar\\nfrequencies, allocating tokens to long frequent\\nfragments as well as to rare individual symbols.\\n34', metadata={'source': 'rag/deep learning.pdf', 'page': 33}),\n",
       " Document(page_content='3.3 Gradient descent\\nExcept in specific cases like the linear regression\\nwe saw in Â§ 1.2, the optimal parameters wâˆ—do\\nnot have a closed-form expression. In the general\\ncase, the tool of choice to minimize a function is\\ngradientdescent. It starts by initializing the pa-\\nrameters with a random w0, and then improves\\nthis estimate by iterating gradientsteps, each\\nconsisting of computing the gradient of the loss\\nwith respect to the parameters, and subtracting\\na fraction of it:\\nwn+1=wnâˆ’Î·âˆ‡â„’|w(wn). (3.1)\\nThis procedure corresponds to moving the cur-\\nrent estimate a bit in the direction that locally\\ndecreases â„’(w)maximally, as illustrated in Fig-\\nure 3.2.\\nLearning rate\\nThehyper-parameterÎ·is called the learn ingrate.\\nIt is a positive value that modulates how quickly\\nthe minimization is done, and must be chosen\\ncarefully.\\nIf it is too small, the optimization will be slow\\nat best, and may be trapped in a localminimum\\nearly. If it is too large, the optimization may\\n35', metadata={'source': 'rag/deep learning.pdf', 'page': 34}),\n",
       " Document(page_content='w\\nwâ„’(w)\\nFigure 3.2: At every point w, the gradient âˆ‡â„’|w(w)is\\nin the direction that maximizes the increase of â„’, or-\\nthogonal to the level curves (top). The gradient descent\\nminimizes â„’(w)iteratively by subtracting a fraction\\nof the gradient at every step, resulting in a trajectory\\nthat follows the steepest descent (bottom).\\n36', metadata={'source': 'rag/deep learning.pdf', 'page': 35}),\n",
       " Document(page_content='bounce around a good minimum and never de-\\nscend into it. As we will see in Â§ 3.6, it can depend\\non the iteration number n.\\nStochastic Gradient Descent\\nAll the losses used in practice can be expressed as\\nan average of a loss per small group of samples,\\nor per sample such as:\\nâ„’(w) =1\\nNNX\\nn=1ð“n(w),\\nwhereð“n(w) =L(f(xn;w),yn)for some L, and\\nthe gradient is then:\\nâˆ‡â„’|w(w) =1\\nNNX\\nn=1âˆ‡ð“n|w(w). (3.2)\\nThe resulting gradientdescent would compute\\nexactly the sum in Equation 3.2, which is usu-\\nally computationally heavy, and then update the\\nparameters according to Equation 3.1. However,\\nunder reasonable assumptions of exchangeabil-\\nity, for instance, if the samples have been prop-\\nerly shuffled, any partial sum of Equation 3.2\\nis an unbiased estimator of the full sum, albeit\\nnoisy. So, updating the parameters from partial\\nsums corresponds to doing more gradient steps\\n37', metadata={'source': 'rag/deep learning.pdf', 'page': 36}),\n",
       " Document(page_content='for the same computational budget, with noisier\\nestimates of the gradient. Due to the redundancy\\nin the data, this happens to be a far more efficient\\nstrategy.\\nWe saw in Â§ 2.1 that processing a batch of sam-\\nples small enough to fit in the computing de-\\nviceâ€™s memory is generally as fast as processing\\na single one. Hence, the standard approach is to\\nsplit the full set ð’Ÿinto batches, and to update\\nthe parameters from the estimate of the gradient\\ncomputed from each. This is called mini-batch\\nstochastic gradient descent, or stochas ticgradi-\\nentdescent ( SGD) for short.\\nIt is important to note that this process is ex-\\ntremely gradual, and that the number of mini-\\nbatches and gradient steps are typically of the\\norder of several million.\\nAs with many algorithms, intuition breaks down\\nin high dimensions, and although it may seem\\nthat this procedure would be easily trapped in\\na local minimum, in reality, due to the number\\nof parameters, the design of the models, and\\nthe stochasticity of the data, its efficiency is far\\ngreater than one might expect.\\nPlenty of variations of this standard strategy\\nhave been proposed. The most popular one is\\n38', metadata={'source': 'rag/deep learning.pdf', 'page': 37}),\n",
       " Document(page_content='Adam [Kingma and Ba, 2014], which keeps run-\\nning estimates of the mean and variance of each\\ncomponent of the gradient, and normalizes them\\nautomatically, avoiding scaling issues and differ-\\nent training speeds in different parts of a model.\\n39', metadata={'source': 'rag/deep learning.pdf', 'page': 38}),\n",
       " Document(page_content='3.4 Backpropagation\\nUsing gradient descent requires a tech-\\nnical means to compute âˆ‡ð“|w(w)where\\nð“=L(f(x;w);y). Given that fandLare\\nboth compositions of standard tensor opera-\\ntions, as for any mathematical expression, the\\nchain rule from differential calculus allows us to\\nget an expression of it.\\nFor the sake of making notation lighter, we will\\nnot specify at which point gradients are com-\\nputed, since the context makes it clear.\\nx(dâˆ’1)x(d)f(d)(Â·;wd)\\nâˆ‡ð“|x(dâˆ’1) âˆ‡ð“|x(d)Ã—Jf(d)|x\\nâˆ‡ð“|wdÃ—Jf(d)|w\\nFigure 3.3: Given a model f=f(D)â—¦ Â·Â·Â· â—¦ f(1), the\\nforward pass computes the outputs x(d)of the f(d)in\\norder (top, black). The backward pass computes the\\ngradients of the loss with respect to the activations x(d)\\n(bottom, blue) and the parameters wd(bottom, red)\\nbackward by multiplying them by the Jacobians.\\n40', metadata={'source': 'rag/deep learning.pdf', 'page': 39}),\n",
       " Document(page_content='Forward and backward passes\\nConsider the simple case of a composition of\\nmappings:\\nf=f(D)â—¦f(Dâˆ’1)â—¦ Â·Â·Â· â—¦ f(1).\\nThe output of f(x;w)can be computed by start-\\ning with x(0)=xand applying iteratively:\\nx(d)=f(d)\\x10\\nx(dâˆ’1);wd\\x11\\n,\\nwithx(D)as the final value.\\nThe individual scalar values of these interme-\\ndiate results x(d)are traditionally called acti-\\nvations in reference to neuron activations, the\\nvalue Dis the depth of the model, the individual\\nmappings f(d)are referred to as layers, as we\\nwill see in Â§ 4.1, and their sequential evaluation\\nis the forward pass (see Figure 3.3, top).\\nConversely, the gradient âˆ‡ð“|x(dâˆ’1)of the loss\\nwith respect to the output x(dâˆ’1)off(dâˆ’1)is\\nthe product of the gradient âˆ‡ð“|x(d)with respect\\nto the output of f(d)multiplied by the Jacobian\\nJf(dâˆ’1)|xoff(dâˆ’1)with respect to its variable\\nx. Thus, the gradients with respect to the out-\\nputs of all the f(d)s can be computed recursively\\nbackward, starting with âˆ‡ð“|x(D)=âˆ‡L|x.\\n41', metadata={'source': 'rag/deep learning.pdf', 'page': 40}),\n",
       " Document(page_content='And the gradient that we are interested in for\\ntraining, that is âˆ‡ð“|wd, is the gradient with re-\\nspect to the output of f(d)multiplied by the Ja-\\ncobian Jf(d)|woff(d)with respect to the param-\\neters.\\nThis iterative computation of the gradients with\\nrespect to the intermediate activations, com-\\nbined with that of the gradients with respect\\nto the layersâ€™ parameters, is the back ward pass\\n(see Figure 3.3, bottom). The combination of\\nthis computation with the procedure of gradient\\ndescent is called back prop agation.\\nIn practice, the implementation details of the\\nforward and backward passes are hidden from\\nprogrammers. Deep learning frameworks are\\nable to automatically construct the sequence of\\noperations to compute gradients.\\nA particularly convenient algorithm is Autograd\\n[Baydin et al., 2015], which tracks tensor opera-\\ntions and builds, on the fly, the combination of\\noperators for gradients. Thanks to this, a piece of\\nimperative programming that manipulates ten-\\nsors can automatically compute the gradient of\\nany quantity with respect to any other.\\n42', metadata={'source': 'rag/deep learning.pdf', 'page': 41}),\n",
       " Document(page_content='Resource usage\\nRegarding the computational cost, as we will\\nsee, the bulk of the computation goes into linear\\noperations, each requiring one matrix product\\nfor the forward pass and two for the products by\\nthe Jacobians for the backward pass, making the\\nlatter roughly twice as costly as the former.\\nThe mem oryrequire ment during inference is\\nroughly equal to that of the most demanding\\nindividual layer. For training, however, the back-\\nward pass requires keeping the activations com-\\nputed during the forward pass to compute the\\nJacobians, which results in a memory usage that\\ngrows proportionally to the modelâ€™s depth. Tech-\\nniques exist to trade the memory usage for com-\\nputation by either relying on reversible layers\\n[Gomez et al., 2017], or using check point ing,\\nwhich consists of storing activations for some\\nlayers only and recomputing the others on the fly\\nwith partial forward passes during the backward\\npass [Chen et al., 2016].\\nVanishing gradient\\nA key historical issue when training a large net-\\nwork is that when the gradient propagates back-\\nwards through an operator, it may be scaled by a\\n43', metadata={'source': 'rag/deep learning.pdf', 'page': 42}),\n",
       " Document(page_content='multiplicative factor, and consequently decrease\\nor increase exponentially when it traverses many\\nlayers. A standard method to prevent it from\\nexploding is gradientnorm clipping, which con-\\nsists of re-scaling the gradient to set its norm to\\na fixed threshold if it is above it [Pascanu et al.,\\n2013].\\nWhen the gradient decreases exponentially, this\\nis called the vanishinggradient, and it may\\nmake the training impossible, or, in its milder\\nform, cause different parts of the model to be\\nupdated at different speeds, degrading their co-\\nadaptation [Glorot and Bengio, 2010].\\nAs we will see in Chapter 4, multiple techniques\\nhave been developed to prevent this from hap-\\npening, reflecting a change in perspective that\\nwas crucial to the success of deep-learning: in-\\nstead of trying to improve generic optimization\\nmethods, the effort shifted to engineering the\\nmodels themselves to make them optimizable.\\n44', metadata={'source': 'rag/deep learning.pdf', 'page': 43}),\n",
       " Document(page_content='3.5 The value of depth\\nAs the term â€œdeep learningâ€ indicates, useful\\nmodels are generally compositions of long se-\\nries of mappings. Training them with gradient\\ndescent results in a sophisticated co-adaptation\\nof the mappings, even though this procedure is\\ngradual and local.\\nWe can illustrate this behavior with a simple\\nmodelR2â†’R2that combines eight layers, each\\nmultiplying its input by a 2Ã—2matrix and ap-\\nplying Tanh per component, with a final linear\\nclassifier. This is a simplified version of the stan-\\ndard Multi -Layer Perceptron that we will see in\\nÂ§ 5.1.\\nIf we train this model with SGD and cross -en-\\ntropy on a toy binary classification task (Figure\\n3.4, top left), the matrices co-adapt to deform the\\nspace until the classification is correct, which\\nimplies that the data have been made linearly\\nseparable before the final affine operation (Fig-\\nure 3.4, bottom right).\\nSuch an example gives a glimpse of what a deep\\nmodel can achieve; however, it is partially mis-\\nleading due to the low dimension of both the sig-\\nnal to process and the internal representations.\\nEverything is kept in 2D here for the sake of\\n45', metadata={'source': 'rag/deep learning.pdf', 'page': 44}),\n",
       " Document(page_content='d= 0 d= 1 d= 2\\nd= 3 d= 4 d= 5\\nd= 6 d= 7 d= 8\\nFigure 3.4: Each plot shows the deformation of the\\nspace and the resulting positioning of the training\\npoints in R2afterdlayers of processing, starting with\\nthe input to the model itself (top left). The oblique line\\nin the last plot (bottom right) shows the final affine\\ndecision.\\n46', metadata={'source': 'rag/deep learning.pdf', 'page': 45}),\n",
       " Document(page_content='visualization, while real models take advantage\\nof representations in high dimensions, which, in\\nparticular, facilitates the optimization by provid-\\ning many degrees of freedom.\\nEmpirical evidence accumulated over twenty\\nyears demonstrates that state-of-the-art perfor-\\nmance across application domains necessitates\\nmodels with tens of layers, such as resid ualnet-\\nworks (see Â§ 5.2) or Trans form ers (see Â§ 5.3).\\nTheoretical results show that, for a fixed com-\\nputational budget or number of parameters, in-\\ncreasing the depth leads to a greater complexity\\nof the resulting mapping [Telgarsky, 2016].\\n47', metadata={'source': 'rag/deep learning.pdf', 'page': 46}),\n",
       " Document(page_content='3.6 Training protocols\\nTraining a deep network requires defining a pro-\\ntocol to make the most of computation and data,\\nand to ensure that performance will be good on\\nnew data.\\nAs we saw in Â§ 1.3, the performance on the train-\\ning samples may be misleading, so in the sim-\\nplest setup one needs at least two sets of samples:\\none is a train ingset, used to optimize the model\\nparameters, and the other is a testset, to evaluate\\nthe performance of the trained model.\\nAdditionally, there are usually hyper-parame-\\nters to adapt, in particular, those related to the\\nmodel architecture, the learning rate, and the\\nregularization terms in the loss. In that case,\\none needs a validation set that is disjoint from\\nboth the training and test sets to assess the best\\nconfiguration.\\nThe full training is usually decomposed into\\nepochs, each of which corresponds to going\\nthrough all the training examples once. The\\nusual dynamic of the losses is that the training\\nloss decreases as long as the optimization runs,\\nwhile the validation loss may reach a minimum\\nafter a certain number of epochs and then start\\nto increase, reflecting an overfitting regime, as\\n48', metadata={'source': 'rag/deep learning.pdf', 'page': 47}),\n",
       " Document(page_content='Loss\\nNumber of epochsOverfitting\\nTrainingValidation\\nFigure 3.5: As training progresses, a modelâ€™s perfor-\\nmance is usually monitored through losses. The train-\\ning loss is the one driving the optimization process and\\ngoes down, while the validation loss is estimated on\\nan other set of examples to assess the overfitting of\\nthe model. Overfitting appears when the model starts\\nto take into account random structures specific to the\\ntraining set at hand, resulting in the validation loss\\nstarting to increase.\\nintroduced in Â§ 1.3 and illustrated in Figure 3.5.\\nParadoxically, although they should suffer from\\nsevere overfitting due to their capacity, large\\nmodels usually continue to improve as training\\nprogresses. This may be due to the inductive\\nbias of the model becoming the main driver of\\noptimization when performance is near perfect\\n49', metadata={'source': 'rag/deep learning.pdf', 'page': 48}),\n",
       " Document(page_content='on the training set [Belkin et al., 2018].\\nAn important design choice is the learn ingrate\\nsched ule during training, that is, the specifica-\\ntion of the value of the learn ingrate at each iter-\\nation of the gradient descent. The general policy\\nis that the learning rate should be initially large\\nto avoid having the optimization being trapped\\nin a bad local minimum early, and that it should\\nget smaller so that the optimized parameter val-\\nues do not bounce around and reach a good min-\\nimum in a narrow valley of the loss landscape.\\nThe training of very large models may take\\nmonths on thousands of powerful GPUs and\\nhave a financial cost of several million dollars. At\\nthis scale, the training may involve many man-\\nual interventions, informed, in particular, by the\\ndynamics of the loss evolution.\\nFine-tuning\\nIt is often beneficial to adapt an already trained\\nmodel to a new task, referred to as a down stream\\ntask.\\nIt can be because the amount of data for the\\noriginal task is plentiful, while they are lim-\\nited for the downstream task, and the two tasks\\nshare enough similarities that statistical struc-\\n50', metadata={'source': 'rag/deep learning.pdf', 'page': 49}),\n",
       " Document(page_content='tures learned for the first provide a good induc-\\ntive bias for the second. It can also be to limit the\\ntraining cost by reusing the patterns encoded in\\nan existing model.\\nAdapting a pre-trained model to a specific task\\nis achieved with fine-tuning, which is a standard\\ntraining procedure for the downstream task, but\\nwhich starts from the pre-trained model instead\\nof using a random initialization.\\nThis is the main strategy for most computer vi-\\nsion applications which generally use a model\\npre-trained for classification on ImageNet [Deng\\net al., 2009] (see Â§ 6.3 and Â§ 6.4), and it is also how\\npurely generative pre-trained Large Language\\nModels are re-purposed as assistant-like models,\\nable to produce interactive dialogues (see Â§ 7.1).\\nWe come back to techniques to cope with lim-\\nited resources in inference and for fine-tuning\\nin Chapter 8.\\n51', metadata={'source': 'rag/deep learning.pdf', 'page': 50}),\n",
       " Document(page_content='3.7 The benefits of scale\\nThere is an accumulation of empirical results\\nshowing that performance, for instance, esti-\\nmated through the loss on test data, improves\\nwith the amount of data according to remarkable\\nscalinglaws, as long as the model size increases\\ncorrespondingly [Kaplan et al., 2020] (see Figure\\n3.6).\\nBenefiting from these scaling laws in the multi-\\nbillion sample regime is possible in part thanks to\\nthe structure of deep models which can be scaled\\nup arbitrarily, as we will see, by increasing the\\nnumber of layers or feature dimensions. But it\\nis also made possible by the distributed nature\\nof the computation they implement, and by the\\nstochas ticgradientdescent, which requires only\\na fraction of the data at a time and can operate\\nwith datasets whose size is orders of magnitude\\ngreater than that of the computing deviceâ€™s mem-\\nory. This has resulted in an exponential growth\\nof the models, as illustrated in Figure 3.7.\\nTypical vision models have 10â€“100million train -\\nable parameters and require 1018â€“1019FLOPs\\nfor training [He et al., 2015; Sevilla et al., 2022].\\nLanguage models have from 100million to hun-\\ndreds of billions of trainable parameters and re-\\n52', metadata={'source': 'rag/deep learning.pdf', 'page': 51}),\n",
       " Document(page_content='Test loss Test loss Test lossCompute (peta-FLOP/s-day)\\nDataset size (tokens)\\nNumber of parameters\\nFigure 3.6: Test loss of a language model vs. the amount\\nof computation in petaflop/s-day, the dataset size in\\ntokens, that is fragments of words, and the model size\\nin parameters [Kaplan et al., 2020].\\n53', metadata={'source': 'rag/deep learning.pdf', 'page': 52}),\n",
       " Document(page_content='Dataset Year Nb. of images Size\\nImageNet 2012 1.2M 150Gb\\nCityscape 2016 25K 60Gb\\nLAION-5B 2022 5.8B 240Tb\\nDataset Year Nb. of books Size\\nWMT-18-de-en 2018 14M 8Gb\\nThe Pile 2020 1.6B 825Gb\\nOSCAR 2020 12B 6Tb\\nTable 3.1: Some examples of publicly available datasets.\\nThe equivalent number of books is an indicative esti-\\nmate for 250 pages of 2000 characters per book.\\nquire 1020â€“1023FLOPs for training [Devlin et al.,\\n2018; Brown et al., 2020; Chowdhery et al., 2022;\\nSevilla et al., 2022]. These latter models require\\nmachines with multiple high-end GPUs.\\nTraining these large models is impossible using\\ndatasets with a detailed ground-truth costly to\\nproduce, which can only be of moderate size.\\nInstead, it is done with datasets automatically\\nproduced by combining data available on the\\ninternet with minimal curation, if any. These\\nsets may combine multiple modalities, such as\\ntext and images from web pages, or sound and\\nimages from videos, which can be used for large-\\nscale supervised training.\\nAs of 2024, the most powerful models are the so-\\n54', metadata={'source': 'rag/deep learning.pdf', 'page': 53}),\n",
       " Document(page_content='2015 2020101810211024\\n1KWh1MWh1GWh\\nAlexNetResNetAlphaGoAlphaZero\\nTransformer\\nGPTBERTGPT-2GPT-3\\nViTPaLM\\nLaMDA\\nWhisper\\nVGG16\\nGoogLeNetCLIP-ViT\\nYearTraining cost (FLOP)\\nFigure 3.7: Training costs in number of FLOP of some\\nlandmark models [Sevilla et al., 2023]. The colors in-\\ndicate the domains of application: Computer Vision\\n(blue), Natural Language Processing (red), or other\\n(black). The dashed lines correspond to the energy con-\\nsumption using A100s SXM in 16-bit precision. For\\nreference, the total electricity consumption in the US in\\n2021 was 3920 TWh.\\n55', metadata={'source': 'rag/deep learning.pdf', 'page': 54}),\n",
       " Document(page_content='called Large Language Models (LLMs), which we\\nwill see in Â§ 5.3 and Â§ 7.1, trained on extremely\\nlarge text datasets (see Table 3.1).\\n56', metadata={'source': 'rag/deep learning.pdf', 'page': 55}),\n",
       " Document(page_content='Part II\\nDeep Models\\n57', metadata={'source': 'rag/deep learning.pdf', 'page': 56}),\n",
       " Document(page_content='Chapter 4\\nModel Components\\nA deep model is nothing more than a complex\\ntensorial computation that can ultimately be\\ndecomposed into standard mathematical oper-\\nations from linear algebra and analysis. Over\\nthe years, the field has developed a large collec-\\ntion of high-level modules with a clear semantic,\\nand complex models combining these modules,\\nwhich have proven to be effective in specific ap-\\nplication domains.\\nEmpirical evidence and theoretical results show\\nthat greater performance is achieved with deeper\\narchitectures, that is, long compositions of map-\\npings. As we saw in section Â§ 3.4, training such\\na model is challenging due to the vanishinggra-\\ndient, and multiple important technical contri-\\nbutions have mitigated this issue.\\n58', metadata={'source': 'rag/deep learning.pdf', 'page': 57}),\n",
       " Document(page_content='4.1 The notion of layer\\nWe call layers standard complex compounded\\ntensor operations that have been designed and\\nempirically identified as being generic and effi-\\ncient. They often incorporate trainable param-\\neters and correspond to a convenient level of\\ngranularity for designing and describing large\\ndeep models. The term is inherited from sim-\\nple multi-layer neural networks, even though\\nmodern models may take the form of a complex\\ngraph of such modules, incorporating multiple\\nparallel pathways.\\nÃ—K\\nXfgn=4Y\\n32Ã—324Ã—4\\nIn the following pages, I try to stick to the con-\\nvention for model depiction illustrated above:\\nâ€¢operators / layers are depicted as boxes,\\nâ€¢darker coloring indicates that they embed\\ntrainable parameters,\\nâ€¢non-default valued hyper-parameters are\\n59', metadata={'source': 'rag/deep learning.pdf', 'page': 58}),\n",
       " Document(page_content='added in blue on their right,\\nâ€¢a dashed outer frame with a multiplicative\\nfactor indicates that a group of layers is repli-\\ncated in series, each with its own set of trainable\\nparameters, if any, and\\nâ€¢in some cases, the dimension of their output is\\nspecified on the right when it differs from their\\ninput.\\nAdditionally, layers that have a complex internal\\nstructure are depicted with a greater height.\\n60', metadata={'source': 'rag/deep learning.pdf', 'page': 59}),\n",
       " Document(page_content='4.2 Linear layers\\nThe most important modules in terms of compu-\\ntation and number of parameters are the Linear\\nlayers. They benefit from decades of research\\nand engineering in algorithmic and chip design\\nfor matrix operations.\\nNote that the term â€œlinearâ€ in deep learning gen-\\nerally refers improperly to an affine operation,\\nwhich is the sum of a linear expression and a\\nconstant bias.\\nFully connected layers\\nThe most basic linear layer is the fully connected\\nlayer, parameterized by a trainable weight ma-\\ntrixWof size Dâ€²Ã—Dandbiasvectorbof dimen-\\nsionDâ€². It implements an affine transformation\\ngeneralized to arbitrary tensor shapes, where\\nthe supplementary dimensions are interpreted\\nas vector indexes. Formally, given an input X\\nof dimension D1Ã—Â·Â·Â·Ã— DKÃ—D, it computes an\\noutput Yof dimension D1Ã—Â·Â·Â·Ã— DKÃ—Dâ€²with\\nâˆ€d1,...,d K,\\nY[d1,...,d K] =WX[d1,...,d K]+b.\\nWhile at first sight such an affine operation\\n61', metadata={'source': 'rag/deep learning.pdf', 'page': 60}),\n",
       " Document(page_content='seems limited to geometric transformations such\\nas rotations, symmetries, and translations, it can\\nin fact do more than that. In particular, projec-\\ntions for dimension reduction or signal filtering,\\nbut also, from the perspective of the dot product\\nbeing a measure of similarity, a matrix-vector\\nproduct can be interpreted as computing match-\\ning scores between the queries, as encoded by\\nthe input vectors, and keys, as encoded by the\\nmatrix rows.\\nAs we saw in Â§ 3.3, the gradient descent starts\\nwith the parametersâ€™ random initialization. If\\nthis is done too naively, as seen in Â§ 3.4, the net-\\nwork may suffer from exploding or vanishing\\nactivations and gradients [Glorot and Bengio,\\n2010]. Deep learning frameworks implement ini-\\ntialization methods that in particular scale the\\nrandom parameters according to the dimension\\nof the input to keep the variance of the activa-\\ntions constant and prevent pathological behav-\\niors.\\nConvolutional layers\\nA linear layer can take as input an arbitrarily-\\nshaped tensor by reshaping it into a vector, as\\nlong as it has the correct number of coefficients.\\nHowever, such a layer is poorly adapted to deal-\\n62', metadata={'source': 'rag/deep learning.pdf', 'page': 61}),\n",
       " Document(page_content='Ï•\\nXY\\nÏˆY\\nXÏ•\\nXY\\nÏˆY\\nXÏ•\\nXY\\nÏˆY\\nX\\n...\\n1D convolution...\\n1D transposed\\nconvolution\\nFigure 4.1: A 1D convolution (left) takes as input\\naDÃ—Ttensor X, applies the same affine mapping\\nÏ•(Â·;w)to every sub-tensor of shape DÃ—K, and stores\\nthe resulting Dâ€²Ã—1tensors into Y. A 1D transposed\\nconvolution (right) takes as input a DÃ—Ttensor, ap-\\nplies the same affine mapping Ïˆ(Â·;w)to every sub-\\ntensor of shape DÃ—1, and sums the shifted resulting\\nDâ€²Ã—Ktensors. Both can process inputs of different\\nsizes.\\n63', metadata={'source': 'rag/deep learning.pdf', 'page': 62}),\n",
       " Document(page_content='XYÏ•\\n2D convolutionHWD\\nYXÏˆ\\n2D transposed\\nconvolution\\nFigure 4.2: A 2D convolution (left) takes as input a\\nDÃ—HÃ—Wtensor X, applies the same affine mapping\\nÏ•(Â·;w)to every sub-tensor of shape DÃ—KÃ—L, and\\nstores the resulting Dâ€²Ã—1Ã—1tensors into Y. A 2D\\ntransposed convolution (right) takes as input a DÃ—\\nHÃ—Wtensor, applies the same affine mapping Ïˆ(Â·;w)\\nto every DÃ—1Ã—1sub-tensor, and sums the shifted\\nresulting Dâ€²Ã—KÃ—Ltensors into Y.\\ning with large tensors, since the number of pa-\\nrameters and number of operations are propor-\\ntional to the product of the input and output\\ndimensions. For instance, to process an RGB\\nimage of size 256Ã—256as input and compute a\\nresult of the same size, it would require approxi-\\nmately 4Ã—1010parameters and multiplications.\\nBesides these practical issues, most of the high-\\ndimension signals are strongly structured. For\\n64', metadata={'source': 'rag/deep learning.pdf', 'page': 63}),\n",
       " Document(page_content='Ï•Y\\nXÏ•Y\\nX\\ns= 2...\\nStrideÏ•Y\\nX\\nd= 2\\nDilationÏ•Y\\nX\\np= 2\\nPadding\\nFigure 4.3: Beside its kernel size and number of input\\n/ output channels, a convolution admits three hyper-\\nparameters: the stride s(left) modulates the step size\\nwhen going through the input tensor, the padding p\\n(top right) specifies how many zero entries are added\\naround the input tensor before processing it, and the\\ndilation d(bottom right) parameterizes the index count\\nbetween coefficients of the filter.\\n65', metadata={'source': 'rag/deep learning.pdf', 'page': 64}),\n",
       " Document(page_content='instance, images exhibit short-term correlations\\nand statistical stationarity with respect to trans-\\nlation, scaling, and certain symmetries. This\\nis not reflected in the inductivebias of a fully\\nconnected layer, which completely ignores the\\nsignal structure.\\nTo leverage these regularities, the tool of choice\\nisconvolutional layers, which are also affine, but\\nprocess time-series or 2D signals locally, with\\nthe same operator everywhere.\\nA1Dconvolution is mainly defined by three hy-\\nper-parameters: its kernelsizeK, its number of\\ninput channels D, its number of output chan-\\nnelsDâ€², and by the trainable parameters wof an\\naffine mapping Ï•(Â·;w) :RDÃ—Kâ†’RDâ€²Ã—1.\\nIt can process any tensor Xof size DÃ—Twith\\nTâ‰¥K, and applies Ï•(Â·;w)to every sub-tensor\\nof size DÃ—KofX, storing the results in a tensor\\nYof size Dâ€²Ã—(Tâˆ’K+1), as pictured in Figure\\n4.1 (left).\\nA2Dconvolution is similar but has a KÃ—Lker-\\nnel and takes as input a DÃ—HÃ—Wtensor (see\\nFigure 4.2, left).\\nBoth operators have for trainable parameters\\nthose of Ï•that can be envisioned as Dâ€²filters\\n66', metadata={'source': 'rag/deep learning.pdf', 'page': 65}),\n",
       " Document(page_content='of size DÃ—KorDÃ—KÃ—Lrespectively, and a\\nbiasvector of dimension Dâ€².\\nSuch a layer is equiv ariant to translation, mean-\\ning that if the input signal is translated, the out-\\nput is similarly transformed. This property re-\\nsults in a desirable inductivebias when dealing\\nwith a signal whose distribution is invariant to\\ntranslation.\\nThey also admit three additional hyper-parame-\\nters, illustrated on Figure 4.3:\\nâ€¢Thepadding specifies how many zero coeffi-\\ncients should be added around the input tensor\\nbefore processing it, particularly to maintain the\\ntensor size when the kernel size is greater than\\none. Its default value is 0.\\nâ€¢Thestride specifies the step size used when go-\\ning through the input, allowing one to reduce the\\noutput size geometrically by using large steps.\\nIts default value is 1.\\nâ€¢Thedilation specifies the index count between\\nthe filter coefficients of the local affine opera-\\ntor. Its default value is 1, and greater values\\ncorrespond to inserting zeros between the coef-\\nficients, which increases the filter / kernel size\\nwhile keeping the number of trainable parame-\\n67', metadata={'source': 'rag/deep learning.pdf', 'page': 66}),\n",
       " Document(page_content='HW\\nModel depth\\nFigure 4.4: Given an activation in a series of convolu-\\ntion layers, here in red, its receptivefield is the area in\\nthe input signal, in blue, that modulates its value. Each\\nintermediate convolutional layer increases the width\\nand height of that area by roughly those of the kernel.\\nters unchanged.\\nExcept for the number of channels, a convo-\\nlutionâ€™s output is usually smaller than its in-\\nput. In the 1D case without padding nor di-\\nlation, if the input is of size T, the kernel of\\nsizeK, and the stride is S, the output is of size\\nTâ€²= (Tâˆ’K)/S+1.\\nGiven an activation computed by a convolutional\\nlayer, or the vector of values for all the channels\\nat a certain location, the portion of the input\\n68', metadata={'source': 'rag/deep learning.pdf', 'page': 67}),\n",
       " Document(page_content='signal that it depends on is called its receptive\\nfield (see Figure 4.4). One of the HÃ—Wsub-\\ntensors corresponding to a single channel of a\\nDÃ—HÃ—Wactivation tensor is called an activa-\\ntion map.\\nConvolutions are used to recombine information,\\ngenerally to reduce the spatial size of the rep-\\nresentation, in exchange for a greater number\\nof channels, which translates into a richer local\\nrepresentation. They can implement differential\\noperators such as edge-detectors, or template\\nmatching mechanisms. A succession of such lay-\\ners can also be envisioned as a compositional and\\nhierarchical representation [Zeiler and Fergus,\\n2014], or as a diffusion process in which infor-\\nmation can be transported by half the kernel size\\nwhen passing through a layer.\\nA converse operation is the trans posed convo-\\nlution that also consists of a localized affine op-\\nerator, defined by similar hyper and trainable\\nparameters as the convolution, but which, for\\ninstance, in the 1D case, applies an affine map-\\npingÏˆ(Â·;w) :RDÃ—1â†’RDâ€²Ã—K,to every DÃ—1\\nsub-tensor of the input, and sums the shifted\\nDâ€²Ã—Kresulting tensors to compute its output.\\nSuch an operator increases the size of the signal\\nand can be understood intuitively as a synthe-\\n69', metadata={'source': 'rag/deep learning.pdf', 'page': 68}),\n",
       " Document(page_content='sis process (see Figure 4.1, right, and Figure 4.2,\\nright).\\nA series of convolutional layers is the usual ar-\\nchitecture for mapping a large-dimension signal,\\nsuch as an image or a sound sample, to a low-\\ndimension tensor. This can be used, for instance,\\nto get class scores for classification or a com-\\npressed representation. Transposed convolution\\nlayers are used the opposite way to build a large-\\ndimension signal from a compressed representa-\\ntion, either to assess that the compressed repre-\\nsentation contains enough information to recon-\\nstruct the signal or for synthesis, as it is easier\\nto learn a density model over a low-dimension\\nrepresentation. We will revisit this in Â§ 5.2.\\n70', metadata={'source': 'rag/deep learning.pdf', 'page': 69}),\n",
       " Document(page_content='4.3 Activation functions\\nIf a network were combining only linear com-\\nponents, it would itself be a linear operator,\\nso it is essential to have non-linearoperations.\\nThese are implemented in particular with activa-\\ntion functions, which are layers that transform\\neach component of the input tensor individually\\nthrough a mapping, resulting in a tensor of the\\nsame shape.\\nThere are many different activation functions,\\nbut the most used is the RectifiedLinearUnit\\n(ReLU) [Glorot et al., 2011], which sets nega-\\ntive values to zero and keeps positive values un-\\nchanged (see Figure 4.5, top right):\\nrelu(x) =(\\n0ifx <0,\\nxotherwise .\\nGiven that the core training strategy of deep-\\nlearning relies on the gradient, it may seem prob-\\nlematic to have a mapping that is not differen-\\ntiable at zero and constant on half the real line.\\nHowever, the main property gradient descent\\nrequires is that the gradient is informative on\\naverage. Parameter initialization and data nor-\\nmalization make half of the activations positive\\n71', metadata={'source': 'rag/deep learning.pdf', 'page': 70}),\n",
       " Document(page_content='Tanh ReLU\\nLeaky ReLU GELU\\nFigure 4.5: Activation functions.\\nwhen the training starts, ensuring that this is the\\ncase.\\nBefore the generalization of ReLU, the standard\\nactivation function was the hyperbolic tangent\\n(Tanh, see Figure 4.5, top left) which saturates\\nexponentially fast on both the negative and pos-\\nitive sides, aggravating the vanishing gradient.\\nOther popular activation functions follow the\\nsame idea of keeping positive values unchanged\\nand squashing the negative values. Leaky ReLU\\n[Maas et al., 2013] applies a small positive multi-\\n72', metadata={'source': 'rag/deep learning.pdf', 'page': 71}),\n",
       " Document(page_content='plying factor to the negative values (see Figure\\n4.5, bottom left):\\nleakyrelu( x) =(\\naxifx <0,\\nxotherwise .\\nAnd GELU [Hendrycks and Gimpel, 2016] is de-\\nfined using the cumulative distribution function\\nof the Gaussian distribution, that is:\\ngelu(x) =xP(Zâ‰¤x),\\nwhere Zâˆ¼ð’©(0,1). It roughly behaves like a\\nsmooth ReLU (see Figure 4.5, bottom right).\\nThe choice of an activation function, in partic-\\nular among the variants of ReLU, is generally\\ndriven by empirical performance.\\n73', metadata={'source': 'rag/deep learning.pdf', 'page': 72}),\n",
       " Document(page_content='4.4 Pooling\\nA classical strategy to reduce the signal size is to\\nuse a pooling operation that combines multiple\\nactivations into one that ideally summarizes the\\ninformation. The most standard operation of this\\nclass is the max pooling layer, which, similarly\\nto convolution, can operate in 1D and 2D and is\\ndefined by a kernelsize.\\nIn its standard form, this layer computes the\\nmaximum activation per channel, over non-\\noverlapping sub-tensors of spatial size equal to\\nthe kernel size. These values are stored in a re-\\nsult tensor with the same number of channels\\nas the input, and whose spatial size is divided\\nby the kernel size. As with the convolution, this\\noperator has three hyper-parameters: padding,\\nstride, and dilation, with the stride being equal\\nto the kernel size by default. A smaller stride\\nresults in a larger resulting tensor, following the\\nsame formula as for convolutions (see Â§ 4.2).\\nThe max operation can be intuitively interpreted\\nas a logical disjunction, or, when it follows a\\nseries of convolutional layers that compute lo-\\ncal scores for the presence of parts, as a way\\nof encoding that at least one instance of a part\\nis present. It loses precise location, making it\\n74', metadata={'source': 'rag/deep learning.pdf', 'page': 73}),\n",
       " Document(page_content='Y\\nXmaxY\\nXmaxY\\nXmax\\n...\\n1D max pooling\\nFigure 4.6: A 1D max pooling takes as input a DÃ—T\\ntensor X, computes the max over non-overlapping 1Ã—\\nLsub-tensors (in blue) and stores the resulting values\\n(in red) in a DÃ—(T/L)tensor Y.\\n75', metadata={'source': 'rag/deep learning.pdf', 'page': 74}),\n",
       " Document(page_content='invariant to local deformations.\\nA standard alternative is the averagepooling\\nlayer that computes the average instead of the\\nmaximum over the sub-tensors. This is a linear\\noperation, whereas max pooling is not.\\n76', metadata={'source': 'rag/deep learning.pdf', 'page': 75}),\n",
       " Document(page_content='4.5 Dropout\\nSome layers have been designed to explicitly\\nfacilitate training or improve the learned repre-\\nsentations.\\nOne of the main contributions of that sort was\\ndropout [Srivastava et al., 2014]. Such a layer\\nhas no trainable parameters, but one hyper-\\nparameter, p, and takes as input a tensor of arbi-\\ntrary shape.\\nIt is usually switched off during testing, in which\\ncase its output is equal to its input. When it is ac-\\ntive, it has a probability pof setting to zero each\\nactivation of the input tensor independently, and\\nit re-scales all the activations by a factor of1\\n1âˆ’p\\nto maintain the expected value unchanged (see\\nFigure 4.7).\\nThe motivation behind dropout is to favor\\nmeaningful individual activation and discourage\\ngroup representation. Since the probability that\\na group of kactivations remains intact through\\na dropout layer is (1âˆ’p)k, joint representations\\nbecome unreliable, making the training proce-\\ndure avoid them. It can also be seen as a noise\\ninjection that makes the training more robust.\\nWhen dealing with images and 2D tensors, the\\n77', metadata={'source': 'rag/deep learning.pdf', 'page': 76}),\n",
       " Document(page_content='11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111 0\\n0\\n00 0\\n00 00\\n0\\n0Ã— Ã—1\\n1âˆ’p\\nTrain TestY\\nXY\\nX\\nFigure 4.7: Dropout can process a tensor of arbitrary\\nshape. During training (left), it sets activations at ran-\\ndom to zero with probability pand applies a multiply-\\ning factor to keep the expected values unchanged. Dur-\\ning test (right), it keeps all the activations unchanged.\\nshort-term correlation of the signals and the re-\\nsulting redundancy negate the effect of dropout,\\nsince activations set to zero can be inferred from\\ntheir neighbors. Hence, dropout for 2D tensors\\nsets entire channels to zero instead of individual\\nactivations (see Figure 4.8).\\nAlthough dropout is generally used to improve\\ntraining and is inactive during inference, it can\\nbe used in certain setups as a randomization\\nstrategy, for instance, to estimate empirically\\nconfidence scores [Gal and Ghahramani, 2015].\\n78', metadata={'source': 'rag/deep learning.pdf', 'page': 77}),\n",
       " Document(page_content='BD\\nH,W\\n1101001 Ã— Ã—1\\n1âˆ’p\\nTrain Test\\nFigure 4.8: 2D signals such as images generally exhibit\\nstrong short-term correlation and individual activa-\\ntions can be inferred from their neighbors. This redun-\\ndancy nullifies the effect of the standard unstructured\\ndropout, so the usual dropout layer for 2D tensors drops\\nentire channels instead of individual values.\\n79', metadata={'source': 'rag/deep learning.pdf', 'page': 78}),\n",
       " Document(page_content='4.6 Normalizing layers\\nAn important class of operators to facilitate the\\ntraining of deep architectures are the normaliz-\\ninglayers, which force the empirical mean and\\nvariance of groups of activations.\\nThe main layer in that family is batch normal-\\nization [Ioffe and Szegedy, 2015], which is the\\nonly standard layer to process batches instead\\nof individual samples. It is parameterized by a\\nhyper-parameter Dand two series of trainable\\nscalar parameters Î²1,...,Î² DandÎ³1,...,Î³ D.\\nGiven a batch of Bsamples x1,...,x Bof dimen-\\nsionD, it first computes for each of the Dcom-\\nponents an empirical mean Ë†mdand variance Ë†vd\\nacross the batch:\\nË†md=1\\nBBX\\nb=1xb,d\\nË†vd=1\\nBBX\\nb=1(xb,dâˆ’Ë†md)2,\\nfrom which it computes for every component\\nxb,da normalized value zb,d, with empirical\\nmean 0and variance 1, and from it the final\\nresult value yb,dwith mean Î²dand standard de-\\n80', metadata={'source': 'rag/deep learning.pdf', 'page': 79}),\n",
       " Document(page_content='BD\\nH,W\\n(Â· âˆ’Ë†md)/âˆšË†vd+ÏµÎ³dÂ·+Î²d\\n(Â· âˆ’Ë†mb)/âˆšË†vb+ÏµÎ³d,h,wÂ·+Î²d,h,w\\nbatchnorm layernorm\\nFigure 4.9: Batch normalization (left) normalizes in\\nmean and variance each group of activations for a\\ngiven d, and scales/shifts that same group of activation\\nwith learned parameters for each d. Layer normaliza-\\ntion (right) normalizes each group of activations for a\\ncertain b, and scales/shifts each group of activations\\nfor a given d,h,w with learned parameters indexed by\\nthe same.\\n81', metadata={'source': 'rag/deep learning.pdf', 'page': 80}),\n",
       " Document(page_content='viation Î³d:\\nâˆ€b, z b,d=xb,dâˆ’Ë†mdâˆšË†vd+Ïµ\\nyb,d=Î³dzb,d+Î²d.\\nBecause this normalization is defined across a\\nbatch, it is done only during training. During\\ntesting, the layer transforms individual samples\\naccording to the Ë†mds and Ë†vds estimated with a\\nmoving average over the full training set, which\\nboils down to a fixed affine transformation per\\ncomponent.\\nThe motivation behind batch normalization was\\nto avoid that a change in scaling in an early layer\\nof the network during training impacts all the\\nlayers that follow, which then have to adapt their\\ntrainable parameters accordingly. Although the\\nactual mode of action may be more complicated\\nthan this initial motivation, this layer consider-\\nably facilitates the training of deep models.\\nIn the case of 2D tensors, to follow the prin-\\nciple of convolutional layers of processing all\\nlocations similarly, the normalization is done\\nper-channel across all 2D positions, and Î²and\\nÎ³remain vectors of dimension Dso that the\\nscaling/shift does not depend on the 2D posi-\\ntion. Hence, if the tensor to be processed is\\n82', metadata={'source': 'rag/deep learning.pdf', 'page': 81}),\n",
       " Document(page_content='of shape BÃ—DÃ—HÃ—W, the layer computes\\n( Ë†md,Ë†vd), ford= 1,...,D from the correspond-\\ningBÃ—HÃ—Wslice, normalizes it accordingly,\\nand finally scales and shifts its components with\\nthe trainable parameters Î²dandÎ³d.\\nSo, given a BÃ—Dtensor, batch normalization\\nnormalizes it across band scales/shifts it ac-\\ncording to d, which can be implemented as a\\ncomponent-wise product by Î³and a sum with\\nÎ². Given a BÃ—DÃ—HÃ—Wtensor, it normal-\\nizes across b,h,w and scales/shifts according to\\nd(see Figure 4.9, left).\\nThis can be generalized depending on these di-\\nmensions. For instance, layer normalization [Ba\\net al., 2016] computes moments and normalizes\\nacross all components of individual samples, and\\nscales and shifts components individually (see\\nFigure 4.9, right). So, given a BÃ—Dtensor, it\\nnormalizes across dand scales/shifts also accord-\\ning to the same. Given a BÃ—DÃ—HÃ—Wtensor,\\nit normalizes it across d,h,w and scales/shifts\\naccording to the same.\\nContrary to batch normalization, since it pro-\\ncesses samples individually, layer normalization\\nbehaves the same during training and testing.\\n83', metadata={'source': 'rag/deep learning.pdf', 'page': 82}),\n",
       " Document(page_content='4.7 Skip connections\\nAnother technique that mitigates the vanishing\\ngradient and allows the training of deep archi-\\ntectures are skip connections [Long et al., 2014;\\nRonneberger et al., 2015]. They are not layers\\nper se, but an architectural design in which out-\\nputs of some layers are transported as-is to other\\nlayers further in the model, bypassing process-\\ning in between. This unmodified signal can be\\nconcatenated or added to the input of the layer\\nthe connection branches into (see Figure 4.10). A\\nparticular type of skip connections are the resid -\\nualconnections which combine the signal with\\na sum, and usually skip only a few layers (see\\nFigure 4.10, right).\\nThe most desirable property of this design is to\\nensure that, even in the case of gradient-killing\\nprocessing at a certain stage, the gradient will\\nstill propagate through the skip connections.\\nResidual connections, in particular, allow for the\\nbuilding of deep models with up to several hun-\\ndred layers, and key models, such as the resid ual\\nnetworks [He et al., 2015] in computer vision\\n(see Â§ 5.2), and the Trans form ers [Vaswani et al.,\\n2017] in natural language processing (see Â§ 5.3),\\nare entirely composed of blocks of layers with\\nresidual connections.\\n84', metadata={'source': 'rag/deep learning.pdf', 'page': 83}),\n",
       " Document(page_content='Â·Â·Â·f(1)f(2)f(3)f(4)f(5)f(6)f(7)f(8)Â·Â·Â·\\nÂ·Â·Â·f(1)f(2)f(3)f(4)f(5)f(6)Â·Â·Â·\\nÂ·Â·Â·f(1)f(2)+f(3)f(4)+Â·Â·Â·\\nFigure 4.10: Skip connections, highlighted in red on this\\nfigure, transport the signal unchanged across multiple\\nlayers. Some architectures (center) that downscale and\\nre-upscale the representation size to operate at multiple\\nscales, have skip connections to feed outputs from the\\nearly parts of the network to later layers operating at\\nthe same scales [Long et al., 2014; Ronneberger et al.,\\n2015]. The residual connections (right) are a special\\ntype of skip connections that sum the original signal\\nto the transformed one, and usually bypass at most a\\nhandful of layers [He et al., 2015].\\n85', metadata={'source': 'rag/deep learning.pdf', 'page': 84}),\n",
       " Document(page_content='Their role can also be to facilitate multi-scale rea-\\nsoning in models that reduce the signal size be-\\nfore re-expanding it, by connecting layers with\\ncompatible sizes, for instance for semanticseg-\\nmentation (see Â§ 6.4). In the case of residual\\nconnections, they may also facilitate learning\\nby simplifying the task to finding a differential\\nimprovement instead of a full update.\\n86', metadata={'source': 'rag/deep learning.pdf', 'page': 85}),\n",
       " Document(page_content='4.8 Attention layers\\nIn many applications, there is a need for an op-\\neration able to combine local information at lo-\\ncations far apart in a tensor. For instance, this\\ncould be distant details for coherent and realistic\\nimagesynthesis, or words at different positions\\nin a paragraph to make a grammatical or seman-\\ntic decision in NaturalLanguage Processing.\\nFully connected layers cannot process large-\\ndimension signals, nor signals of variable size,\\nandconvolutional layers are not able to prop-\\nagate information quickly. Strategies that ag-\\ngregate the results of convolutions, for instance,\\nby averaging them over large spatial areas, suf-\\nfer from mixing multiple signals into a limited\\nnumber of dimensions.\\nAttention layers specifically address this prob-\\nlem by computing an attention score for each\\ncomponent of the resulting tensor to each com-\\nponent of the input tensor, without locality con-\\nstraints, and averaging the features across the\\nfull tensor accordingly [Vaswani et al., 2017].\\nEven though they are substantially more com-\\nplicated than other layers, they have become a\\nstandard element in many recent models. They\\nare, in particular, the key building block of Trans -\\n87', metadata={'source': 'rag/deep learning.pdf', 'page': 86}),\n",
       " Document(page_content='KQ\\nVYA A\\nComputes Aq,1,...,A q,NKV Computes Yq\\nFigure 4.11: The attention operator can be inter-\\npreted as matching every query Qqwith all the\\nkeysK1,...,K NKVto get normalized attention scores\\nAq,1,...,A q,NKV(left, and Equation 4.1), and then av-\\neraging the values V1,...,V NKVwith these scores to\\ncompute the resulting Yq(right, and Equation 4.2).\\nform ers, the dominant architecture for Large\\nLanguage Models. See Â§ 5.3 and Â§ 7.1.\\nAttention operator\\nGiven\\nâ€¢a tensor Qofqueries of size NQÃ—DQK,\\nâ€¢a tensor Kofkeys of size NKVÃ—DQK, and\\nâ€¢a tensor Vofvalues of size NKVÃ—DV,\\ntheattention operator computes a tensor\\nY= att( Q,K,V )\\nof dimension NQÃ—DV. To do so, it first com-\\nputes for every query index qand every key in-\\n88', metadata={'source': 'rag/deep learning.pdf', 'page': 87}),\n",
       " Document(page_content='dexkan attention score Aq,kas the softargmax\\nof the dot products between the query Qqand\\nthe keys:\\nAq,k=exp\\x10\\n1âˆš\\nDQKQqÂ·Kk\\x11\\nP\\nlexp\\x10\\n1âˆš\\nDQKQqÂ·Kl\\x11, (4.1)\\nwhere the scaling factor1âˆš\\nDQKkeeps the range\\nof values roughly unchanged even for large DQK.\\nThen a retrieved value is computed for each\\nquery by averaging the values according to the\\nattention scores (see Figure 4.11):\\nYq=X\\nkAq,kVk. (4.2)\\nSo if a query Qnmatches one key Kmfar more\\nthan all the others, the corresponding attention\\nscore An,mwill be close to one, and the retrieved\\nvalue Ynwill be the value Vmassociated to that\\nkey. But, if it matches several keys equally, then\\nYnwill be the average of the associated values.\\nThis can be implemented as\\natt(Q,K,V ) = softargmax\\x12QKT\\nâˆš\\nDQK\\x13\\n| {z }\\nAV.\\n89', metadata={'source': 'rag/deep learning.pdf', 'page': 88}),\n",
       " Document(page_content='Q K VTÃ—expâŠ™1/Î£kdropoutA\\nMÃ—Y\\nMasked\\nsoftargmax\\nFigure 4.12: The attention operator Y= att( Q,K,V )\\ncomputes first an attention matrix Aas the per-query\\nsoftargmax of QKT, which may be masked by a con-\\nstant matrix Mbefore the normalization. This atten-\\ntion matrix goes through a dropout layer before being\\nmultiplied by Vto get the resulting Y. This operator\\ncan be made causal by taking Mfull of 1s below the\\ndiagonal and zeros above.\\n90', metadata={'source': 'rag/deep learning.pdf', 'page': 89}),\n",
       " Document(page_content='This operator is usually extended in two ways,\\nas depicted in Figure 4.12. First, the attention\\nmatrix can be masked by multiplying it before\\nthe softargmax normalization by a Boolean ma-\\ntrixM. This allows, for instance, to make the\\noperator causal by taking Mfull of 1s below the\\ndiagonal and zero above, preventing Yqfrom de-\\npending on keys and values of indices kgreater\\nthanq. Second, the attention matrix is processed\\nby a dropout layer (see Â§ 4.5) before being multi-\\nplied by V, providing the usual benefits during\\ntraining.\\nSince a dot product is computed for every\\nquery/key pair, the computational cost of the at-\\ntention operator is quadratic with the sequence\\nlength. This happens to be problematic, as some\\nof the applications of these methods require to\\nprocess sequences of tens of thousands, or more\\ntokens. Multiple attempts have been made at\\nreducing this cost, for instance by combining a\\ndense attention to a local window with a long-\\nrange sparse attention [Beltagy et al., 2020], or\\nlinearizing the operator to benefit from the asso-\\nciativity of the matrix product and compute the\\nkey-value product before multiplying with the\\nqueries [Katharopoulos et al., 2020].\\n91', metadata={'source': 'rag/deep learning.pdf', 'page': 90}),\n",
       " Document(page_content='Ã—WQ\\n1Ã—WK\\n1Ã—WV\\n1att\\nÃ—WQ\\n2Ã—WK\\n2Ã—WV\\n2att\\nÃ—WQ\\n3Ã—WK\\n3Ã—WV\\n3att\\nÃ—WQ\\n4Ã—WK\\n4Ã—WV\\n4att\\nÃ—WQ\\nHÃ—WK\\nHÃ—WV\\nHatt(Y1| Â·Â·Â· |YH)\\nXQXKXVÃ—WOY\\nÃ—H\\nFigure 4.13: The Multi-head Attention layer applies\\nfor each of its h= 1,...,H heads a parametrized lin-\\near transformation to individual elements of the input\\nsequences XQ,XK,XVto get sequences Q,K,V that\\nare processed by the attention operator to compute Yh.\\nThese Hsequences are concatenated along features,\\nand individual elements are passed through one last\\nlinear operator to get the final result sequence Y.\\n92', metadata={'source': 'rag/deep learning.pdf', 'page': 91}),\n",
       " Document(page_content='Multi-head Attention Layer\\nThis parameterless attention operator is the key\\nelement in the Multi -Head Attention layer de-\\npicted in Figure 4.13. The structure of this layer\\nis defined by several hyper-parameters: a num-\\nberHof heads, and the shapes of three series of\\nHtrainable weight matrices\\nâ€¢WQof size HÃ—DÃ—DQK,\\nâ€¢WKof size HÃ—DÃ—DQK, and\\nâ€¢WVof size HÃ—DÃ—DV,\\nto compute respectively the queries, the keys,\\nand the values from the input, and a final weight\\nmatrix WOof size HDVÃ—Dto aggregate the\\nper-head results.\\nIt takes as input three sequences\\nâ€¢XQof size NQÃ—D,\\nâ€¢XKof size NKVÃ—D, and\\nâ€¢XVof size NKVÃ—D,\\nfrom which it computes, for h= 1,...,H ,\\nYh= att\\x00\\nXQWQ\\nh,XKWK\\nh,XVWV\\nh\\x01\\n.\\nThese sequences Y1,...,Y Hare concatenated\\nalong the feature dimension and each individual\\nelement of the resulting sequence is multiplied\\n93', metadata={'source': 'rag/deep learning.pdf', 'page': 92}),\n",
       " Document(page_content='byWOto get the final result:\\nY= (Y1| Â·Â·Â· |YH)WO.\\nAs we will see in Â§ 5.3 and in Figure 5.6, this\\nlayer is used to build two model sub-structures:\\nself-attention blocks, in which the three input\\nsequences XQ,XK, and XVare the same, and\\ncross -attention blocks, where XKandXVare\\nthe same.\\nIt is noteworthy that the attention operator,\\nand consequently the multi-head attention layer\\nwhen there is no masking, is invariant to a per-\\nmutation of the keys and values, and equiv ariant\\nto a permutation of the queries, as it would per-\\nmute the resulting tensor similarly.\\n94', metadata={'source': 'rag/deep learning.pdf', 'page': 93}),\n",
       " Document(page_content='4.9 Token embedding\\nIn many situations, we need to convert discrete\\ntokens into vectors. This can be done with an em-\\nbedding layer, which consists of a lookup table\\nthat directly maps integers to vectors.\\nSuch a layer is defined by two hyper-parame-\\nters: the number Nof possible token values,\\nand the dimension Dof the output vectors, and\\none trainable NÃ—Dweight matrix M.\\nGiven as input an integer tensor Xof dimen-\\nsionD1Ã—Â·Â·Â·Ã— DKand values in {0,...,N âˆ’1}\\nsuch a layer returns a real-valued tensor Yof\\ndimension D1Ã—Â·Â·Â·Ã— DKÃ—Dwith\\nâˆ€d1,...,d K,\\nY[d1,...,d K] =M[X[d1,...,d K]].\\n95', metadata={'source': 'rag/deep learning.pdf', 'page': 94}),\n",
       " Document(page_content='4.10 Positional encoding\\nWhile the processing of a fully connected layer\\nis specific to both the positions of the features\\nin the input tensor and to the positions of the\\nresulting activations in the output tensor, con-\\nvolutional layers and Multi -Head Attention lay-\\ners are oblivious to the absolute position in the\\ntensor. This is key to their strong invariance and\\ninductivebias, which is beneficial for dealing\\nwith a stationary signal.\\nHowever, this can be an issue in certain situ-\\nations where proper processing has to access\\nthe absolute positioning. This is the case, for\\ninstance, for image synthesis, where the statis-\\ntics of a scene are not totally stationary, or in\\nnatural language processing, where the relative\\npositions of words strongly modulate the mean-\\ning of a sentence.\\nThe standard way of coping with this problem\\nis to add or concatenate to the feature represen-\\ntation, at every position, a positional encoding,\\nwhich is a feature vector that depends on the po-\\nsition in the tensor. This positional encoding can\\nbe learned as other layer parameters, or defined\\nanalytically.\\nFor instance, in the original Trans former model,\\n96', metadata={'source': 'rag/deep learning.pdf', 'page': 95}),\n",
       " Document(page_content='for a series of vectors of dimension D, Vaswani\\net al. [2017] add an encoding of the sequence\\nindex as a series of sines and cosines at various\\nfrequencies:\\npos-enc[ t,d] =\\uf8f1\\n\\uf8f2\\n\\uf8f3sin\\x10\\nt\\nTd/D\\x11\\nifdâˆˆ2N\\ncos\\x10\\nt\\nT(dâˆ’1)/D\\x11\\notherwise,\\nwithT= 104.\\n97', metadata={'source': 'rag/deep learning.pdf', 'page': 96}),\n",
       " Document(page_content='Chapter 5\\nArchitectures\\nThe field of deep learning has developed over\\nthe years for each application domain multiple\\ndeep architectures that exhibit good trade-offs\\nwith respect to multiple criteria of interest: e.g.\\nease of training, accuracy of prediction, memory\\nfootprint, computational cost, scalability.\\n98', metadata={'source': 'rag/deep learning.pdf', 'page': 97}),\n",
       " Document(page_content='5.1 Multi-Layer Perceptrons\\nThe simplest deep architecture is the Multi -Layer\\nPerceptron ( MLP), which takes the form of a\\nsuccession of fully connected layers separated\\nbyactivationfunctions. See an example in Figure\\n5.1. For historical reasons, in such a model, the\\nnumber of hiddenlayers refers to the number of\\nlinear layers, excluding the last one.\\nA key theoretical result is the universalapproxi-\\nmation theorem [Cybenko, 1989] which states\\nthat, if the activation function Ïƒis continuous\\nXfully-connrelufully-connrelufully-connY\\n5025102\\nHidden\\nlayers\\nFigure 5.1: This multi-layer perceptron takes as input\\na one-dimensional tensor of size 50, is composed of\\nthree fully connected layers with outputs of dimensions\\nrespectively 25,10, and 2, the two first followed by\\nReLU layers.\\n99', metadata={'source': 'rag/deep learning.pdf', 'page': 98}),\n",
       " Document(page_content='and not polynomial, any continuous function f\\ncan be approximated arbitrarily well uniformly\\non a compact domain, which is bounded and\\ncontains its boundary, by a model of the form\\nl2â—¦Ïƒâ—¦l1where l1andl2are affine. Such a model\\nis aMLP with a single hidden layer, and this\\nresult implies that it can approximate anything\\nof practical value. However, this approximation\\nholds if the dimension of the first linear layerâ€™s\\noutput can be arbitrarily large.\\nIn spite of their simplicity, MLPs remain an im-\\nportant tool when the dimension of the signal\\nto be processed is not too large.\\n100', metadata={'source': 'rag/deep learning.pdf', 'page': 99}),\n",
       " Document(page_content='5.2 Convolutional networks\\nThe standard architecture for processingimages\\nis aconvolutional network, or convnet, that com-\\nbines multiple convolutional layers, either to re-\\nduce the signal size before it can be processed by\\nfully connected layers, or to output a 2D signal\\nalso of large size.\\nLeNet-like\\nThe original LeNet model for image classifica-\\ntion [LeCun et al., 1998] combines a series of 2D\\nconvolutional layers and max pooling layers that\\nplay the role of feature extractor, with a series of\\nfully connected layers which act as a MLP and\\nperform the classification per se (see Figure 5.2).\\nThis architecture was the blueprint for many\\nmodels that share its structure and are simply\\nlarger, such as AlexNet [Krizhevsky et al., 2012]\\nor the VGG family [Simonyan and Zisserman,\\n2014].\\nResidual networks\\nStandard convolutional neural networks that fol-\\nlow the architecture of the LeNet family are not\\neasily extended to deep architectures and suffer\\nfrom the vanishing gradient problem. The resid -\\n101', metadata={'source': 'rag/deep learning.pdf', 'page': 100}),\n",
       " Document(page_content='Xconv-2dk=5maxpoolk=3reluconv-2dk=5maxpoolk=2relureshapefully-connrelufully-connË†P(Y)\\n1Ã—28Ã—2832Ã—24Ã—2432Ã—8Ã—864Ã—4Ã—464Ã—2Ã—225620010\\nFeature\\nextractorClassifier\\nFigure 5.2: Example of a small LeNet-like network for\\nclassifying 28Ã—28grayscale images of handwritten\\ndigits [LeCun et al., 1998]. Its first half is convolutional,\\nand alternates convolutional layers per se and max\\npooling layers, reducing the signal dimension from\\n28Ã—28scalars to 256. Its second half processes this\\n256-dimensional feature vector through a one hidden\\nlayer perceptron to compute 10logit scores correspond-\\ning to the ten possible digits.\\n102', metadata={'source': 'rag/deep learning.pdf', 'page': 101}),\n",
       " Document(page_content='Xconv-2dk=1batchnormreluconv-2dk=3p=1batchnormreluconv-2dk=1batchnorm+reluY\\nCÃ—HÃ—WC\\n2Ã—HÃ—WCÃ—HÃ—WCÃ—HÃ—W\\nFigure 5.3: A residual block.\\nualnetworks, or ResNets, proposed by He et al.\\n[2015] explicitly address the issue of the vanish-\\ning gradient with resid ualconnections (see Â§ 4.7),\\nwhich allow hundreds of layers. They have be-\\ncome standard architectures for computer vision\\napplications, and exist in multiple versions de-\\npending on the number of layers. We are going\\nto look in detail at the architecture of the ResNet -\\n50 for classification.\\nAs other ResNets, it is composed of a series of\\n103', metadata={'source': 'rag/deep learning.pdf', 'page': 102}),\n",
       " Document(page_content='Xconv-2dk=1batchnormreluconv-2dk=3s=S p=1batchnormreluconv-2dk=1batchnorm+reluY\\nconv-2dk=1s=Sbatchnorm\\nCÃ—HÃ—WC\\nSÃ—HÃ—WC\\nSÃ—H\\nSÃ—W\\nS4C\\nSÃ—H\\nSÃ—W\\nS4C\\nSÃ—H\\nSÃ—W\\nS\\nFigure 5.4: A downscaling residual block. It admits a\\nhyper-parameter S, the stride of the first convolution\\nlayer, which modulates the reduction of the tensor size.\\nresid ualblocks, each combining several convolu-\\ntional layers,batch norm layers, and ReLU layers,\\nwrapped in a residual connection. Such a block\\nis pictured in Figure 5.3.\\nA key requirement for high performance with\\nreal images is to propagate a signal with a large\\nnumber of channels, to allow for a rich repre-\\nsentation. However, the parameter count of a\\n104', metadata={'source': 'rag/deep learning.pdf', 'page': 103}),\n",
       " Document(page_content='Ã—2Ã—3Ã—5Ã—2\\nXconv-2dk=7s=2p=3batchnormrelumaxpoolk=3s=2p=1dresblock\\nS=1resblockdresblock\\nS=2resblockdresblock\\nS=2resblockdresblock\\nS=2resblockavgpoolk=7reshapefully-connË†P(Y)\\n3Ã—224Ã—22464Ã—112Ã—11264Ã—56Ã—56256Ã—56Ã—56512Ã—28Ã—281024Ã—14Ã—142048Ã—7Ã—72048Ã—1Ã—120481000\\nFigure 5.5: Structure of the ResNet-50 [He et al., 2015].\\n105', metadata={'source': 'rag/deep learning.pdf', 'page': 104}),\n",
       " Document(page_content='convolutional layer, and its computational cost,\\nare quadratic with the number of channels. This\\nresidual block mitigates this problem by first re-\\nducing the number of channels with a 1Ã—1con-\\nvolution, then operating spatially with a 3Ã—3\\nconvolution on this reduced number of chan-\\nnels, and then upscaling the number of channels,\\nagain with a 1Ã—1convolution.\\nThe network reduces the dimensionality of the\\nsignal to finally compute the logits for the clas-\\nsification. This is done thanks to an architec-\\nture composed of several sections, each starting\\nwith a down scalingresid ualblock that halves\\nthe height and width of the signal, and doubles\\nthe number of channels, followed by a series\\nof residual blocks. Such a downscaling resid-\\nual block has a structure similar to a standard\\nresidual block, except that it requires a residual\\nconnection that changes the tensor shape. This\\nis achieved with a 1Ã—1convolution with a stride\\nof two (see Figure 5.4).\\nThe overall structure of the ResNet-50 is pre-\\nsented in Figure 5.5. It starts with a 7Ã—7convo-\\nlutional layer that converts the three-channel in-\\nput image to a 64-channel image of half the size,\\nfollowed by four sections of residual blocks. Sur-\\nprisingly, in the first section, there is no down-\\n106', metadata={'source': 'rag/deep learning.pdf', 'page': 105}),\n",
       " Document(page_content='scaling, only an increase of the number of chan-\\nnels by a factor of 4. The output of the last resid-\\nual block is 2048Ã—7Ã—7, which is converted to a\\nvector of dimension 2048 by an average pooling\\nof kernel size 7Ã—7, and then processed through\\na fully-connected layer to get the final logits,\\nhere for 1000 classes.\\n107', metadata={'source': 'rag/deep learning.pdf', 'page': 106}),\n",
       " Document(page_content='5.3 Attention models\\nAs stated in Â§ 4.8, many applications, particu-\\nlarly from natural language processing, benefit\\ngreatly from models that include attention mech-\\nanisms. The architecture of choice for such tasks,\\nwhich has been instrumental in recent advances\\nin deep learning, is the Trans former proposed\\nby Vaswani et al. [2017].\\nTransformer\\nThe original Transformer, pictured in Figure 5.7,\\nwas designed for sequence-to-sequence transla-\\ntion. It combines an encoder that processes the\\ninput sequence to get a refined representation,\\nand an autoregressive decoder that generates\\neach token of the result sequence, given the en-\\ncoderâ€™s representation of the input sequence and\\nthe output tokens generated so far.\\nAs the residual convolutional networks of Â§ 5.2,\\nboth the encoder and the decoder of the Trans-\\nformer are sequences of compounded blocks\\nbuilt with residual connections.\\nâ€¢Thefeed-forward block, pictured at the top of\\nFigure 5.6 is a one hidden layer MLP, preceded\\nby a layer normalization. It can update represen-\\ntations at every position separately.\\n108', metadata={'source': 'rag/deep learning.pdf', 'page': 107}),\n",
       " Document(page_content='XQKVlayernormfully-conngelufully-conndropout+Y\\nXQKVlayernormQ K Vmha+Y\\nXQlayernormQ K Vmha+Y\\nXKV\\nFigure 5.6: Feed-forward block (top), self-attention\\nblock (bottom left) and cross -attention block (bottom\\nright). These specific structures proposed by Radford\\net al. [2018] differ slightly from the original architec-\\nture of Vaswani et al. [2017], in particular by having\\nthe layer normalization first in the residual blocks.\\n109', metadata={'source': 'rag/deep learning.pdf', 'page': 108}),\n",
       " Document(page_content='Ã—NÃ—N\\nX1,...,X Tembed+self-attffwZ1,...,Z T\\npos-enc0,Y1,...,Y Sâˆ’1embed+causal\\nself-attQ KVcross-attffwfully-connË†P(Y1),...,Ë†P(YS|Ys<S)\\npos-enc\\nEncoderDecoder\\nTTÃ—DTÃ—DSSÃ—DSÃ—DSÃ—V\\nFigure 5.7: Original encoder-decoder Trans former\\nmodel for sequence-to-sequence translation [Vaswani\\net al., 2017].\\n110', metadata={'source': 'rag/deep learning.pdf', 'page': 109}),\n",
       " Document(page_content='â€¢Theself-attention block, pictured on the bot-\\ntom left of Figure 5.6, is a Multi -Head Attention\\nlayer (see Â§ 4.8), that recombines information\\nglobally, allowing any position to collect infor-\\nmation from any other positions, preceded by\\nalayer normalization. This block can be made\\ncausal by using an adequate mask in the atten-\\ntion layer, as described in Â§ 4.8\\nâ€¢Thecross -attentionblock, pictured on the bot-\\ntom right of Figure 5.6, is similar except that it\\ntakes as input two sequences, one to compute\\nthe queries and one to compute the keys and\\nvalues.\\nThe encoder of the Transformer (see Figure\\n5.7, bottom), recodes the input sequence of dis-\\ncrete tokens X1,...X Twith an embedding layer\\n(see Â§ 4.9), and adds a positional encoding (see\\nÂ§ 4.10), before processing it with several self-\\nattention blocks to generate a refined represen-\\ntation Z1,...,Z T.\\nThe decoder (see Figure 5.7, top), takes as in-\\nput the sequence Y1,...,Y Sâˆ’1of result tokens\\nproduced so far, similarly recodes them through\\nan embedding layer, adds a positional encoding,\\nand processes it through alternating causal self-\\nattention blocks and cross-attention blocks to\\n111', metadata={'source': 'rag/deep learning.pdf', 'page': 110}),\n",
       " Document(page_content='Ã—N\\n0,X1,...,X Tâˆ’1embed+causal\\nself-attffwfully-connË†P(X1),...,Ë†P(XT|Xt<T)\\npos-enc\\nTTÃ—DTÃ—DTÃ—V\\nFigure 5.8: GPT model [Radford et al., 2018].\\nproduce the logits predicting the next tokens.\\nThese cross-attention blocks compute their keys\\nand values from the encoderâ€™s result represen-\\ntation Z1,...,Z T, which allows the resulting se-\\nquence to be a function of the original sequence\\nX1,...,X T.\\nAs we saw in Â§ 3.2 being causal ensures that\\nsuch a model can be trained by minimizing the\\ncross-entropy summed across the full sequence.\\nGenerative Pre-trained Transformer\\nTheGenerativePre-trained Trans former ( GPT)\\n[Radford et al., 2018, 2019], pictured in Figure 5.8\\n112', metadata={'source': 'rag/deep learning.pdf', 'page': 111}),\n",
       " Document(page_content='is a pure autoregressive model that consists of a\\nsuccession of causal self-attention blocks, hence\\na causal version of the original Transformer en-\\ncoder.\\nThis class of models scales extremely well, up\\nto hundreds of billions of trainable parameters\\n[Brown et al., 2020]. We will come back to their\\nuse for text generation in Â§ 7.1.\\nVision Transformer\\nTransformers have been put to use for image\\nclassification with the Vision Trans former ( ViT)\\nmodel [Dosovitskiy et al., 2020] (see Figure 5.9).\\nIt splits the three-channel input image into M\\npatches of resolution PÃ—P, which are then flat-\\ntened to create a sequence of vectors X1,...,X M\\nof shape MÃ—3P2. This sequence is multiplied\\nby a trainable matrix WEof shape 3P2Ã—Dto\\nmap it to an MÃ—Dsequence, to which is con-\\ncatenated one trainable vector E0. The resulting\\n(M+1)Ã—Dsequence E0,...,E Mis then pro-\\ncessed through multiple self-attention blocks.\\nSee Â§ 5.3 and Figure 5.6.\\nThe first element Z0in the resultant sequence,\\nwhich corresponds to E0and is not associated\\nwith any part of the image, is finally processed\\n113', metadata={'source': 'rag/deep learning.pdf', 'page': 112}),\n",
       " Document(page_content='Ã—N\\nE0,E1,...,E M+self-attffwZ0,Z1,...,Z Mfully-conngelufully-conngelufully-connË†P(Y)\\nÃ—WE\\nX1,...,X ME0pos-encMLP\\nreadout\\nImage\\nencoderMÃ—3P2(M+1)Ã—D(M+1)Ã—DDC\\nFigure 5.9: Vision Transformer model [Dosovitskiy\\net al., 2020].\\n114', metadata={'source': 'rag/deep learning.pdf', 'page': 113}),\n",
       " Document(page_content='by a two-hidden-layer MLP to get the final C\\nlogits. Such a token, added for a readout of a\\nclass prediction, was introduced by Devlin et al.\\n[2018] in the BERT model and is referred to as a\\nCLS token.\\n115', metadata={'source': 'rag/deep learning.pdf', 'page': 114}),\n",
       " Document(page_content='Part III\\nApplications\\n116', metadata={'source': 'rag/deep learning.pdf', 'page': 115}),\n",
       " Document(page_content='Chapter 6\\nPrediction\\nA first category of applications, such as face\\nrecognition, sentiment analysis, object detection,\\nor speech recognition, requires predicting an un-\\nknown value from an available signal.\\n117', metadata={'source': 'rag/deep learning.pdf', 'page': 116}),\n",
       " Document(page_content='6.1 Image denoising\\nA direct application of deep models to image\\nprocessing is to recover from degradation by\\nutilizing the redundancy in the statistical struc-\\nture of images. The petals of a sunflower in a\\ngrayscale picture can be colored with high confi-\\ndence, and the texture of a geometric shape such\\nas a table on a low-light, grainy picture can be\\ncorrected by averaging it over a large area likely\\nto be uniform.\\nAdenoisingautoencoder is a model that takes\\na degraded signal ËœXas input and computes an\\nestimate of the original signal X. For images, it\\nis a convolutional network that may integrate\\nskip-connections, in particular to combine repre-\\nsentations at the same resolution obtained early\\nand late in the model, as well as attention layers\\nto facilitate taking into account elements that\\nare far away from each other.\\nSuch a model is trained by collecting a large num-\\nber of clean samples paired with their degraded\\ninputs. The latter can be captured in degraded\\nconditions, such as low-light or inadequate fo-\\ncus, or generated algorithmically, for instance,\\nby converting the clean sample to grayscale, re-\\nducing its size, or aggressively compressing it\\n118', metadata={'source': 'rag/deep learning.pdf', 'page': 117}),\n",
       " Document(page_content='with a lossy compression method.\\nThe standard training procedure for denoising\\nautoencoders uses the MSE loss summed across\\nall pixels, in which case the model aims at com-\\nputing the best average clean picture, given the\\ndegraded one, that is E[X|ËœX]. This quantity\\nmay be problematic when Xis not completely\\ndetermined by ËœX, in which case some parts\\nof the generated signal may be an unrealistic,\\nblurry average.\\n119', metadata={'source': 'rag/deep learning.pdf', 'page': 118}),\n",
       " Document(page_content='6.2 Image classification\\nImage classification is the simplest strategy for\\nextracting semantics from an image and consists\\nof predicting a class from a finite, predefined\\nnumber of classes, given an input image.\\nThe standard models for this task are convolu-\\ntional networks, such as ResNets (see Â§ 5.2), and\\nattention-based models such as ViT (see Â§ 5.3).\\nThese models generate a vector of logits with as\\nmany dimensions as there are classes.\\nThe training procedure simply minimizes the\\ncross-entropy loss (see Â§ 3.1). Usually, perfor-\\nmance can be improved with data augmenta-\\ntion, which consists of modifying the training\\nsamples with hand-designed random transfor-\\nmations that do not change the semantic content\\nof the image, such as cropping, scaling, mirror-\\ning, or color changes.\\n120', metadata={'source': 'rag/deep learning.pdf', 'page': 119}),\n",
       " Document(page_content='6.3 Object detection\\nA more complex task for image understanding is\\nobjectdetection, in which the objective is, given\\nan input image, to predict the classes and posi-\\ntions of objects of interest.\\nAn object position is formalized as the four co-\\nordinates (x1,y1,x2,y2)of a rectangular bound-\\ning box, and the ground truth associated with\\neach training image is a list of such bounding\\nboxes, each labeled with the class of the object\\ncontained therein.\\nThe standard approach to solve this task, for in-\\nstance, by the SingleShot Detector (SSD) [Liu\\net al., 2015]), is to use a convolutional neural\\nnetwork that produces a sequence of image\\nrepresentations Zsof size DsÃ—HsÃ—Ws, s=\\n1,...,S , with decreasing spatial resolution HsÃ—\\nWsdown to 1Ã—1fors=S(see Figure 6.1). Each\\nof these tensors covers the input image in full, so\\ntheh,w indices correspond to a partitioning of\\nthe image lattice into regular squares that gets\\ncoarser when sincreases.\\nAs seen in Â§ 4.2, and illustrated in Figure 4.4,\\ndue to the succession of convolutional layers, a\\nfeature vector (Zs[0,h,w],...,Z s[Dsâˆ’1,h,w])\\nis a descriptor of an area of the image, called its\\n121', metadata={'source': 'rag/deep learning.pdf', 'page': 120}),\n",
       " Document(page_content='X\\nZ1\\nZ2ZSâˆ’1ZS\\n......\\nFigure 6.1: A convolutional object detector processes the\\ninput image to generate a sequence of representations\\nof decreasing resolutions. It computes for every h,w, at\\nevery scale s, a pre-defined number of bounding boxes\\nwhose centers are in the image area corresponding to\\nthat cell, and whose sizes are such that they fit in its\\nreceptive field. Each prediction takes the form of the\\nestimates (Ë†x1,Ë†x2,Ë†y1,Ë†y2), represented by the red boxes\\nabove, and a vector of C+1logits for the Cclasses of\\ninterest, and an additional â€œno objectâ€ class.\\n122', metadata={'source': 'rag/deep learning.pdf', 'page': 121}),\n",
       " Document(page_content='Figure 6.2: Examples of object detection with the Single-\\nShot Detector [Liu et al., 2015].\\n123', metadata={'source': 'rag/deep learning.pdf', 'page': 122}),\n",
       " Document(page_content='receptivefield, that is larger than this square but\\ncentered on it. This results in a non-ambiguous\\nmatching of any bounding box (x1,x2,y1,y2)to\\nas,h,w , determined respectively by max( x2âˆ’\\nx1,y2âˆ’y1),y1+y2\\n2, andx1+x2\\n2.\\nDetection is achieved by adding Sconvolutional\\nlayers, each processing a Zsand computing, for\\nevery tensor indices h,w, the coordinates of a\\nbounding box and the associated logits. If there\\nareCobject classes, there are C+1logits, the\\nadditional one standing for â€œno object.â€ Hence,\\neach additional convolution layer has 4+C+1\\noutput channels. The SSD algorithm in particu-\\nlar generates several bounding boxes per s,h,w ,\\neach dedicated to a hard-coded range of aspect\\nratios.\\nTraining sets for object detection are costly to\\ncreate, since the labeling with bounding boxes\\nrequires a slow human intervention. To mitigate\\nthis issue, the standard approach is to fine-tune\\na convolutional model that has been pre-trained\\non a large classification dataset such as VGG-16\\nfor the original SSD, and to replace its final fully-\\nconnected layers with additional convolutional\\nones. Surprisingly, models trained for classifica-\\ntion only learn feature representations that can\\nbe repurposed for object detection, even though\\n124', metadata={'source': 'rag/deep learning.pdf', 'page': 123}),\n",
       " Document(page_content='that task involves the regression of geometric\\nquantities.\\nDuring training, every ground-truth bounding\\nbox is associated with its s,h,w , and induces a\\nloss term composed of a cross-entropy loss for\\nthe logits, and a regression loss such as MSE\\nfor the bounding box coordinates. Every other\\ns,h,w free of bounding-box match induces a\\ncross-entropy only penalty to predict the class\\nâ€œno objectâ€.\\n125', metadata={'source': 'rag/deep learning.pdf', 'page': 124}),\n",
       " Document(page_content='6.4 Semantic segmentation\\nThe finest-grain prediction task for image under-\\nstanding is semanticsegmentation, which con-\\nsists of predicting, for each pixel, the class of the\\nobject to which it belongs. This can be achieved\\nwith a standard convolutional neural network\\nthat outputs a convolutional map with as many\\nchannels as classes, carrying the estimated logits\\nfor every pixel.\\nWhile a standard residual network, for instance,\\ncan generate a dense output of the same reso-\\nlution as its input, as for object detection, this\\ntask requires operating at multiple scales. This\\nis necessary so that any object, or sufficiently\\ninformative sub-part, regardless of its size, is\\ncaptured somewhere in the model by the feature\\nrepresentation at a single tensor position. Hence,\\nstandard architectures for this task downscale\\nthe image with a series of convolutional layers\\nto increase the receptive field of the activations,\\nand re-upscale it with a series of trans posed con-\\nvolutional layers, or other upscaling methods\\nsuch as bilinear interpolation, to make the pre-\\ndiction at high resolution.\\nHowever, a strict downscaling-upscaling archi-\\ntecture does not allow for operating at a fine\\n126', metadata={'source': 'rag/deep learning.pdf', 'page': 125}),\n",
       " Document(page_content='Figure 6.3: Semantic segmentation results with the\\nPyramid Scene Parsing Network [Zhao et al., 2016].\\ngrain when making the final prediction, since all\\nthe signal has been transmitted through a low-\\nresolution representation at some point. Models\\nthat apply such downscaling-upscaling serially\\nmitigate these issues with skip connections from\\nlayers at a certain resolution, before downscal-\\ning, to layers at the same resolution, after upscal-\\ning [Long et al., 2014; Ronneberger et al., 2015].\\nModels that do it in parallel, after a convolutional\\n127', metadata={'source': 'rag/deep learning.pdf', 'page': 126}),\n",
       " Document(page_content='backbone, concatenate the resulting multi-scale\\nrepresentation after upscaling, before making\\nthe final per-pixel prediction [Zhao et al., 2016].\\nTraining is achieved with a standard cross-\\nentropy summed over all the pixels. As for ob-\\nject detection, training can start from a network\\npre-trained on a large-scale image classification\\ndataset to compensate for the limited availability\\nof segmentation ground truth.\\n128', metadata={'source': 'rag/deep learning.pdf', 'page': 127}),\n",
       " Document(page_content='6.5 Speech recognition\\nSpeech recog nition consists of converting a\\nsound sample into a sequence of words. There\\nhave been plenty of approaches to this problem\\nhistorically, but a conceptually simple and recent\\none proposed by Radford et al. [2022] consists of\\ncasting it as a sequence-to-sequence translation\\nand then solving it with a standard attention-\\nbased Trans former, as described in Â§ 5.3.\\nTheir model first converts the sound signal into a\\nspectrogram, which is a one-dimensional series\\nTÃ—D, that encodes at every time step a vector\\nof energies in Dfrequency bands. The associ-\\nated text is encoded with the BPE tokenizer (see\\nÂ§ 3.2).\\nThe spectrogram is processed through a few 1D\\nconvolutional layers, and the resulting repre-\\nsentation is fed into the encoder of the Trans-\\nformer. The decoder directly generates a discrete\\nsequence of tokens, that correspond to one of\\nthe possible tasks considered during training.\\nMultiple objectives are considered: transcription\\nof English or non-English text, translation from\\nany language to English, or detection of non-\\nspeech sequences, such as background music or\\nambient noise.\\n129', metadata={'source': 'rag/deep learning.pdf', 'page': 128}),\n",
       " Document(page_content='This approach allows leveraging extremely large\\ndatasets that combine multiple types of sound\\nsources with diverse ground truths.\\nIt is noteworthy that even though the ultimate\\ngoal of this approach is to produce a transla-\\ntion as deterministic as possible given the input\\nsignal, it is formally the sampling of a text dis-\\ntribution conditioned on a sound sample, hence\\na synthesis process. The decoder is, in fact, ex-\\ntremely similar to the generative model of Â§ 7.1.\\n130', metadata={'source': 'rag/deep learning.pdf', 'page': 129}),\n",
       " Document(page_content='6.6 Text-image representations\\nA powerful approach to image understanding\\nconsists of learning consistent image and text\\nrepresentations, such that an image, or a textual\\ndescription of it, would be mapped to the same\\nfeature vector.\\nThe Contrastive Language -ImagePre-train ing\\n(CLIP) proposed by Radford et al. [2021] com-\\nbines an image encoder f, which is a ViT, and\\na text encoder g, which is a GPT. See Â§ 5.3 for\\nboth.\\nTo repurpose a GPT as a text encoder, instead of a\\nstandard autoregressive model, they add an â€œend\\nof sentenceâ€ token to the input sequence, and use\\nthe representation of this token in the last layer\\nas the embedding. Its dimension is between 512\\nand1024 , depending on the configuration.\\nThose two models are trained from scratch using\\na dataset of 400 million image-text pairs (ik,tk)\\ncollected from the internet. The training proce-\\ndure follows the standard mini-batch stochastic\\ngradient descent approach but relies on a con-\\ntrastive loss. The embeddings are computed for\\nevery image and every text of the Npairs in the\\nmini-batch, and a cosine similarity measure is\\ncomputed not only between text and image em-\\n131', metadata={'source': 'rag/deep learning.pdf', 'page': 130}),\n",
       " Document(page_content='beddings from each pair, but also across pairs, re-\\nsulting in an NÃ—Nmatrix of similarity scores:\\nlm,n=f(im)Â·g(tn), m= 1,...,N,n = 1,...,N.\\nThe model is trained with cross-entropy so that,\\nâˆ€nthe values l1,n,...,l N,ninterpreted as logit\\nscores predict n, and similarly for ln,1,...,l n,N.\\nThis means that âˆ€n,m, s.t.nÌ¸=mthe similarity\\nln,nis unambiguously greater than both ln,mand\\nlm,n.\\nWhen it has been trained, this model can be used\\nto do zero-shot prediction, that is, classifying a\\nsignal in the absence of training examples by\\ndefining a series of candidate classes with text\\ndescriptions, and computing the similarity of the\\nembedding of an image with the embedding of\\neach of those descriptions (see Figure 6.4).\\nAdditionally, since the textual descriptions are\\noften detailed, such a model has to capture a\\nricher representation of images and pick up cues\\nbeyond what is necessary for instance for classifi-\\ncation. This translates to excellent performance\\non challenging datasets such as ImageNet Adver-\\nsarial [Hendrycks et al., 2019] which was specifi-\\ncally designed to degrade or erase cues on which\\nstandard predictors rely.\\n132', metadata={'source': 'rag/deep learning.pdf', 'page': 131}),\n",
       " Document(page_content='Figure 6.4: The CLIP text-image embedding [Radford\\net al., 2021] allows for zero-shot prediction by predicting\\nwhich class description embedding is the most consis-\\ntent with the image embedding.\\n133', metadata={'source': 'rag/deep learning.pdf', 'page': 132}),\n",
       " Document(page_content='6.7 Reinforcement learning\\nMany problems, such as strategy games or\\nrobotic control, can be formalized with a discrete-\\ntime state process Stand reward process Rtthat\\ncan be modulated by choosing actions At. If\\nStisMarko vian, meaning that it carries alone\\nas much information about the future as all the\\npast states until that instant, such an object is a\\nMarko vian Decision Process ( MDP).\\nGiven an MDP, the objective is classically to find\\napolicyÏ€such that At=Ï€(St)maximizes the\\nexpectation of the return, which is an accumu-\\nlated discounted reward:\\nE\\uf8ee\\n\\uf8f0X\\ntâ‰¥0Î³tRt\\uf8f9\\n\\uf8fb,\\nfor a discount factor 0< Î³ < 1.\\nThis is the standard setup of Reinforce ment\\nLearn ing (RL), and it can be worked out by intro-\\nducing the optimal state-action value function\\nQ(s,a)which is the expected return if we exe-\\ncute action ain state s, and then follow the opti-\\nmalpolicy. It provides a means to compute the\\noptimal policy as Ï€(s) = argmaxaQ(s,a), and,\\nthanks to the Markovian assumption, it verifies\\n134', metadata={'source': 'rag/deep learning.pdf', 'page': 133}),\n",
       " Document(page_content='theBellman equa tion:\\nQ(s,a) = (6.1)\\nE\\x14\\nRt+Î³max\\naâ€²Q(St+1,aâ€²)\\x0c\\x0c\\x0c\\x0cSt=s,At=a\\x15\\n,\\nfrom which we can design a procedure to train\\na parametric model Q(Â·,Â·;w).\\nTo apply this framework to play classical Atari\\nvideo games, Mnih et al. [2015] use for Stthe con-\\ncatenation of the frame at time tand the three\\nthat precede, so that the Markovian assumption\\nis reasonable, and use for Qa model dubbed the\\nDeep Q-Network ( DQN), composed of two con-\\nvolutional layers and one fully connected layer\\nwith one output value per action, following the\\nclassical structure of a LeNet (see Â§ 5.2).\\nTraining is achieved by alternatively playing and\\nrecording episodes, and building mini-batches of\\ntuples (sn,an,rn,sâ€²n)âˆ¼(St,At,Rt,St+1)taken\\nacross stored episodes and time steps, and mini-\\nmizing\\nâ„’(w) =1\\nNNX\\nn=1(Q(sn,an;w)âˆ’yn)2(6.2)\\nwith one iteration of SGD, where yn=rnif this\\ntuple is the end of the episode, and yn=rn+\\nÎ³max aQ(sâ€²n,a; Â¯w)otherwise.\\n135', metadata={'source': 'rag/deep learning.pdf', 'page': 134}),\n",
       " Document(page_content='Frame numberValue\\nFigure 6.5: This graph shows the evolution of the state\\nvalue V(St) = max aQ(St,a)during a game of Break-\\nout. The spikes at time points (1) and (2) correspond to\\nclearing a brick, at time point (3) it is about to break\\nthrough to the top line, and at (4) it does, which ensures\\na high future reward [Mnih et al., 2015].\\nHere Â¯wis a constant copy of w, i.e. the gradient\\ndoes not propagate through it to w. This is nec-\\nessary since the target value in Equation 6.1 is\\nthe expectation of yn, while it is ynitself which\\nis used in Equation 6.2. Fixing winynresults in\\na better approximation of the desirable gradient.\\nA key issue is the policy used to collect episodes.\\nMnih et al. [2015] simply use the Ïµ-greedy strat-\\negy, which consists of taking an action com-\\npletely at random with probability Ïµ, and the\\noptimal action argmaxaQ(s,a)otherwise. In-\\njecting a bit of randomness is necessary to favor\\n136', metadata={'source': 'rag/deep learning.pdf', 'page': 135}),\n",
       " Document(page_content='exploration.\\nTraining is done with ten million frames corre-\\nsponding to a bit less than eight days of game-\\nplay. The trained network computes accurate\\nestimates of the state values (see Figure 6.5), and\\nreaches human performance on a majority of the\\n49 games used in the experimental validation.\\n137', metadata={'source': 'rag/deep learning.pdf', 'page': 136}),\n",
       " Document(page_content='Chapter 7\\nSynthesis\\nA second category of applications distinct from\\nprediction is synthesis. It consists of fitting a\\ndensity model to training samples and providing\\nmeans to sample from this model.\\n138', metadata={'source': 'rag/deep learning.pdf', 'page': 137}),\n",
       " Document(page_content='7.1 Text generation\\nThe standard approach to text synthesis is to\\nuse an attention-based, autoregressivemodel. A\\nvery successful model proposed by Radford et al.\\n[2018], is the GPT which we described in Â§ 5.3.\\nThis architecture has been used for very large\\nmodels, such as OpenAIâ€™s 175-billion-parameter\\nGPT-3 [Brown et al., 2020]. It is composed of 96\\nself-attention blocks, each with 96 heads, and\\nprocesses tokens of dimension 12,288, with a\\nhidden dimension of 49,512 in the MLPs of the\\nattention blocks.\\nWhen such a model is trained on a very large\\ndataset, it results in a Large Language Model\\n(LLM), which exhibits extremely powerful prop-\\nerties. Besides the syntactic and grammatical\\nstructure of the language, it has to integrate\\nvery diverse knowledge, e.g. to predict the word\\nfollowing â€œThe capital of Japan isâ€, â€œif water is\\nheated to 100 Celsius degrees it turns intoâ€, or\\nâ€œbecause her puppy was sick, Jane wasâ€.\\nThis results in particular in the ability to solve\\nfew-shot prediction, where only a handful of\\ntraining examples are available, as illustrated\\nin Figure 7.1. More surprisingly, when given a\\ncarefully crafted prompt, it can exhibit abilities\\n139', metadata={'source': 'rag/deep learning.pdf', 'page': 138}),\n",
       " Document(page_content='I: I love apples, O: positive, I: music is my passion, O:\\npositive, I: my job is boring, O: negative, I: frozen pizzas\\nare awesome, O: positive,\\nI: I love apples, O: positive, I: music is my passion, O:\\npositive, I: my job is boring, O: negative, I: frozen pizzas\\ntaste like cardboard, O: negative,\\nI: water boils at 100 degrees, O: physics, I: the square\\nroot of two is irrational, O: mathematics, I: the set of\\nprime numbers is infinite, O: mathematics, I: gravity is\\nproportional to the mass, O: physics,\\nI: water boils at 100 degrees, O: physics, I: the square\\nroot of two is irrational, O: mathematics, I: the set of\\nprime numbers is infinite, O: mathematics, I: squares\\nare rectangles, O: mathematics,\\nFigure 7.1: Examples of few-shot prediction with a 120\\nmillion parameter GPT model from Hugging Face. In\\neach example, the beginning of the sentence was given\\nas a prompt, and the model generated the part in bold.\\nfor question answering, problem solving, and\\nchain -of-thought that appear eerily close to high-\\nlevel reasoning [Chowdhery et al., 2022; Bubeck\\net al., 2023].\\nDue to these remarkable capabilities, these mod-\\nels are sometimes called foun dation models\\n[Bommasani et al., 2021].\\nHowever, even though it integrates a very large\\nbody of knowledge, such a model may be inad-\\n140', metadata={'source': 'rag/deep learning.pdf', 'page': 139}),\n",
       " Document(page_content='equate for practical applications, in particular\\nwhen interacting with human users. In many\\nsituations, one needs responses that follow the\\nstatistics of a helpful dialog with an assistant.\\nThis differs from the statistics of available large\\ntraining sets, which combine novels, encyclope-\\ndias, forum messages, and blog posts.\\nThis discrepancy is addressed by fine-tuning\\nsuch a language model (see Â§ 3.6). The current\\ndominant strategy is Reinforce ment Learn ing\\nfrom Human Feed back ( RLHF) [Ouyang et al.,\\n2022], which consists of creating small labeled\\ntraining sets by asking users to either write\\nresponses or provide ratings of generated re-\\nsponses. The former can be used as-is to fine-\\ntune the language model, and the latter can be\\nused to train a reward network that predicts\\nthe rating and use it as a target to fine-tune the\\nlanguage model with a standard Reinforce ment\\nLearn ing approach.\\n141', metadata={'source': 'rag/deep learning.pdf', 'page': 140}),\n",
       " Document(page_content='7.2 Image generation\\nMultiple deep methods have been developed to\\nmodel and sample from a high-dimensional den-\\nsity. A powerful approach for imagesynthesis\\nrelies on inverting a diffusion process. Such a\\ngenerative model is referred to, somehow incor-\\nrectly, as a diffusion model.\\nThe principle consists of defining analytically\\na process that gradually degrades any sample,\\nand consequently transforms the complex and\\nunknown density of the data into a simple and\\nwell-known density such as a normal, and train-\\ning a deep architecture to invert this degradation\\nprocess [Ho et al., 2020].\\nGiven a fixed T, the diffusion process defines a\\nprobability distribution over series of T+1im-\\nages as follows: sample x0uniformly from the\\ndataset, and then sequentially sample xt+1âˆ¼\\np(xt+1|xt),t= 0,...,Tâˆ’1, where the condi-\\ntional distribution pis defined analytically and\\nsuch that it gradually erases the structure that\\nwas in x0. The setup should degrade the signal\\nso much that the distribution p(xT)has a known\\nanalytical form which can be sampled.\\nFor instance, Ho et al. [2020] normalize the data\\nto have a mean of 0and a variance of 1, and their\\n142', metadata={'source': 'rag/deep learning.pdf', 'page': 141}),\n",
       " Document(page_content='xT\\nx0\\nFigure 7.2: Image synthesis with denoising diffusion\\n[Ho et al., 2020]. Each sample starts as a white noise\\nxT(top), and is gradually de-noised by sampling iter-\\natively xtâˆ’1|xtâˆ¼ð’©(xt+f(xt,t;w),Ïƒt).\\n143', metadata={'source': 'rag/deep learning.pdf', 'page': 142}),\n",
       " Document(page_content='diffusion process consists of adding a bit of white\\nnoise and re-normalizing the variance to 1. This\\nprocess exponentially reduces the importance of\\nx0, andxtâ€™s density can rapidly be approximated\\nwith a normal.\\nThe denoiser fis a deep architecture that\\nshould model and allow sampling from\\nf(xtâˆ’1,xt,t;w)â‰ƒp(xtâˆ’1|xt). It can be shown,\\nthanks to a variational bound, that if this\\none-step reverse process is accurate enough,\\nsampling xTâˆ¼p(xT)and denoising Tsteps\\nwithfresults in x0that follows p(x0).\\nTraining fcan be achieved by generating a large\\nnumber of sequences x(n)\\n0,...,x(n)\\nT, picking a tn\\nin each, and maximizing\\nX\\nnlogf\\x10\\nx(n)\\ntnâˆ’1,x(n)\\ntn,tn;w\\x11\\n.\\nGiven their diffusion process, Ho et al. [2020]\\nhave a denoising of the form:\\nxtâˆ’1|xtâˆ¼ð’©(xt+f(xt,t;w);Ïƒt), (7.1)\\nwhere Ïƒtis defined analytically.\\nIn practice, such a model initially hallucinates\\nstructures by pure luck in the random noise, and\\n144', metadata={'source': 'rag/deep learning.pdf', 'page': 143}),\n",
       " Document(page_content='then gradually builds more elements that emerge\\nfrom the noise by reinforcing the most likely\\ncontinuation of the image obtained thus far.\\nThis approach can be extended to text-\\nconditioned synthesis, to generate images\\nthat match a description. For instance, Nichol\\net al. [2021] add to the mean of the denoising\\ndistribution of Equation 7.1 a bias that goes in\\nthe direction of increasing the CLIP matching\\nscore (see Â§ 6.6) between the produced image\\nand the conditioning text description.\\n145', metadata={'source': 'rag/deep learning.pdf', 'page': 144}),\n",
       " Document(page_content='Chapter 8\\nThe Compute Schism\\nThe scale of deep architectures is critical to their\\nperformance and, as we saw in Â§ 3.7, Large Lan-\\nguage Models in particular may require amounts\\nof memory and computation that greatly exceed\\nthose of consumer hardware.\\nWhile training such a model from scratch re-\\nquires resources available only to large corpora-\\ntions or public bodies, techniques have been de-\\nveloped to allow inference and adaptation to spe-\\ncific tasks under strong resource constraints. Al-\\nlowing to run models locally instead of through\\na provider may be highly desirable for cost or\\nconfidentiality reasons.\\n146', metadata={'source': 'rag/deep learning.pdf', 'page': 145}),\n",
       " Document(page_content='8.1 Prompt Engineering\\nThe simplest strategy to specialize or improve a\\nLarge Language Model with a limited computa-\\ntional budget is to use prompt engineering, that\\nis, to carefully craft the beginning of the text se-\\nquence to bias the autoregressive process [Sahoo\\net al., 2024]. This approach moves a part of the\\ninformation traditionally encoded in the modelâ€™s\\nparameters to the input.\\nWe saw in Â§ 7.1 a simple example of few-shot\\nprediction, to use an LLM for a text classification\\ntask without fine-tuning. A long and sophisti-\\ncated prompt allows generalizing this strategy\\nto complex tasks.\\nSince the promptâ€™s role is to leverage the â€œgoodâ€\\nbiases that were present in the training set, it\\nbenefits from surprising strategies such as stat-\\ning that the response is generated by a skilled\\nprofessional [Xu et al., 2023].\\nThe context size of a language model, that is,\\nthe number of tokens it can operate on, directly\\nmodulates the quantity of information that can\\nbe provided in the prompt. This is mostly con-\\nstrained by the computational cost of standard\\nattention models, which is quadratic with the\\ncontext size (see Â§ 4.8).\\n147', metadata={'source': 'rag/deep learning.pdf', 'page': 146}),\n",
       " Document(page_content='Q: Gina has 105 beans, she gives 23 beans to Bob, and\\nprepares a soup with 53 beans. How many beans are\\nleft? A: There are 29 beans left.\\nQ: I prepare 53 pancakes, eat 5 of them and give 7 to\\nGina. I then prepare 26 more. How many pancakes are\\nleft? A: 27 pancakes are left.\\nQ: Gina has 105 beans, she gives 23 beans to Bob, and\\nprepares a soup with 53 beans. How many beans are\\nleft? A: Letâ€™s proceed step by step: Gina has 105 beans,\\nshe gives 23 beans to Bob (82 left), and prepares a soup\\nwith 53 beans (29 left). So there are 29 beans left.\\nQ: I prepare 53 pancakes, eat 5 of them and give 7 to\\nGina. I then prepare 26 more. How many pancakes are\\nleft? A: Letâ€™s proceed step by step: 53 pancakes, eat 5\\nof them (48 left), give 7 to Gina (41 left), prepare\\n26 more (67 left). So there are 67 pancakes left.\\nFigure 8.1: Example of a chain-of-thought to improve\\nthe response of the Llama-3-8B base model. In the\\ntwo examples, the beginning of the text in normal font\\nis the prompt, and the generated part is indicated in\\nbold. The generation without chain-of-thought (top)\\nleads to an incorrect answer, while the generation with\\nit (bottom) generates a correct answer, by explicitly\\nproducing multiple simple arithmetic operations.\\n148', metadata={'source': 'rag/deep learning.pdf', 'page': 147}),\n",
       " Document(page_content='Chain of Thought\\nA remarkable type of prompting aims at making\\nthe model generate intermediate steps before\\ngenerating the response itself.\\nSuch a chain -of-thought is composed of succes-\\nsive steps that are simpler, hence have been bet-\\nter modeled during training, and are predicted\\nmore deterministically [Wei et al., 2022; Kojima\\net al., 2022]. See Figure 8.1 for an example.\\nRetrieval-Augmented Generation\\nPrompt engineering can also be put to use to\\nconnect a language model to an external knowl-\\nedge base. It plays the role of a smart interface\\nthat allows the end user to formulate questions\\nin natural language and get back a response that\\ncombines information that is not encoded in the\\nmodelâ€™s parameters [Lewis et al., 2020].\\nFor such Retrieval -Augmented Generation\\n(RAG), an embedding model is used to retrieve\\ndocuments whose embedding is correlated to\\nthat of the userâ€™s query. Then, a prompt is con-\\nstructed by joining these retrieved documents\\nwith instructions to combine them, and the\\ngenerative model produces the response to the\\nuser.\\n149', metadata={'source': 'rag/deep learning.pdf', 'page': 148}),\n",
       " Document(page_content='8.2 Quantization\\nAlthough training or generating multiple\\nstreams can benefit from high-end parallel\\ncomputing devices, deployment of a Large\\nLanguage Model for individual use requires\\ngenerally single-stream inference, which is\\nbounded by memory size and speed far more\\nthan by computation.\\nAs stated in Â§ 2.1, parameters, activations, and\\ngradients are usually encoded with 32or16bits.\\nThe precision it provides is necessary for train-\\ning, to allow gradual changes to accumulate.\\nHowever, since activations are the sums of many\\nterms, quan tization during inference is mitigated\\nby an averaging effect. This is even more true\\nwith large architectures, and models quantized\\ndown to 6or4bits per parameter exhibit remark-\\nable performance. Additionally to reducing the\\nmemory footprint, quantization also improves\\ninference speed significantly.\\nThis has motivated the development of soft-\\nware to quantize existing models with Post-\\nTrain ingQuan tization, and run them in single-\\nstream inference on consumer hardware, such\\nas llama.cpp [Llama.cpp, 2023]. This framework\\nimplements multiple formats, that apply specific\\n150', metadata={'source': 'rag/deep learning.pdf', 'page': 149}),\n",
       " Document(page_content='2 4 8 16 325.566.5\\nSize (Gigabytes)Perplexity\\nFigure 8.2: Perplexity of quantized versions of the lan-\\nguage models Llama-7B (blue) and 13B (red) [Touvron\\net al., 2023] on the wikitext corpus, as a function of\\nthe parametersâ€™ memory footprint. The crosses are the\\noriginal FP16 models and the dots correspond to differ-\\nent levels of quantization with llama.cpp [Llama.cpp,\\n2023].\\nquantization levels for the different weight matri-\\nces of a language model. For instance the quan-\\ntization may use more bits for the WVweights\\nof the attention blocks, and for the weights of\\nthe feed-forward blocks.\\nAn example of llama.cppâ€™s quantization is Q4_1.\\n151', metadata={'source': 'rag/deep learning.pdf', 'page': 150}),\n",
       " Document(page_content='It quantizes individually sub-blocks of 32entries\\nof the original weight matrix by storing for each\\na scaling factor dand a bias min the original\\nFP16 encoding, and encoding each entry xwith4\\nbits as a value qâˆˆ {0,...,24âˆ’1}. The resulting\\nde-quantized value being Ëœx=dq+m.\\nSuch a block was encoded originally as 32values\\nin FP16, hence 64bytes, while the quantized\\nversion needs 4bytes for qandmand32Â·4bits\\n=16bytes for the entries, hence a total of 20\\nbytes.\\nSuch an aggressive quantization surprisingly de-\\ngrades only marginally the performance of the\\nmodels, as illustrated on Figure 8.2.\\nAn alternative to Post-Training Quantization is\\nQuan tization-Aware Train ing that applies quan-\\ntization during the forward pass but keeps high-\\nprecision encoding of parameters and gradients,\\nand propagates the gradients during the back-\\nward pass as if there was no quantization [Ma\\net al., 2024].\\n152', metadata={'source': 'rag/deep learning.pdf', 'page': 151}),\n",
       " Document(page_content='8.3 Adapters\\nAs we saw in Â§ 3.6, fine-tuning is a key strat-\\negy to reuse pre-trained models. Since it aims\\nat making only minor changes to an existing\\nmodel, techniques have been developed that add\\ncomponents with few parameters, referred to\\nasadapters, to the pre-trained architecture, and\\nfreeze all the original parameters [Houlsby et al.,\\n2019].\\nThe current dominant method is the Low-Rank\\nAdap tation ( LoRA), which adds low-rank cor-\\nrections to some of the modelâ€™s weight matrices\\n[Hu et al., 2021].\\nFormally, given a linear operation of the form\\nXWT, where Xis aNÃ—Dtensor of activations\\nfor a batch of Nsamples, and Wis aCÃ—D\\nweight matrix, the LoRA adapter replaces this\\noperation with X(W+BA)T, where AandB\\nare two trainable matrices of size RÃ—Dand\\nCÃ—Rrespectively, with Râ‰ªmin(C,D), and\\nthe matrix Wis removed from the trainable pa-\\nrameters. The matrix Ais initialized with ran-\\ndom Gaussian values, and Bis set to zero, so\\nthat the fine-tuning starts with a model that com-\\nputes an output identical to that of the original\\none.\\n153', metadata={'source': 'rag/deep learning.pdf', 'page': 152}),\n",
       " Document(page_content='The total number of parameters to optimize with\\nthis approach is generally a few percent of the\\nnumber of parameters in the original model.\\nThe standard procedure to fine-tune a trans -\\nformer with such adapters is to change only the\\nweight matrices in the attention blocks, and to\\nkeep the MLP of the feed-forward blocks un-\\nchanged. The same strategy has been used suc-\\ncessfully to tune diffusion denoising models by\\nfine-tuning the attention blocks responsible for\\nthe text-based conditioning.\\nSince fine-tuning with LoRA adapters drastically\\nreduces the number of trainable parameters, it\\nreduces the memory footprint required by op-\\ntimizers such as Adam, which generally store\\ntwo running average per parameter to optimize.\\nAlso, it reduces slightly the computation during\\ntheback ward pass.\\nFor commercial applications that require a large\\nnumber of fine-tuned models, the ABpairs can\\nbe stored separately from the original model,\\nwhich has to be stored only once. And finally,\\ncontrary to other type of adapters, the modifica-\\ntions can be integrated into the original architec-\\nture, simply by adding ABtoW, resulting in an\\narchitecture and parameter count for inference\\n154', metadata={'source': 'rag/deep learning.pdf', 'page': 153}),\n",
       " Document(page_content='strictly identical to that of the base model.\\nWe saw that quantization degrade modelsâ€™ ac-\\ncuracy only marginally. However, gradient de-\\nscent requires high precision in both the gra-\\ndient and the trained parameters, to allow the\\naccumulation of small changes. The QLoRA ap-\\nproach combines a quantized base model and un-\\nquantized Low-Rank Adap tation to reduce the\\nmemory requirement even more [Dettmers et al.,\\n2023].\\n155', metadata={'source': 'rag/deep learning.pdf', 'page': 154}),\n",
       " Document(page_content='8.4 Model merging\\nAn alternative to the fine-tuning and prompting\\nmethods seen in the previous sections consists\\nof combining multiple models with diverse ca-\\npabilities into a single one, without additional\\ntraining.\\nModel merg ing relies on the compatibility be-\\ntween multiple fine-tuned versions of a base\\nmodel.\\nIlharco et al. [2022] showed that models obtained\\nby fine-tuning a CLIP base model on several im-\\nage classification data-sets can be combined in\\nthe parameter space, where they exhibit Task\\nArith metic properties.\\nFormally, let Î¸be the parameter vector of a pre-\\ntrained model, and for t= 1,...,T , letÎ¸tand\\nÏ„t=Î¸tâˆ’Î¸be respectively the parameters af-\\nter fine-tuning on task tand the corresponding\\nresidual. Experiments show that the model with\\nparameters Î¸+Ï„1+Â·Â·Â·+Ï„Texhibits multi-task\\ncapabilities. Similarly, subtracting a Ï„tdegrades\\nthe performance on the corresponding task.\\nMethods have been developed to reduce the in-\\nterference between the different residuals and\\nimprove the performance when the number of\\n156', metadata={'source': 'rag/deep learning.pdf', 'page': 155}),\n",
       " Document(page_content='tasks increases [Yadav et al., 2023; Yu et al., 2023].\\nAn alternative to merging models in parame-\\nter space is to recombine their layers. Akiba\\net al. [2024] combine merging the parameters\\nand re-combining layers, and rely on a stochas-\\ntic optimization to deal with the combinatorial\\nexplosion. Experiments with three fine-tuned\\nversions of Mistral-7B [Jiang et al., 2023] show\\nthat combining these two merging strategies out-\\nperforms both of them.\\n157', metadata={'source': 'rag/deep learning.pdf', 'page': 156}),\n",
       " Document(page_content='The Missing Bits\\nFor the sake of concision, this volume skips many\\nimportant topics, in particular:\\nRecurrent Neural Networks\\nBefore attention models showed greater perfor-\\nmance, Recurrent NeuralNetworks ( RNN) were\\nthe standard approach for dealing with temporal\\nsequences such as text or sound samples. These\\narchitectures possess an internal hiddenstate\\nthat gets updated each time a component of the\\nsequence is processed. Their main components\\nare layers such as LSTM [Hochreiter and Schmid-\\nhuber, 1997] or GRU [Cho et al., 2014].\\nTraining a recurrent architecture amounts to\\nunfolding it in time, which results in a long\\ncomposition of operators. This has historically\\nprompted the design of key techniques now used\\nfor deep architectures such as rectifiers and gat-\\ning, a form of skip connections which are modu-\\n158', metadata={'source': 'rag/deep learning.pdf', 'page': 157}),\n",
       " Document(page_content='lated dynamically.\\nOne of the key drawbacks of traditional recur-\\nrent architectures is that the structure of the\\ncomputation xt+1=f(xt)imposes to process\\nthe input sequence serially, which takes a time\\nproportional to T. In contrast, transformers, for\\ninstance, can take advantage of parallel compu-\\ntation, resulting in a constant time if enough\\ncomputing units are available.\\nThis is addressed by architectures such as QRNN\\n[Bradbury et al., 2016], S4 [Gu et al., 2021], or\\nMamba [Gu and Dao, 2023], whose recurrent op-\\nerations are affine so that the ftthemselves, and\\nconsequently the xt=ft(x0), can be computed\\nin parallel, resulting in a constant time if fdoes\\nnot depend on tandlogTotherwise, again if\\nenough parallel computing units are available.\\nAutoencoder\\nAnautoencoder is a model that maps an input\\nsignal, possibly of high dimension, to a low-\\ndimension latent representation, and then maps\\nit back to the original signal, ensuring that infor-\\nmation has been preserved. We saw it in Â§ 6.1\\nfor denoising, but it can also be used to auto-\\nmatically discover a meaningful low-dimension\\n159', metadata={'source': 'rag/deep learning.pdf', 'page': 158}),\n",
       " Document(page_content='parameterization of the data manifold.\\nTheVariational Autoencoder ( VAE) proposed by\\nKingma and Welling [2013] is a generative model\\nwith a similar structure. It imposes, through\\nthe loss, a pre-defined distribution on the latent\\nrepresentation. This allows, after training, the\\ngeneration of new samples by sampling the la-\\ntent representation according to this imposed\\ndistribution and then mapping back through the\\ndecoder.\\nGenerative Adversarial Networks\\nAnother approach to density modeling is the\\nGenerativeAdversarialNetworks ( GAN) intro-\\nduced by Goodfellow et al. [2014]. This method\\ncombines a generator, which takes a random in-\\nput following a fixed distribution as input and\\nproduces a structured signal such as an image,\\nand a discrim inator, which takes a sample as\\ninput and predicts whether it comes from the\\ntraining set or if it was generated by the genera-\\ntor.\\nTraining optimizes the discriminator to mini-\\nmize a standard cross-entropy loss, and the gen-\\nerator to maximize the discriminatorâ€™s loss. It\\ncan be shown that, at equilibrium, the gener-\\n160', metadata={'source': 'rag/deep learning.pdf', 'page': 159}),\n",
       " Document(page_content='ator produces samples indistinguishable from\\nreal data. In practice, when the gradient flows\\nthrough the discriminator to the generator, it\\ninforms the latter about the cues that the dis-\\ncriminator uses that need to be addressed.\\nGraph Neural Networks\\nMany applications require processing signals\\nwhich are not organized regularly on a grid. For\\ninstance, proteins, 3D meshes, geographic loca-\\ntions, or social interactions are more naturally\\nstructured as graphs. Standard convolutional\\nnetworks or even attention models are poorly\\nadapted to process such data, and the tool of\\nchoice for such a task is Graph NeuralNetworks\\n(GNN) [Scarselli et al., 2009].\\nThese models are composed of layers that com-\\npute activations at each vertex by combining\\nlinearly the activations located at its immediate\\nneighboring vertices. This operation is very sim-\\nilar to a standard convolution, except that the\\ndata structure does not reflect any geometrical\\ninformation associated with the feature vectors\\nthey carry.\\n161', metadata={'source': 'rag/deep learning.pdf', 'page': 160}),\n",
       " Document(page_content='Self-supervised training\\nAs stated in Â§ 7.1, even though they are trained\\nonly to predict the next word, Large Language\\nModels trained on large unlabeled datasets such\\nasGPT (see Â§ 5.3) are able to solve various tasks,\\nsuch as identifying the grammatical role of a\\nword, answering questions, or even translating\\nfrom one language to another [Radford et al.,\\n2019].\\nSuch models constitute one category of a larger\\nclass of methods that fall under the name of self-\\nsupervised learn ing, and try to take advantage\\nof unlabeled datasets [Balestriero et al., 2023].\\nThe key principle of these methods is to define a\\ntask that does not require labels but necessitates\\nfeature representations which are useful for the\\nreal task of interest, for which a small labeled\\ndataset exists. In computer vision, for instance,\\nimage features can be optimized so that they are\\ninvariant to data transformations that do not\\nchange the semantic content of the image, while\\nbeing statistically uncorrelated [Zbontar et al.,\\n2021].\\nIn both NLP and computer vision, a powerful\\ngeneric strategy is to train a model to recover\\nparts of the signal that have been masked [Devlin\\n162', metadata={'source': 'rag/deep learning.pdf', 'page': 161}),\n",
       " Document(page_content='et al., 2018; Zhou et al., 2021].\\n163', metadata={'source': 'rag/deep learning.pdf', 'page': 162}),\n",
       " Document(page_content='Bibliography\\nT. Akiba, M. Shing, Y. Tang, et al. Evolution-\\nary Optimization of Model Merging Recipes.\\nCoRR , abs/2403.13187, 2024. [pdf]. 157\\nJ. L. Ba, J. R. Kiros, and G. E. Hinton. Layer\\nNormalization. CoRR , abs/1607.06450, 2016.\\n[pdf]. 83\\nR. Balestriero, M. Ibrahim, V. Sobal, et al. A\\nCookbook of Self-Supervised Learning. CoRR ,\\nabs/2304.12210, 2023. [pdf]. 162\\nA. Baydin, B. Pearlmutter, A. Radul, and\\nJ. Siskind. Automatic differentiation in\\nmachine learning: a survey. CoRR ,\\nabs/1502.05767, 2015. [pdf]. 42\\nM. Belkin, D. Hsu, S. Ma, and S. Mandal. Rec-\\nonciling modern machine learning and the\\nbias-variance trade-off. CoRR , abs/1812.11118,\\n2018. [pdf]. 50\\n164', metadata={'source': 'rag/deep learning.pdf', 'page': 163}),\n",
       " Document(page_content='I. Beltagy, M. Peters, and A. Cohan. Long-\\nformer: The Long-Document Transformer.\\nCoRR , abs/2004.05150, 2020. [pdf]. 91\\nR. Bommasani, D. Hudson, E. Adeli, et al. On\\nthe Opportunities and Risks of Foundation\\nModels. CoRR , abs/2108.07258, 2021. [pdf].\\n140\\nJ. Bradbury, S. Merity, C. Xiong, and R. Socher.\\nQuasi-Recurrent Neural Networks. CoRR ,\\nabs/1611.01576, 2016. [pdf]. 159\\nT. Brown, B. Mann, N. Ryder, et al. Lan-\\nguage Models are Few-Shot Learners. CoRR ,\\nabs/2005.14165, 2020. [pdf]. 54, 113, 139\\nS. Bubeck, V. Chandrasekaran, R. Eldan, et al.\\nSparks of Artificial General Intelligence:\\nEarly experiments with GPT-4. CoRR ,\\nabs/2303.12712, 2023. [pdf]. 140\\nT. Chen, B. Xu, C. Zhang, and C. Guestrin. Train-\\ning Deep Nets with Sublinear Memory Cost.\\nCoRR , abs/1604.06174, 2016. [pdf]. 43\\nK. Cho, B. van Merrienboer, Ã‡. GÃ¼lÃ§ehre,\\net al. Learning Phrase Representations using\\nRNN Encoder-Decoder for Statistical Machine\\nTranslation. CoRR , abs/1406.1078, 2014. [pdf].\\n158\\n165', metadata={'source': 'rag/deep learning.pdf', 'page': 164}),\n",
       " Document(page_content='A. Chowdhery, S. Narang, J. Devlin, et al. PaLM:\\nScaling Language Modeling with Pathways.\\nCoRR , abs/2204.02311, 2022. [pdf]. 9, 54, 140\\nG. Cybenko. Approximation by superpositions\\nof a sigmoidal function. Mathematics of Con-\\ntrol, Signals, and Systems , 2(4):303â€“314, De-\\ncember 1989. [pdf]. 99\\nJ. Deng, W. Dong, R. Socher, et al. ImageNet:\\nA Large-Scale Hierarchical Image Database.\\nInConference on Computer Vision and Pattern\\nRecognition (CVPR) , 2009. [pdf]. 51\\nT. Dettmers, A. Pagnoni, A. Holtzman, and\\nL. Zettlemoyer. QLoRA: Efficient Finetuning\\nof Quantized LLMs. CoRR , abs/2305.14314,\\n2023. [pdf]. 155\\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova.\\nBERT: Pre-training of Deep Bidirectional\\nTransformers for Language Understanding.\\nCoRR , abs/1810.04805, 2018. [pdf]. 54, 115,\\n162\\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, et al.\\nAn Image is Worth 16x16 Words: Transform-\\ners for Image Recognition at Scale. CoRR ,\\nabs/2010.11929, 2020. [pdf]. 113, 114\\n166', metadata={'source': 'rag/deep learning.pdf', 'page': 165}),\n",
       " Document(page_content='K. Fukushima. Neocognitron: A self-organizing\\nneural network model for a mechanism of\\npattern recognition unaffected by shift in po-\\nsition. Biological Cybernetics , 36(4):193â€“202,\\nApril 1980. [pdf]. 2\\nY. Gal and Z. Ghahramani. Dropout as\\na Bayesian Approximation: Representing\\nModel Uncertainty in Deep Learning. CoRR ,\\nabs/1506.02142, 2015. [pdf]. 78\\nX. Glorot and Y. Bengio. Understanding the dif-\\nficulty of training deep feedforward neural\\nnetworks. In International Conference on Arti-\\nficial Intelligence and Statistics (AISTATS) , 2010.\\n[pdf]. 44, 62\\nX. Glorot, A. Bordes, and Y. Bengio. Deep Sparse\\nRectifier Neural Networks. In International\\nConference on Artificial Intelligence and Statis-\\ntics (AISTATS) , 2011. [pdf]. 71\\nA. Gomez, M. Ren, R. Urtasun, and R. Grosse.\\nThe Reversible Residual Network: Backprop-\\nagation Without Storing Activations. CoRR ,\\nabs/1707.04585, 2017. [pdf]. 43\\nI. J. Goodfellow, J. Pouget-Abadie, M. Mirza,\\net al. Generative Adversarial Networks. CoRR ,\\nabs/1406.2661, 2014. [pdf]. 160\\n167', metadata={'source': 'rag/deep learning.pdf', 'page': 166}),\n",
       " Document(page_content='A. Gu and T. Dao. Mamba: Linear-Time Se-\\nquence Modeling with Selective State Spaces.\\nCoRR , abs/2312.00752, 2023. [pdf]. 159\\nA. Gu, K. Goel, and C. RÃ©. Efficiently Modeling\\nLong Sequences with Structured State Spaces.\\nCoRR , abs/2111.00396, 2021. [pdf]. 159\\nK. He, X. Zhang, S. Ren, and J. Sun. Deep Resid-\\nual Learning for Image Recognition. CoRR ,\\nabs/1512.03385, 2015. [pdf]. 52, 84, 85, 103,\\n105\\nD. Hendrycks and K. Gimpel. Gaussian Error\\nLinear Units (GELUs). CoRR , abs/1606.08415,\\n2016. [pdf]. 73\\nD. Hendrycks, K. Zhao, S. Basart, et al. Natural\\nAdversarial Examples. CoRR , abs/1907.07174,\\n2019. [pdf]. 132\\nJ. Ho, A. Jain, and P. Abbeel. Denoising Diffusion\\nProbabilistic Models. CoRR , abs/2006.11239,\\n2020. [pdf]. 142, 143, 144\\nS. Hochreiter and J. Schmidhuber. Long Short-\\nTerm Memory. Neural Computation , 9(8):1735â€“\\n1780, 1997. [pdf]. 158\\nN. Houlsby, A. Giurgiu, S. Jastrzebski, et al.\\nParameter-Efficient Transfer Learning for\\nNLP. CoRR , abs/1902.00751, 2019. [pdf]. 153\\n168', metadata={'source': 'rag/deep learning.pdf', 'page': 167}),\n",
       " Document(page_content='E. Hu, Y. Shen, P. Wallis, et al. LoRA: Low-Rank\\nAdaptation of Large Language Models. CoRR ,\\nabs/2106.09685, 2021. [pdf]. 153\\nG. Ilharco, M. Ribeiro, M. Wortsman, et al. Edit-\\ning Models with Task Arithmetic. CoRR ,\\nabs/2212.04089, 2022. [pdf]. 156\\nS. Ioffe and C. Szegedy. Batch Normalization: Ac-\\ncelerating Deep Network Training by Reduc-\\ning Internal Covariate Shift. In International\\nConference on Machine Learning (ICML) , 2015.\\n[pdf]. 80\\nA. Jiang, A. Sablayrolles, A. Mensch, et al. Mistral\\n7B.CoRR , abs/2310.06825, 2023. [pdf]. 157\\nJ. Kaplan, S. McCandlish, T. Henighan, et al. Scal-\\ning Laws for Neural Language Models. CoRR ,\\nabs/2001.08361, 2020. [pdf]. 52, 53\\nA. Katharopoulos, A. Vyas, N. Pappas, and\\nF. Fleuret. Transformers are RNNs: Fast Au-\\ntoregressive Transformers with Linear Atten-\\ntion. In Proceedings of the International Confer-\\nence on Machine Learning (ICML) , pages 5294â€“\\n5303, 2020. [pdf]. 91\\nD. Kingma and J. Ba. Adam: A Method for\\nStochastic Optimization. CoRR , abs/1412.6980,\\n2014. [pdf]. 39\\n169', metadata={'source': 'rag/deep learning.pdf', 'page': 168}),\n",
       " Document(page_content='D. P. Kingma and M. Welling. Auto-Encoding\\nVariational Bayes. CoRR , abs/1312.6114, 2013.\\n[pdf]. 160\\nT. Kojima, S. Gu, M. Reid, et al. Large Lan-\\nguage Models are Zero-Shot Reasoners. CoRR ,\\nabs/2205.11916, 2022. [pdf]. 149\\nA. Krizhevsky, I. Sutskever, and G. Hinton. Ima-\\ngeNet Classification with Deep Convolutional\\nNeural Networks. In Neural Information Pro-\\ncessing Systems (NIPS) , 2012. [pdf]. 8, 101\\nY. LeCun, B. Boser, J. S. Denker, et al. Back-\\npropagation applied to handwritten zip code\\nrecognition. Neural Computation , 1(4):541â€“\\n551, 1989. [pdf]. 8\\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner.\\nGradient-based learning applied to document\\nrecognition. Proceedings of the IEEE , 86(11):\\n2278â€“2324, 1998. [pdf]. 101, 102\\nP. Lewis, E. Perez, A. Piktus, et al. Retrieval-\\nAugmented Generation for Knowledge-\\nIntensive NLP Tasks. CoRR , abs/2005.11401,\\n2020. [pdf]. 149\\nW. Liu, D. Anguelov, D. Erhan, et al. SSD: Single\\nShot MultiBox Detector. CoRR , abs/1512.02325,\\n2015. [pdf]. 121, 123\\n170', metadata={'source': 'rag/deep learning.pdf', 'page': 169}),\n",
       " Document(page_content='Llama.cpp. Llama.cpp git repository, June 2023.\\n[web]. 150, 151\\nJ. Long, E. Shelhamer, and T. Darrell. Fully Con-\\nvolutional Networks for Semantic Segmenta-\\ntion. CoRR , abs/1411.4038, 2014. [pdf]. 84, 85,\\n127\\nS. Ma, H. Wang, L. Ma, et al. The Era of 1-bit\\nLLMs: All Large Language Models are in 1.58\\nBits. CoRR , abs/2402.17764, 2024. [pdf]. 152\\nA. L. Maas, A. Y. Hannun, and A. Y. Ng. Rec-\\ntifier nonlinearities improve neural network\\nacoustic models. In proceedings of the ICML\\nWorkshop on Deep Learning for Audio, Speech\\nand Language Processing , 2013. [pdf]. 72\\nV. Mnih, K. Kavukcuoglu, D. Silver, et al. Human-\\nlevel control through deep reinforcement\\nlearning. Nature , 518(7540):529â€“533, February\\n2015. [pdf]. 135, 136\\nA. Nichol, P. Dhariwal, A. Ramesh, et al. GLIDE:\\nTowards Photorealistic Image Generation and\\nEditing with Text-Guided Diffusion Models.\\nCoRR , abs/2112.10741, 2021. [pdf]. 145\\nL. Ouyang, J. Wu, X. Jiang, et al. Training lan-\\nguage models to follow instructions with hu-\\n171', metadata={'source': 'rag/deep learning.pdf', 'page': 170}),\n",
       " Document(page_content='man feedback. CoRR , abs/2203.02155, 2022.\\n[pdf]. 141\\nR. Pascanu, T. Mikolov, and Y. Bengio. On the dif-\\nficulty of training recurrent neural networks.\\nInInternational Conference on Machine Learn-\\ning (ICML) , 2013. [pdf]. 44\\nA. Radford, J. Kim, C. Hallacy, et al. Learn-\\ning Transferable Visual Models From Natural\\nLanguage Supervision. CoRR , abs/2103.00020,\\n2021. [pdf]. 131, 133\\nA. Radford, J. Kim, T. Xu, et al. Robust Speech\\nRecognition via Large-Scale Weak Supervi-\\nsion. CoRR , abs/2212.04356, 2022. [pdf]. 129\\nA. Radford, K. Narasimhan, T. Salimans, and\\nI. Sutskever. Improving Language Understand-\\ning by Generative Pre-Training, 2018. [pdf].\\n109, 112, 139\\nA. Radford, J. Wu, R. Child, et al. Language\\nModels are Unsupervised Multitask Learners,\\n2019. [pdf]. 112, 162\\nO. Ronneberger, P. Fischer, and T. Brox. U-Net:\\nConvolutional Networks for Biomedical Im-\\nage Segmentation. In Medical Image Comput-\\ning and Computer-Assisted Intervention , 2015.\\n[pdf]. 84, 85, 127\\n172', metadata={'source': 'rag/deep learning.pdf', 'page': 171}),\n",
       " Document(page_content='P. Sahoo, A. Singh, S. Saha, et al. A Systematic\\nSurvey of Prompt Engineering in Large Lan-\\nguage Models: Techniques and Applications.\\nCoRR , abs/2402.07927, 2024. [pdf]. 147\\nF. Scarselli, M. Gori, A. C. Tsoi, et al. The Graph\\nNeural Network Model. IEEE Transactions\\non Neural Networks (TNN) , 20(1):61â€“80, 2009.\\n[pdf]. 161\\nR. Sennrich, B. Haddow, and A. Birch. Neural\\nMachine Translation of Rare Words with Sub-\\nword Units. CoRR , abs/1508.07909, 2015. [pdf].\\n34\\nJ. Sevilla, L. Heim, A. Ho, et al. Compute Trends\\nAcross Three Eras of Machine Learning. CoRR ,\\nabs/2202.05924, 2022. [pdf]. 8, 52, 54\\nJ. Sevilla, P. Villalobos, J. F. CerÃ³n, et al. Param-\\neter, Compute and Data Trends in Machine\\nLearning, May 2023. [web]. 55\\nK. Simonyan and A. Zisserman. Very Deep Con-\\nvolutional Networks for Large-Scale Image\\nRecognition. CoRR , abs/1409.1556, 2014. [pdf].\\n101\\nN. Srivastava, G. Hinton, A. Krizhevsky, et al.\\nDropout: A Simple Way to Prevent Neural\\n173', metadata={'source': 'rag/deep learning.pdf', 'page': 172}),\n",
       " Document(page_content='Networks from Overfitting. Journal of Ma-\\nchine Learning Research (JMLR) , 15:1929â€“1958,\\n2014. [pdf]. 77\\nM. Telgarsky. Benefits of depth in neural net-\\nworks. CoRR , abs/1602.04485, 2016. [pdf]. 47\\nH. Touvron, T. Lavril, G. Izacard, et al. LLaMA:\\nOpen and Efficient Foundation Language Mod-\\nels.CoRR , abs/2302.13971, 2023. [pdf]. 151\\nA. Vaswani, N. Shazeer, N. Parmar, et al. Atten-\\ntion Is All You Need. CoRR , abs/1706.03762,\\n2017. [pdf]. 84, 87, 97, 108, 109, 110\\nJ. Wei, X. Wang, D. Schuurmans, et al. Chain of\\nThought Prompting Elicits Reasoning in Large\\nLanguage Models. CoRR , abs/2201.11903, 2022.\\n[pdf]. 149\\nB. Xu, A. Yang, J. Lin, et al. ExpertPrompting: In-\\nstructing Large Language Models to be Distin-\\nguished Experts. CoRR , abs/2305.14688, 2023.\\n[pdf]. 147\\nP. Yadav, D. Tam, L. Choshen, et al. TIES-\\nMerging: Resolving Interference When Merg-\\ning Models. CoRR , abs/2306.01708, 2023. [pdf].\\n157\\nL. Yu, B. Yu, H. Yu, et al. Language Models\\nare Super Mario: Absorbing Abilities from\\n174', metadata={'source': 'rag/deep learning.pdf', 'page': 173}),\n",
       " Document(page_content='Homologous Models as a Free Lunch. CoRR ,\\nabs/2311.03099, 2023. [pdf]. 157\\nJ. Zbontar, L. Jing, I. Misra, et al. Barlow Twins:\\nSelf-Supervised Learning via Redundancy Re-\\nduction. CoRR , abs/2103.03230, 2021. [pdf].\\n162\\nM. D. Zeiler and R. Fergus. Visualizing and Un-\\nderstanding Convolutional Networks. In Eu-\\nropean Conference on Computer Vision (ECCV) ,\\n2014. [pdf]. 69\\nH. Zhao, J. Shi, X. Qi, et al. Pyramid Scene\\nParsing Network. CoRR , abs/1612.01105, 2016.\\n[pdf]. 127, 128\\nJ. Zhou, C. Wei, H. Wang, et al. iBOT: Im-\\nage BERT Pre-Training with Online Tokenizer.\\nCoRR , abs/2111.07832, 2021. [pdf]. 163\\n175', metadata={'source': 'rag/deep learning.pdf', 'page': 174}),\n",
       " Document(page_content='Index\\n1D convolution, 66\\n2D convolution, 66\\nactivation, 23, 41\\nfunction, 71, 99\\nmap, 69\\nAdam, 39, 154\\nadapter, 153\\naffine operation, 61\\nartificial neural network, 8, 11\\nattention operator, 88\\nautoencoder, 159\\ndenoising, 118\\nAutograd, 42\\nautoregressive model, seemodel, autoregressive\\naverage pooling, 76\\nbackpropagation, 42\\nbackward pass, 42, 154\\nbasis function regression, 14\\nbatch, 21, 38\\nbatch normalization, 80, 104\\n176', metadata={'source': 'rag/deep learning.pdf', 'page': 175}),\n",
       " Document(page_content='Bellman equation, 135\\nbias vector, 61, 67\\nBPE, seeByte Pair Encoding\\nByte Pair Encoding, 34, 129\\ncache memory, 21\\ncapacity, 16\\ncausal, 32, 90, 111\\nmodel, seemodel, causal\\nchain rule (derivative), 40\\nchain rule (probability), 30\\nchain-of-thought, 140, 149\\nchannel, 23\\ncheckpointing, 43\\nclassification, 18, 26, 101, 120\\nCLIP, seeContrastive Language-Image\\nPre-training\\nCLS token, 115\\ncomputational cost, 43, 91\\ncontext size, 147\\nContrastive Language-Image Pre-training, 131,\\n156\\ncontrastive loss, 27, 131\\nconvnet, seeconvolutional network\\nconvolution, 66\\nconvolutional layer, seelayer, convolutional\\nconvolutional network, 101\\ncross-attention block, 94, 109, 111\\ncross-entropy, 27, 31, 45\\n177', metadata={'source': 'rag/deep learning.pdf', 'page': 176}),\n",
       " Document(page_content='data augmentation, 120\\ndeep learning, 8, 11\\nDeep Q-Network, 135\\ndenoising autoencoder, seeautoencoder,\\ndenoising\\ndensity modeling, 18\\ndepth, 41\\ndiffusion model, 142\\ndilation, 67, 74\\ndiscriminator, 160\\ndownscaling residual block, 106\\ndownstream task, 50\\nDQN, seeDeep Q-Network\\ndropout, 77, 91\\nembedding layer, seelayer, embedding\\nepoch, 48\\nequivariance, 67, 94\\nfeed-forward block, 108, 109\\nfew-shot prediction, 139\\nfilter, 66\\nfine-tune, 124\\nfine-tuning, 51, 141\\nflops, 22\\nforward pass, 41\\nfoundation model, 140\\nFP32, 22\\nframework, 23\\n178', metadata={'source': 'rag/deep learning.pdf', 'page': 177}),\n",
       " Document(page_content='GAN, seeGenerative Adversarial Networks\\nGELU, 73\\nGenerative Adversarial Networks, 160\\nGenerative Pre-trained Transformer, 112, 131,\\n139, 162\\ngenerator, 160\\nGNN, seeGraph Neural Network\\nGPT, seeGenerative Pre-trained Transformer\\nGPU, seeGraphical Processing Unit\\ngradient descent, 35, 37, 40, 45\\ngradient norm clipping, 44\\ngradient step, 35\\nGraph Neural Network, 161\\nGraphical Processing Unit, 8, 20\\nground truth, 18\\nhidden layer, seelayer, hidden\\nhidden state, 158\\nhyper parameter, seeparameter, hyper\\nhyperbolic tangent, 72\\nimage processing, 101\\nimage synthesis, 87, 142\\ninductive bias, 17, 49, 66, 67, 96\\ninvariance, 76, 94, 96, 162\\nkernel size, 66, 74\\nkey, 88\\nLarge Language Model, 51, 56, 88, 139, 146, 162\\n179', metadata={'source': 'rag/deep learning.pdf', 'page': 178}),\n",
       " Document(page_content='layer, 41, 59\\nattention, 87\\nconvolutional, 66, 74, 87, 96, 101, 104, 121,\\n126, 129\\nembedding, 95, 111\\nfully connected, 61, 87, 96, 99, 101\\nhidden, 99\\nlinear, 61\\nMulti-Head Attention, 93, 96, 111\\nnormalizing, 80\\nreversible, 43\\nlayer normalization, 83, 108, 111\\nLeaky ReLU, 72\\nlearning rate, 35, 50\\nlearning rate schedule, 50\\nLeNet, 101, 102\\nlinear layer, seelayer, linear\\nLLM, seeLarge Language Model\\nlocal minimum, 35\\nlogit, 26, 31\\nLoRA, seeLow-Rank Adaptation\\nloss, 12\\nLow-Rank Adaptation, 153, 155\\nmachine learning, 11, 17, 18\\nMarkovian Decision Process, 134\\nMarkovian property, 134\\nmax pooling, 74, 101\\nMDP, seeMarkovian, Decision Process\\n180', metadata={'source': 'rag/deep learning.pdf', 'page': 179}),\n",
       " Document(page_content='mean squared error, 14, 26\\nmemory requirement, 43\\nmemory speed, 21\\nmetric learning, 27\\nMLP, seemulti-layer perceptron, 154\\nmodel, 12\\nautoregressive, 30, 31, 139\\ncausal, 33, 91, 111, 112\\nparametric, 12\\npre-trained, 51, 124, 128\\nmodel merging, 156\\nmulti-layer perceptron, 45, 99â€“101, 108\\nNatural Language Processing, 87\\nNLP, seeNatural Language Processing\\nnon-linearity, 71\\nnormalizing layer, seelayer, normalizing\\nobject detection, 121\\noverfitting, 17, 48\\npadding, 67, 74\\nparameter, 12\\nhyper, 13, 35, 48, 66, 67, 74, 93, 95\\nparametric model, seemodel, parametric\\npeak performance, 22\\nPerplexity, 151\\nperplexity, 31\\npolicy, 134\\noptimal, 134\\n181', metadata={'source': 'rag/deep learning.pdf', 'page': 180}),\n",
       " Document(page_content='pooling, 74\\npositional encoding, 96, 111\\nPost-Training Quantization, 150\\nposterior probability, 26\\npre-trained model, seemodel, pre-trained\\nprompt, 139, 140\\nengineering, 147\\nquantization, 150\\nQuantization-Aware Training, 152\\nquery, 88\\nRAG, seeRetrieval-Augmented Generation\\nrandom initialization, 62\\nreceptive field, 68, 69, 124\\nrectified linear unit, 71, 158\\nrecurrent neural network, 158\\nregression, 18\\nReinforcement Learning, 134, 141\\nReinforcement Learning from Human Feedback,\\n141\\nReLU, seerectified linear unit\\nresidual\\nblock, 104\\nconnection, 84, 103\\nnetwork, 47, 84, 103\\nResNet-50, 103\\nRetrieval-Augmented Generation, 149\\nreturn, 134\\n182', metadata={'source': 'rag/deep learning.pdf', 'page': 181}),\n",
       " Document(page_content='reversible layer, seelayer, reversible\\nRL,seeReinforcement Learning\\nRLHF, seeReinforcement Learning from Human\\nFeeback\\nRNN, seerecurrent neural network\\nscaling laws, 52\\nself-attention block, 94, 109, 111\\nself-supervised learning, 162\\nsemantic segmentation, 86, 126\\nSGD, seestochastic gradient descent\\nSingle Shot Detector, 121\\nskip connection, 84, 127, 158\\nsoftargmax, 26, 89\\nsoftmax, 26\\nspeech recognition, 129\\nSSD, seeSingle Shot Detector\\nstochastic gradient descent, 38, 45, 52\\nstride, 67, 74\\nsupervised learning, 19\\nTanh, seehyperbolic tangent\\nTask Arithmetic, 156\\ntensor, 23\\ntensor cores, 21\\nTensor Processing Unit, 21\\ntest set, 48\\ntext synthesis, 139\\ntoken, 30\\n183', metadata={'source': 'rag/deep learning.pdf', 'page': 182}),\n",
       " Document(page_content='tokenizer, 34, 129\\nTPU, seeTensor Processing Unit\\ntrainable parameter, 12, 23, 52\\ntraining, 12\\ntraining set, 12, 25, 48\\nTransformer, 47, 84, 88, 96, 108, 110, 129\\ntransformer, 154\\ntransposed convolution, 69, 126\\nunderfitting, 16\\nuniversal approximation theorem, 99\\nunsupervised learning, 19\\nVAE, seevariational, autoencoder\\nvalidation set, 48\\nvalue, 88\\nvanishing gradient, 44, 58\\nvariational\\nautoencoder, 160\\nbound, 144\\nVision Transformer, 113, 131\\nViT, seeVision Transformer\\nvocabulary, 30\\nweight, 13\\ndecay, 28\\nmatrix, 61\\nzero-shot prediction, 132\\n184', metadata={'source': 'rag/deep learning.pdf', 'page': 183}),\n",
       " Document(page_content='This book is licensed under the Creative Com-\\nmons BY-NC-SA 4.0 International License.\\nV1.2â€“May 19, 2024\\n185', metadata={'source': 'rag/deep learning.pdf', 'page': 184})]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('rag/deep learning.pdf')\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The Little Book\\nof\\nDeep Learning\\nFranÃ§ois Fleuret', metadata={'source': 'rag/deep learning.pdf', 'page': 0}),\n",
       " Document(page_content='FranÃ§ois Fleuret is a professor of computer sci-\\nence at the University of Geneva, Switzerland.\\nThe cover illustration is a schematic of the\\nNeocognitron by Fukushima [1980], a key an-\\ncestor of deep neural networks.\\nThis ebook is formatted to fit on a phone screen.', metadata={'source': 'rag/deep learning.pdf', 'page': 1}),\n",
       " Document(page_content='Contents\\nContents 5\\nList of figures 7\\nForeword 8\\nI Foundations 10\\n1 Machine Learning 11\\n1.1 Learning from data . . . . . . . 12\\n1.2 Basis function regression . . . . 14\\n1.3 Under and overfitting . . . . . . 16\\n1.4 Categories of models . . . . . . 18\\n2 Efficient Computation 20\\n2.1 GPUs, TPUs, and batches . . . . 21\\n2.2 Tensors . . . . . . . . . . . . . . 23\\n3 Training 25\\n3.1 Losses . . . . . . . . . . . . . . 26\\n3.2 Autoregressive models . . . . . 30\\n3.3 Gradient descent . . . . . . . . 35\\n3', metadata={'source': 'rag/deep learning.pdf', 'page': 2}),\n",
       " Document(page_content='3.4 Backpropagation . . . . . . . . 40\\n3.5 The value of depth . . . . . . . 45\\n3.6 Training protocols . . . . . . . 48\\n3.7 The benefits of scale . . . . . . 52\\nII Deep Models 57\\n4 Model Components 58\\n4.1 The notion of layer . . . . . . . 59\\n4.2 Linear layers . . . . . . . . . . . 61\\n4.3 Activation functions . . . . . . 71\\n4.4 Pooling . . . . . . . . . . . . . . 74\\n4.5 Dropout . . . . . . . . . . . . . 77\\n4.6 Normalizing layers . . . . . . . 80\\n4.7 Skip connections . . . . . . . . 84\\n4.8 Attention layers . . . . . . . . . 87\\n4.9 Token embedding . . . . . . . . 95\\n4.10 Positional encoding . . . . . . . 96\\n5 Architectures 98\\n5.1 Multi-Layer Perceptrons . . . . 99\\n5.2 Convolutional networks . . . . 101\\n5.3 Attention models . . . . . . . . 108\\nIII Applications 116\\n6 Prediction 117\\n6.1 Image denoising . . . . . . . . . 118\\n6.2 Image classification . . . . . . . 120\\n6.3 Object detection . . . . . . . . . 121\\n4', metadata={'source': 'rag/deep learning.pdf', 'page': 3}),\n",
       " Document(page_content='6.4 Semantic segmentation . . . . . 126\\n6.5 Speech recognition . . . . . . . 129\\n6.6 Text-image representations . . . 131\\n6.7 Reinforcement learning . . . . . 134\\n7 Synthesis 138\\n7.1 Text generation . . . . . . . . . 139\\n7.2 Image generation . . . . . . . . 142\\n8 The Compute Schism 146\\n8.1 Prompt Engineering . . . . . . 147\\n8.2 Quantization . . . . . . . . . . . 150\\n8.3 Adapters . . . . . . . . . . . . . 153\\n8.4 Model merging . . . . . . . . . 156\\nThe missing bits 158\\nBibliography 164\\nIndex 176\\n5', metadata={'source': 'rag/deep learning.pdf', 'page': 4})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=20)\n",
    "text_splitter.split_documents(docs)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The Little Book\\nof\\nDeep Learning\\nFranÃ§ois Fleuret', metadata={'source': 'rag/deep learning.pdf', 'page': 0}),\n",
       " Document(page_content='FranÃ§ois Fleuret is a professor of computer sci-\\nence at the University of Geneva, Switzerland.\\nThe cover illustration is a schematic of the\\nNeocognitron by Fukushima [1980], a key an-\\ncestor of deep neural networks.\\nThis ebook is formatted to fit on a phone screen.', metadata={'source': 'rag/deep learning.pdf', 'page': 1}),\n",
       " Document(page_content='Contents\\nContents 5\\nList of figures 7\\nForeword 8\\nI Foundations 10\\n1 Machine Learning 11\\n1.1 Learning from data . . . . . . . 12\\n1.2 Basis function regression . . . . 14\\n1.3 Under and overfitting . . . . . . 16\\n1.4 Categories of models . . . . . . 18\\n2 Efficient Computation 20\\n2.1 GPUs, TPUs, and batches . . . . 21\\n2.2 Tensors . . . . . . . . . . . . . . 23\\n3 Training 25\\n3.1 Losses . . . . . . . . . . . . . . 26\\n3.2 Autoregressive models . . . . . 30\\n3.3 Gradient descent . . . . . . . . 35\\n3', metadata={'source': 'rag/deep learning.pdf', 'page': 2}),\n",
       " Document(page_content='3.4 Backpropagation . . . . . . . . 40\\n3.5 The value of depth . . . . . . . 45\\n3.6 Training protocols . . . . . . . 48\\n3.7 The benefits of scale . . . . . . 52\\nII Deep Models 57\\n4 Model Components 58\\n4.1 The notion of layer . . . . . . . 59\\n4.2 Linear layers . . . . . . . . . . . 61\\n4.3 Activation functions . . . . . . 71\\n4.4 Pooling . . . . . . . . . . . . . . 74\\n4.5 Dropout . . . . . . . . . . . . . 77\\n4.6 Normalizing layers . . . . . . . 80\\n4.7 Skip connections . . . . . . . . 84\\n4.8 Attention layers . . . . . . . . . 87\\n4.9 Token embedding . . . . . . . . 95\\n4.10 Positional encoding . . . . . . . 96\\n5 Architectures 98\\n5.1 Multi-Layer Perceptrons . . . . 99\\n5.2 Convolutional networks . . . . 101\\n5.3 Attention models . . . . . . . . 108\\nIII Applications 116\\n6 Prediction 117\\n6.1 Image denoising . . . . . . . . . 118\\n6.2 Image classification . . . . . . . 120\\n6.3 Object detection . . . . . . . . . 121\\n4', metadata={'source': 'rag/deep learning.pdf', 'page': 3}),\n",
       " Document(page_content='6.4 Semantic segmentation . . . . . 126\\n6.5 Speech recognition . . . . . . . 129\\n6.6 Text-image representations . . . 131\\n6.7 Reinforcement learning . . . . . 134\\n7 Synthesis 138\\n7.1 Text generation . . . . . . . . . 139\\n7.2 Image generation . . . . . . . . 142\\n8 The Compute Schism 146\\n8.1 Prompt Engineering . . . . . . 147\\n8.2 Quantization . . . . . . . . . . . 150\\n8.3 Adapters . . . . . . . . . . . . . 153\\n8.4 Model merging . . . . . . . . . 156\\nThe missing bits 158\\nBibliography 164\\nIndex 176\\n5', metadata={'source': 'rag/deep learning.pdf', 'page': 4}),\n",
       " Document(page_content='List of Figures\\n1.1 Kernel regression . . . . . . . . . . 14\\n1.2 Overfitting of kernel regression . . 16\\n3.1 Causal autoregressive model . . . . 32\\n3.2 Gradient descent . . . . . . . . . . . 36\\n3.3 Backpropagation . . . . . . . . . . . 40\\n3.4 Feature warping . . . . . . . . . . . 46\\n3.5 Training and validation losses . . . 49\\n3.6 Scaling laws . . . . . . . . . . . . . 53\\n3.7 Model training costs . . . . . . . . . 55\\n4.1 1D convolution . . . . . . . . . . . . 63\\n4.2 2D convolution . . . . . . . . . . . . 64\\n4.3 Stride, padding, and dilation . . . . 65\\n4.4 Receptive field . . . . . . . . . . . . 68\\n4.5 Activation functions . . . . . . . . . 72\\n4.6 Max pooling . . . . . . . . . . . . . 75\\n4.7 Dropout . . . . . . . . . . . . . . . . 78\\n4.8 Dropout 2D . . . . . . . . . . . . . . 79\\n4.9 Batch normalization . . . . . . . . . 81\\n4.10 Skip connections . . . . . . . . . . . 85\\n6', metadata={'source': 'rag/deep learning.pdf', 'page': 5}),\n",
       " Document(page_content='4.11 Attention operator interpretation . 88\\n4.12 Complete attention operator . . . . 90\\n4.13 Multi-Head Attention layer . . . . . 92\\n5.1 Multi-Layer Perceptron . . . . . . . 99\\n5.2 LeNet-like convolutional model . . 102\\n5.3 Residual block . . . . . . . . . . . . 103\\n5.4 Downscaling residual block . . . . . 104\\n5.5 ResNet-50 . . . . . . . . . . . . . . . 105\\n5.6 Transformer components . . . . . . 109\\n5.7 Transformer . . . . . . . . . . . . . 110\\n5.8 GPT model . . . . . . . . . . . . . . 112\\n5.9 ViT model . . . . . . . . . . . . . . 114\\n6.1 Convolutional object detector . . . 122\\n6.2 Object detection with SSD . . . . . 123\\n6.3 Semantic segmentation with PSP . . 127\\n6.4 CLIP zero-shot prediction . . . . . . 133\\n6.5 DQN state value evolution . . . . . 136\\n7.1 Few-shot prediction with a GPT . . 140\\n7.2 Denoising diffusion . . . . . . . . . 143\\n8.1 Chain-of-thought . . . . . . . . . . 148\\n8.2 Quantization . . . . . . . . . . . . . 151\\n7', metadata={'source': 'rag/deep learning.pdf', 'page': 6}),\n",
       " Document(page_content='Foreword\\nThe current period of progress in artificial in-\\ntelligence was triggered when Krizhevsky et al.\\n[2012] demonstrated that an artificialneuralnet-\\nwork designed twenty years earlier [LeCun et al.,\\n1989] could outperform complex state-of-the-\\nart image recognition methods by a huge mar-\\ngin, simply by being a hundred times larger and\\ntrained on a dataset similarly scaled up.\\nThis breakthrough was made possible thanks\\ntoGraph icalProcessingUnits ( GPUs), highly\\nparallel consumer-grade computing devices de-\\nveloped for real-time image synthesis and repur-\\nposed for artificial neural networks.\\nSince then, under the umbrella term of â€œ deep\\nlearn ing,â€ innovations in the structures of these\\nnetworks, the strategies to train them, and ded-\\nicated hardware have allowed for an exponen-\\ntial increase in both their size and the quantity\\nof training data they take advantage of [Sevilla\\n8', metadata={'source': 'rag/deep learning.pdf', 'page': 7}),\n",
       " Document(page_content='et al., 2022]. This has resulted in a wave of suc-\\ncessful applications across technical domains,\\nfrom computer vision and robotics to speech\\nprocessing, and since 2020 in the development\\nof Large Language Models with general proto-\\nreasoning capabilities [Chowdhery et al., 2022].\\nAlthough the bulk of deep learning is not difficult\\nto understand, it combines diverse components\\nsuch as linear algebra, calculus, probabilities, op-\\ntimization, signal processing, programming, al-\\ngorithmics, and high-performance computing,\\nmaking it complicated to learn.\\nInstead of trying to be exhaustive, this little book\\nis limited to the background necessary to under-\\nstand a few important models. This proved to\\nbe a popular approach, resulting in more than\\n500,000 downloads of the PDF file in the 12\\nmonths following its announcement on Twitter.\\nIf you did not get this book from its official URL\\nhttps://fleuret.org/public/lbdl.pdf\\nplease do so, to allow the estimation of the num-\\nber of readers.', metadata={'source': 'rag/deep learning.pdf', 'page': 8}),\n",
       " Document(page_content='ber of readers.\\nFranÃ§ois Fleuret,\\nMay 19, 2024\\n9', metadata={'source': 'rag/deep learning.pdf', 'page': 8}),\n",
       " Document(page_content='Part I\\nFoundations\\n10', metadata={'source': 'rag/deep learning.pdf', 'page': 9}),\n",
       " Document(page_content='Chapter 1\\nMachine Learning\\nDeep learn ing belongs historically to the larger\\nfield of statistical machine learn ing, as it funda-\\nmentally concerns methods that are able to learn\\nrepresentations from data. The techniques in-\\nvolved come originally from artificialneuralnet-\\nworks, and the â€œdeepâ€ qualifier highlights that\\nmodels are long compositions of mappings, now\\nknown to achieve greater performance.\\nThe modularity, versatility, and scalability of\\ndeep models have resulted in a plethora of spe-\\ncific mathematical methods and software devel-\\nopment tools, establishing deep learning as a\\ndistinct and vast technical field.\\n11', metadata={'source': 'rag/deep learning.pdf', 'page': 10}),\n",
       " Document(page_content='1.1 Learning from data\\nThe simplest use case for a model trained from\\ndata is when a signal xis accessible, for instance,\\nthe picture of a license plate, from which one\\nwants to predict a quantity y, such as the string\\nof characters written on the plate.\\nIn many real-world situations where xis a high-\\ndimensional signal captured in an uncontrolled\\nenvironment, it is too complicated to come up\\nwith an analytical recipe that relates xandy.\\nWhat one can do is to collect a large train ing\\nsetð’Ÿof pairs (xn,yn), and devise a paramet-\\nricmodel f. This is a piece of computer code\\nthat incorporates train able parameterswthat\\nmodulate its behavior, and such that, with the\\nproper values wâˆ—, it is a good predictor. â€œGoodâ€\\nhere means that if an xis given to this piece\\nof code, the value Ë†y=f(x;wâˆ—)it computes is\\na good estimate of the ythat would have been\\nassociated with xin the training set had it been\\nthere.\\nThis notion of goodness is usually formalized\\nwith a lossâ„’(w)which is small when f(Â·;w)is', metadata={'source': 'rag/deep learning.pdf', 'page': 11}),\n",
       " Document(page_content='good on ð’Ÿ. Then, train ing the model consists of\\ncomputing a value wâˆ—that minimizes â„’(wâˆ—).\\n12', metadata={'source': 'rag/deep learning.pdf', 'page': 11}),\n",
       " Document(page_content='Most of the content of this book is about the defi-\\nnition of f, which, in realistic scenarios, is a com-\\nplex combination of pre-defined sub-modules.\\nThe trainable parameters that compose ware of-\\nten called weights, by analogy with the synaptic\\nweights of biological neural networks. In addi-\\ntion to these parameters, models usually depend\\nonhyper-parameters, which are set according\\nto domain prior knowledge, best practices, or re-\\nsource constraints. They may also be optimized\\nin some way, but with techniques different from\\nthose used to optimize w.\\n13', metadata={'source': 'rag/deep learning.pdf', 'page': 12}),\n",
       " Document(page_content='1.2 Basis function regression\\nWe can illustrate the training of a model in a sim-\\nple case where xnandynare two real numbers,\\nthe loss is the mean squared error:\\nâ„’(w) =1\\nNNX\\nn=1(ynâˆ’f(xn;w))2, (1.1)\\nandf(Â·;w)is a linear combination of a pre-\\ndefined basis of functions f1,...,f K, with w=\\n(w1,...,w K):\\nf(x;w) =KX\\nk=1wkfk(x).\\nSince f(xn;w)is linear with respect to the wks\\nandâ„’(w)is quadratic with respect to f(xn;w),\\nFigure 1.1: Given a basis of functions (blue curves)\\nand a training set (black dots), we can compute an\\noptimal linear combination of the former (red curve)\\nto approximate the latter for the mean squared error.\\n14', metadata={'source': 'rag/deep learning.pdf', 'page': 13}),\n",
       " Document(page_content='the loss â„’(w)is quadratic with respect to the\\nwks, and finding wâˆ—that minimizes it boils down\\nto solving a linear system. See Figure 1.1 for an\\nexample with Gaussian kernels as fk.\\n15', metadata={'source': 'rag/deep learning.pdf', 'page': 14}),\n",
       " Document(page_content='1.3 Under and overfitting\\nA key element is the interplay between the capac-\\nity of the model, that is its flexibility and ability\\nto fit diverse data, and the amount and quality\\nof the training data. When the capacity is insuf-\\nficient, the model cannot fit the data, resulting\\nin a high error during training. This is referred\\nto as underfitting.\\nOn the contrary, when the amount of data is in-\\nsufficient, as illustrated in Figure 1.2, the model\\nwill often learn characteristics specific to the\\ntraining examples, resulting in excellent perfor-\\nmance during training, at the cost of a worse\\nFigure 1.2: If the amount of training data (black dots)\\nis small compared to the capacity of the model, the em-\\npirical performance of the fitted model during training\\n(red curve) reflects poorly its actual fit to the underly-\\ning data structure (thin black curve), and consequently\\nits usefulness for prediction.\\n16', metadata={'source': 'rag/deep learning.pdf', 'page': 15}),\n",
       " Document(page_content='fit to the global structure of the data, and poor\\nperformance on new inputs. This phenomenon\\nis referred to as overfitting.\\nSo, a large part of the art of applied machine\\nlearn ing is to design models that are not too\\nflexible yet still able to fit the data. This is done\\nby crafting the right inductivebias in a model,\\nwhich means that its structure corresponds to\\nthe underlying structure of the data at hand.\\nEven though this classical perspective is relevant\\nfor reasonably-sized deep models, things get con-\\nfusing with large ones that have a very large\\nnumber of trainable parameters and extreme ca-\\npacity yet still perform well on prediction. We\\nwill come back to this in Â§ 3.6 and Â§ 3.7.\\n17', metadata={'source': 'rag/deep learning.pdf', 'page': 16}),\n",
       " Document(page_content='1.4 Categories of models\\nWe can organize the use of machine learn ing\\nmodels into three broad categories:\\nâ€¢Regression consists of predicting a\\ncontinuous-valued vector yâˆˆRK, for instance,\\na geometrical position of an object, given an\\ninput signal X. This is a multi-dimensional\\ngeneralization of the setup we saw in Â§ 1.2. The\\ntraining set is composed of pairs of an input\\nsignal and a ground -truth value.\\nâ€¢Classification aims at predicting a value from\\na finite set {1,...,C}, for instance, the label Yof\\nan image X. As with regression, the training set\\nis composed of pairs of input signal, and ground-\\ntruth quantity, here a label from that set. The\\nstandard way of tackling this is to predict one\\nscore per potential class, such that the correct\\nclass has the maximum score.\\nâ€¢Densitymodeling has as its objective to model\\nthe probability density function of the data ÂµX\\nitself, for instance, images. In that case, the train-\\ning set is composed of values xnwithout associ-', metadata={'source': 'rag/deep learning.pdf', 'page': 17}),\n",
       " Document(page_content='ated quantities to predict, and the trained model\\nshould allow for the evaluation of the probability\\ndensity function, or sampling from the distribu-\\ntion, or both.\\n18', metadata={'source': 'rag/deep learning.pdf', 'page': 17}),\n",
       " Document(page_content='Both regression and classification are generally\\nreferred to as supervised learn ing, since the\\nvalue to be predicted, which is required as a\\ntarget during training, has to be provided, for in-\\nstance, by human experts. On the contrary, den-\\nsity modeling is usually seen as unsupervised\\nlearn ing, since it is sufficient to take existing\\ndata without the need for producing an associ-\\nated ground-truth.\\nThese three categories are not disjoint; for in-\\nstance, classification can be cast as class-score\\nregression, or discrete sequence density model-\\ning as iterated classification. Furthermore, they\\ndo not cover all cases. One may want to predict\\ncompounded quantities, or multiple classes, or\\nmodel a density conditional on a signal.\\n19', metadata={'source': 'rag/deep learning.pdf', 'page': 18}),\n",
       " Document(page_content='Chapter 2\\nEfficient\\nComputation\\nFrom an implementation standpoint, deep learn-\\ning is about executing heavy computations with\\nlarge amounts of data. The Graph icalProcessing\\nUnits ( GPUs) have been instrumental in the suc-\\ncess of the field by allowing such computations\\nto be run on affordable hardware.\\nThe importance of their use, and the resulting\\ntechnical constraints on the computations that\\ncan be done efficiently, force the research in the\\nfield to constantly balance mathematical sound-\\nness and implementability of novel methods.\\n20', metadata={'source': 'rag/deep learning.pdf', 'page': 19}),\n",
       " Document(page_content='2.1 GPUs, TPUs, and batches\\nGraphical Processing Units were originally de-\\nsigned for real-time image synthesis, which re-\\nquires highly parallel architectures that happen\\nto be well suited for deep models. As their usage\\nfor AI has increased, GPUs have been equipped\\nwith dedicated tensorcores, and deep-learning\\nspecialized chips such as Googleâ€™s TensorPro-\\ncessingUnits ( TPUs) have been developed.\\nA GPU possesses several thousand parallel units\\nand its own fast memory. The limiting factor\\nis usually not the number of computing units,\\nbut the read-write operations tomem ory. The\\nslowest link is between the CPU memory and\\nthe GPU memory, and consequently one should\\navoid copying data across devices. Moreover,\\nthe structure of the GPU itself involves multiple\\nlevels of cache mem ory, which are smaller but\\nfaster, and computation should be organized to\\navoid copies between these different caches.\\nThis is achieved, in particular, by organizing the', metadata={'source': 'rag/deep learning.pdf', 'page': 20}),\n",
       " Document(page_content='computation in batches ofsamples that can fit\\nentirely in the GPU memory and are processed\\nin parallel. When an operator combines a sample\\nand model parameters, both have to be moved\\nto the cache memory near the actual computing\\n21', metadata={'source': 'rag/deep learning.pdf', 'page': 20}),\n",
       " Document(page_content='units. Proceeding by batches allows for copying\\nthe model parameters only once, instead of doing\\nit for each sample. In practice, a GPU processes\\na batch that fits in memory almost as quickly as\\nit would process a single sample.\\nA standard GPU has a theoretical peak perfor-\\nmance of 1013â€“1014floating-point operations\\n(FLOPs) per second, and its memory typically\\nranges from 8to80gigabytes. The standard\\nFP32 encoding of float numbers is on 32bits, but\\nempirical results show that using encoding on\\n16bits, or even less for some operands, does not\\ndegrade performance.\\nWe will come back in Â§ 3.7 to the large size of\\ndeep architectures.\\n22', metadata={'source': 'rag/deep learning.pdf', 'page': 21}),\n",
       " Document(page_content='2.2 Tensors\\nGPUs and deep learn ingframe works such as Py-\\nTorch or JAX manipulate the quantities to be\\nprocessed by organizing them as tensors, which\\nare series of scalars arranged along several dis-\\ncrete axes. They are elements of RN1Ã—Â·Â·Â·Ã— ND\\nthat generalize the notion of vector and matrix.\\nTensors are used to represent both the signals to\\nbe processed, the train able parameters of the\\nmodels, and the intermediate quantities they\\ncompute. The latter are called activations, in\\nreference to neuronal activations.\\nFor instance, a time series is naturally encoded\\nas aTÃ—Dtensor, or, for historical reasons, as a\\nDÃ—Ttensor, where Tis its duration and Dis\\nthe dimension of the feature representation at\\nevery time step, often referred to as the number\\nofchan nels. Similarly, a 2D-structured signal can\\nbe represented as a DÃ—HÃ—Wtensor, where H\\nandWare its height and width. An RGB image\\nwould correspond to D= 3, but the number of\\nchannels can grow up to several thousands in\\nlarge models.', metadata={'source': 'rag/deep learning.pdf', 'page': 22}),\n",
       " Document(page_content='large models.\\nAdding more dimensions allows for the represen-\\ntation of series of objects. For example, fifty RGB\\nimages of resolution 32Ã—24can be encoded as\\n23', metadata={'source': 'rag/deep learning.pdf', 'page': 22}),\n",
       " Document(page_content='a50Ã—3Ã—24Ã—32tensor.\\nDeep learning libraries provide a large number\\nof operations that encompass standard linear\\nalgebra, complex reshaping and extraction, and\\ndeep-learning specific operations, some of which\\nwe will see in Chapter 4. The implementation of\\ntensors separates the shape representation from\\nthe storage layout of the coefficients in mem-\\nory, which allows many reshaping, transposing,\\nand extraction operations to be done without\\ncoefficient copying, hence extremely rapidly.\\nIn practice, virtually any computation can be\\ndecomposed into elementary tensor operations,\\nwhich avoids non-parallel loops at the language\\nlevel and poor memory management.\\nBesides being convenient tools, tensors are\\ninstrumental in achieving computational effi-\\nciency. All the people involved in the develop-\\nment of an operational deep model, from the\\ndesigners of the drivers, libraries, and models\\nto those of the computers and chips, know that\\nthe data will be manipulated as tensors. The', metadata={'source': 'rag/deep learning.pdf', 'page': 23}),\n",
       " Document(page_content='resulting constraints on locality and block de-\\ncomposability enable all the actors in this chain\\nto come up with optimal designs.\\n24', metadata={'source': 'rag/deep learning.pdf', 'page': 23}),\n",
       " Document(page_content='Chapter 3\\nTraining\\nAs introduced in Â§ 1.1, training a model consists\\nof minimizing a loss â„’(w)which reflects the\\nperformance of the predictor f(Â·;w)on a train -\\ningsetð’Ÿ.\\nSince models are usually extremely complex, and\\ntheir performance is directly related to how well\\nthe loss is minimized, this minimization is a key\\nchallenge, which involves both computational\\nand mathematical difficulties.\\n25', metadata={'source': 'rag/deep learning.pdf', 'page': 24}),\n",
       " Document(page_content='3.1 Losses\\nThe example of the mean squared error from\\nEquation 1.1 is a standard loss for predicting a\\ncontinuous value.\\nFor density modeling, the standard loss is the\\nlikelihood of the data. If f(x;w)is to be inter-\\npreted as a normalized log-probability or log-\\ndensity, the loss is the opposite of the sum of its\\nvalues over training samples, which corresponds\\nto the likelihood of the data-set.\\nCross-entropy\\nForclassification, the usual strategy is that the\\noutput of the model is a vector with one com-\\nponent f(x;w)yper class y, interpreted as the\\nlogarithm of a non-normalized probability, or\\nlogit.\\nWith Xthe input signal and Ythe class to pre-\\ndict, we can then compute from fan estimate\\nof the posteriorprob abilities:\\nË†P(Y=y|X=x) =expf(x;w)yP\\nzexpf(x;w)z.\\nThis expression is generally called the softmax,\\nor more adequately, the softargmax, of the logits.\\n26', metadata={'source': 'rag/deep learning.pdf', 'page': 25}),\n",
       " Document(page_content='To be consistent with this interpretation, the\\nmodel should be trained to maximize the proba-\\nbility of the true classes, hence to minimize the\\ncross -entropy, expressed as:\\nâ„’ce(w) =âˆ’1\\nNNX\\nn=1logË†P(Y=yn|X=xn)\\n=1\\nNNX\\nn=1âˆ’logexpf(xn;w)ynP\\nzexpf(xn;w)z| {z }\\nLce(f(xn;w),yn).\\nContrastive loss\\nIn certain setups, even though the value to be\\npredicted is continuous, the supervision takes\\nthe form of ranking constraints. The typical do-\\nmain where this is the case is metriclearn ing,\\nwhere the objective is to learn a measure of dis-\\ntance between samples such that a sample xa\\nfrom a certain semantic class is closer to any\\nsample xbof the same class than to any sample\\nxcfrom another class. For instance, xaandxb\\ncan be two pictures of a certain person, and xca\\npicture of someone else.\\nThe standard approach for such cases is to min-\\nimize a contrastive loss, in that case, for in-\\nstance, the sum over triplets (xa,xb,xc), such\\n27', metadata={'source': 'rag/deep learning.pdf', 'page': 26}),\n",
       " Document(page_content='thatya=ybÌ¸=yc, of\\nmax(0 ,1âˆ’f(xa,xc;w)+f(xa,xb;w)).\\nThis quantity will be strictly positive unless\\nf(xa,xc;w)â‰¥1+f(xa,xb;w).\\nEngineering the loss\\nUsually, the loss minimized during training is\\nnot the actual quantity one wants to optimize\\nultimately, but a proxy for which finding the best\\nmodel parameters is easier. For instance, cross-\\nentropy is the standard loss for classification,\\neven though the actual performance measure is\\na classification error rate, because the latter has\\nno informative gradient, a key requirement as\\nwe will see in Â§ 3.3.\\nIt is also possible to add terms to the loss that\\ndepend on the trainable parameters of the model\\nthemselves to favor certain configurations.\\nThe weight decay regularization, for instance,\\nconsists of adding to the loss a term proportional\\nto the sum of the squared parameters. This can\\nbe interpreted as having a Gaussian Bayesian\\nprior on the parameters, which favors smaller\\nvalues and thereby reduces the influence of the', metadata={'source': 'rag/deep learning.pdf', 'page': 27}),\n",
       " Document(page_content='data. This degrades performance on the train-\\n28', metadata={'source': 'rag/deep learning.pdf', 'page': 27}),\n",
       " Document(page_content='ing set, but reduces the gap between the per-\\nformance in training and that on new, unseen\\ndata.\\n29', metadata={'source': 'rag/deep learning.pdf', 'page': 28}),\n",
       " Document(page_content='3.2 Autoregressive models\\nA key class of methods, particularly for deal-\\ning with discrete sequences in natural language\\nprocessing and computer vision, are the autore-\\ngressivemodels,\\nThe chain rule for probabilities\\nSuch models put to use the chain rule from prob-\\nability theory:\\nP(X1=x1,X2=x2,...,X T=xT) =\\nP(X1=x1)\\nÃ—P(X2=x2|X1=x1)\\n...\\nÃ—P(XT=xT|X1=x1,...,X Tâˆ’1=xTâˆ’1).\\nAlthough this decomposition is valid for a ran-\\ndom sequence of any type, it is particularly effi-\\ncient when the signal of interest is a sequence\\noftokens from a finite vocabulary{1,...K}.\\nWith the convention that the additional token âˆ…\\nstands for an â€œunknownâ€ quantity, we can rep-\\nresent the event {X1=x1,...,X t=xt}as the\\nvector (x1,...,x t,âˆ…,...,âˆ…).\\n30', metadata={'source': 'rag/deep learning.pdf', 'page': 29}),\n",
       " Document(page_content='Then, a model\\nf:{âˆ…,1,...,K }Tâ†’RK\\nwhich, given such an input, computes a vector\\nltofKlogits corresponding to\\nË†P(Xt|X1=x1,...,X tâˆ’1=xtâˆ’1),\\nallows to sample one token given the previous\\nones.\\nThe chain rule ensures that by sampling Tto-\\nkensxt, one at a time given the previously sam-\\npledx1,...,x tâˆ’1, we get a sequence that follows\\nthe joint distribution. This is an autoregressive\\ngenerative model.\\nTraining such a model can be done by minimiz-\\ning the sum across training sequences and time\\nsteps of the cross -entropy loss\\nLce\\x00\\nf(x1,...,x tâˆ’1,âˆ…,...,âˆ…;w),xt\\x01\\n,\\nwhich is formally equivalent to maximizing the\\nlikelihood of the true xts.\\nThe value that is classically monitored is not the\\ncross-entropy itself, but the perplexity, which is\\ndefined as the exponential of the cross-entropy.\\nIt corresponds to the number of values of a uni-\\nform distribution with the same entropy, which\\nis generally more interpretable.\\n31', metadata={'source': 'rag/deep learning.pdf', 'page': 30}),\n",
       " Document(page_content='x1x2 ... xTâˆ’2xTâˆ’1l1 l2 l3 ... lTâˆ’1 lT\\nf\\nFigure 3.1: An autoregressive model f, iscausal if a\\ntime step xtof the input sequence modulates the pre-\\ndicted logits lsonly if s > t , as depicted by the blue\\narrows. This allows computing the distributions at all\\nthe time steps in one pass during training. During sam-\\npling, however, the ltandxtare computed sequentially,\\nthe latter sampled with the former, as depicted by the\\nred arrows.\\nCausal models\\nThe training procedure we just described re-\\nquires a different input for each t, and the bulk\\nof the computation done for t < tâ€²is repeated for\\ntâ€². This is extremely inefficient since Tis often\\nof the order of hundreds or thousands.\\nThe standard strategy to address this issue is to\\ndesign a model fthat predicts all the vectors of\\nlogits l1,...,l Tat once, that is:\\nf:{1,...,K }Tâ†’RTÃ—K,\\n32', metadata={'source': 'rag/deep learning.pdf', 'page': 31}),\n",
       " Document(page_content='but with a computational structure such that the\\ncomputed logits ltforxtdepend only on the\\ninput values x1,...,x tâˆ’1.\\nSuch a model is called causal, since it corre-\\nsponds, in the case of temporal series, to not\\nletting the future influence the past, as illustrated\\nin Figure 3.1.\\nThe consequence is that the output at every posi-\\ntion is the one that would be obtained if the input\\nwere only available up to before that position.\\nDuring training, it allows one to compute the\\noutput for a full sequence and to maximize the\\npredicted probabilities of all the tokens of that\\nsame sequence, which again boils down to mini-\\nmizing the sum of the per-token cross-entropy.\\nNote that, for the sake of simplicity, we have\\ndefined fas operating on sequences of a fixed\\nlength T. However, models used in practice,\\nsuch as the transformers we will see in Â§ 5.3, are\\nable to process sequences of arbitrary length.\\nTokenizer\\nOne important technical detail when dealing', metadata={'source': 'rag/deep learning.pdf', 'page': 32}),\n",
       " Document(page_content='with natural languages is that the representation\\nas tokens can be done in multiple ways, ranging\\nfrom the finest granularity of individual symbols\\n33', metadata={'source': 'rag/deep learning.pdf', 'page': 32}),\n",
       " Document(page_content='to entire words. The conversion to and from the\\ntoken representation is carried out by a separate\\nalgorithm called a tokenizer.\\nA standard method is the Byte Pair Encoding\\n(BPE) [Sennrich et al., 2015] that constructs to-\\nkens by hierarchically merging groups of char-\\nacters, trying to get tokens that represent frag-\\nments of words of various lengths but of similar\\nfrequencies, allocating tokens to long frequent\\nfragments as well as to rare individual symbols.\\n34', metadata={'source': 'rag/deep learning.pdf', 'page': 33}),\n",
       " Document(page_content='3.3 Gradient descent\\nExcept in specific cases like the linear regression\\nwe saw in Â§ 1.2, the optimal parameters wâˆ—do\\nnot have a closed-form expression. In the general\\ncase, the tool of choice to minimize a function is\\ngradientdescent. It starts by initializing the pa-\\nrameters with a random w0, and then improves\\nthis estimate by iterating gradientsteps, each\\nconsisting of computing the gradient of the loss\\nwith respect to the parameters, and subtracting\\na fraction of it:\\nwn+1=wnâˆ’Î·âˆ‡â„’|w(wn). (3.1)\\nThis procedure corresponds to moving the cur-\\nrent estimate a bit in the direction that locally\\ndecreases â„’(w)maximally, as illustrated in Fig-\\nure 3.2.\\nLearning rate\\nThehyper-parameterÎ·is called the learn ingrate.\\nIt is a positive value that modulates how quickly\\nthe minimization is done, and must be chosen\\ncarefully.\\nIf it is too small, the optimization will be slow\\nat best, and may be trapped in a localminimum\\nearly. If it is too large, the optimization may\\n35', metadata={'source': 'rag/deep learning.pdf', 'page': 34}),\n",
       " Document(page_content='w\\nwâ„’(w)\\nFigure 3.2: At every point w, the gradient âˆ‡â„’|w(w)is\\nin the direction that maximizes the increase of â„’, or-\\nthogonal to the level curves (top). The gradient descent\\nminimizes â„’(w)iteratively by subtracting a fraction\\nof the gradient at every step, resulting in a trajectory\\nthat follows the steepest descent (bottom).\\n36', metadata={'source': 'rag/deep learning.pdf', 'page': 35}),\n",
       " Document(page_content='bounce around a good minimum and never de-\\nscend into it. As we will see in Â§ 3.6, it can depend\\non the iteration number n.\\nStochastic Gradient Descent\\nAll the losses used in practice can be expressed as\\nan average of a loss per small group of samples,\\nor per sample such as:\\nâ„’(w) =1\\nNNX\\nn=1ð“n(w),\\nwhereð“n(w) =L(f(xn;w),yn)for some L, and\\nthe gradient is then:\\nâˆ‡â„’|w(w) =1\\nNNX\\nn=1âˆ‡ð“n|w(w). (3.2)\\nThe resulting gradientdescent would compute\\nexactly the sum in Equation 3.2, which is usu-\\nally computationally heavy, and then update the\\nparameters according to Equation 3.1. However,\\nunder reasonable assumptions of exchangeabil-\\nity, for instance, if the samples have been prop-\\nerly shuffled, any partial sum of Equation 3.2\\nis an unbiased estimator of the full sum, albeit\\nnoisy. So, updating the parameters from partial\\nsums corresponds to doing more gradient steps\\n37', metadata={'source': 'rag/deep learning.pdf', 'page': 36}),\n",
       " Document(page_content='for the same computational budget, with noisier\\nestimates of the gradient. Due to the redundancy\\nin the data, this happens to be a far more efficient\\nstrategy.\\nWe saw in Â§ 2.1 that processing a batch of sam-\\nples small enough to fit in the computing de-\\nviceâ€™s memory is generally as fast as processing\\na single one. Hence, the standard approach is to\\nsplit the full set ð’Ÿinto batches, and to update\\nthe parameters from the estimate of the gradient\\ncomputed from each. This is called mini-batch\\nstochastic gradient descent, or stochas ticgradi-\\nentdescent ( SGD) for short.\\nIt is important to note that this process is ex-\\ntremely gradual, and that the number of mini-\\nbatches and gradient steps are typically of the\\norder of several million.\\nAs with many algorithms, intuition breaks down\\nin high dimensions, and although it may seem\\nthat this procedure would be easily trapped in\\na local minimum, in reality, due to the number\\nof parameters, the design of the models, and', metadata={'source': 'rag/deep learning.pdf', 'page': 37}),\n",
       " Document(page_content='the stochasticity of the data, its efficiency is far\\ngreater than one might expect.\\nPlenty of variations of this standard strategy\\nhave been proposed. The most popular one is\\n38', metadata={'source': 'rag/deep learning.pdf', 'page': 37}),\n",
       " Document(page_content='Adam [Kingma and Ba, 2014], which keeps run-\\nning estimates of the mean and variance of each\\ncomponent of the gradient, and normalizes them\\nautomatically, avoiding scaling issues and differ-\\nent training speeds in different parts of a model.\\n39', metadata={'source': 'rag/deep learning.pdf', 'page': 38}),\n",
       " Document(page_content='3.4 Backpropagation\\nUsing gradient descent requires a tech-\\nnical means to compute âˆ‡ð“|w(w)where\\nð“=L(f(x;w);y). Given that fandLare\\nboth compositions of standard tensor opera-\\ntions, as for any mathematical expression, the\\nchain rule from differential calculus allows us to\\nget an expression of it.\\nFor the sake of making notation lighter, we will\\nnot specify at which point gradients are com-\\nputed, since the context makes it clear.\\nx(dâˆ’1)x(d)f(d)(Â·;wd)\\nâˆ‡ð“|x(dâˆ’1) âˆ‡ð“|x(d)Ã—Jf(d)|x\\nâˆ‡ð“|wdÃ—Jf(d)|w\\nFigure 3.3: Given a model f=f(D)â—¦ Â·Â·Â· â—¦ f(1), the\\nforward pass computes the outputs x(d)of the f(d)in\\norder (top, black). The backward pass computes the\\ngradients of the loss with respect to the activations x(d)\\n(bottom, blue) and the parameters wd(bottom, red)\\nbackward by multiplying them by the Jacobians.\\n40', metadata={'source': 'rag/deep learning.pdf', 'page': 39}),\n",
       " Document(page_content='Forward and backward passes\\nConsider the simple case of a composition of\\nmappings:\\nf=f(D)â—¦f(Dâˆ’1)â—¦ Â·Â·Â· â—¦ f(1).\\nThe output of f(x;w)can be computed by start-\\ning with x(0)=xand applying iteratively:\\nx(d)=f(d)\\x10\\nx(dâˆ’1);wd\\x11\\n,\\nwithx(D)as the final value.\\nThe individual scalar values of these interme-\\ndiate results x(d)are traditionally called acti-\\nvations in reference to neuron activations, the\\nvalue Dis the depth of the model, the individual\\nmappings f(d)are referred to as layers, as we\\nwill see in Â§ 4.1, and their sequential evaluation\\nis the forward pass (see Figure 3.3, top).\\nConversely, the gradient âˆ‡ð“|x(dâˆ’1)of the loss\\nwith respect to the output x(dâˆ’1)off(dâˆ’1)is\\nthe product of the gradient âˆ‡ð“|x(d)with respect\\nto the output of f(d)multiplied by the Jacobian\\nJf(dâˆ’1)|xoff(dâˆ’1)with respect to its variable\\nx. Thus, the gradients with respect to the out-\\nputs of all the f(d)s can be computed recursively\\nbackward, starting with âˆ‡ð“|x(D)=âˆ‡L|x.\\n41', metadata={'source': 'rag/deep learning.pdf', 'page': 40}),\n",
       " Document(page_content='And the gradient that we are interested in for\\ntraining, that is âˆ‡ð“|wd, is the gradient with re-\\nspect to the output of f(d)multiplied by the Ja-\\ncobian Jf(d)|woff(d)with respect to the param-\\neters.\\nThis iterative computation of the gradients with\\nrespect to the intermediate activations, com-\\nbined with that of the gradients with respect\\nto the layersâ€™ parameters, is the back ward pass\\n(see Figure 3.3, bottom). The combination of\\nthis computation with the procedure of gradient\\ndescent is called back prop agation.\\nIn practice, the implementation details of the\\nforward and backward passes are hidden from\\nprogrammers. Deep learning frameworks are\\nable to automatically construct the sequence of\\noperations to compute gradients.\\nA particularly convenient algorithm is Autograd\\n[Baydin et al., 2015], which tracks tensor opera-\\ntions and builds, on the fly, the combination of\\noperators for gradients. Thanks to this, a piece of\\nimperative programming that manipulates ten-', metadata={'source': 'rag/deep learning.pdf', 'page': 41}),\n",
       " Document(page_content='sors can automatically compute the gradient of\\nany quantity with respect to any other.\\n42', metadata={'source': 'rag/deep learning.pdf', 'page': 41}),\n",
       " Document(page_content='Resource usage\\nRegarding the computational cost, as we will\\nsee, the bulk of the computation goes into linear\\noperations, each requiring one matrix product\\nfor the forward pass and two for the products by\\nthe Jacobians for the backward pass, making the\\nlatter roughly twice as costly as the former.\\nThe mem oryrequire ment during inference is\\nroughly equal to that of the most demanding\\nindividual layer. For training, however, the back-\\nward pass requires keeping the activations com-\\nputed during the forward pass to compute the\\nJacobians, which results in a memory usage that\\ngrows proportionally to the modelâ€™s depth. Tech-\\nniques exist to trade the memory usage for com-\\nputation by either relying on reversible layers\\n[Gomez et al., 2017], or using check point ing,\\nwhich consists of storing activations for some\\nlayers only and recomputing the others on the fly\\nwith partial forward passes during the backward\\npass [Chen et al., 2016].\\nVanishing gradient', metadata={'source': 'rag/deep learning.pdf', 'page': 42}),\n",
       " Document(page_content='Vanishing gradient\\nA key historical issue when training a large net-\\nwork is that when the gradient propagates back-\\nwards through an operator, it may be scaled by a\\n43', metadata={'source': 'rag/deep learning.pdf', 'page': 42}),\n",
       " Document(page_content='multiplicative factor, and consequently decrease\\nor increase exponentially when it traverses many\\nlayers. A standard method to prevent it from\\nexploding is gradientnorm clipping, which con-\\nsists of re-scaling the gradient to set its norm to\\na fixed threshold if it is above it [Pascanu et al.,\\n2013].\\nWhen the gradient decreases exponentially, this\\nis called the vanishinggradient, and it may\\nmake the training impossible, or, in its milder\\nform, cause different parts of the model to be\\nupdated at different speeds, degrading their co-\\nadaptation [Glorot and Bengio, 2010].\\nAs we will see in Chapter 4, multiple techniques\\nhave been developed to prevent this from hap-\\npening, reflecting a change in perspective that\\nwas crucial to the success of deep-learning: in-\\nstead of trying to improve generic optimization\\nmethods, the effort shifted to engineering the\\nmodels themselves to make them optimizable.\\n44', metadata={'source': 'rag/deep learning.pdf', 'page': 43}),\n",
       " Document(page_content='3.5 The value of depth\\nAs the term â€œdeep learningâ€ indicates, useful\\nmodels are generally compositions of long se-\\nries of mappings. Training them with gradient\\ndescent results in a sophisticated co-adaptation\\nof the mappings, even though this procedure is\\ngradual and local.\\nWe can illustrate this behavior with a simple\\nmodelR2â†’R2that combines eight layers, each\\nmultiplying its input by a 2Ã—2matrix and ap-\\nplying Tanh per component, with a final linear\\nclassifier. This is a simplified version of the stan-\\ndard Multi -Layer Perceptron that we will see in\\nÂ§ 5.1.\\nIf we train this model with SGD and cross -en-\\ntropy on a toy binary classification task (Figure\\n3.4, top left), the matrices co-adapt to deform the\\nspace until the classification is correct, which\\nimplies that the data have been made linearly\\nseparable before the final affine operation (Fig-\\nure 3.4, bottom right).\\nSuch an example gives a glimpse of what a deep\\nmodel can achieve; however, it is partially mis-', metadata={'source': 'rag/deep learning.pdf', 'page': 44}),\n",
       " Document(page_content='leading due to the low dimension of both the sig-\\nnal to process and the internal representations.\\nEverything is kept in 2D here for the sake of\\n45', metadata={'source': 'rag/deep learning.pdf', 'page': 44}),\n",
       " Document(page_content='d= 0 d= 1 d= 2\\nd= 3 d= 4 d= 5\\nd= 6 d= 7 d= 8\\nFigure 3.4: Each plot shows the deformation of the\\nspace and the resulting positioning of the training\\npoints in R2afterdlayers of processing, starting with\\nthe input to the model itself (top left). The oblique line\\nin the last plot (bottom right) shows the final affine\\ndecision.\\n46', metadata={'source': 'rag/deep learning.pdf', 'page': 45}),\n",
       " Document(page_content='visualization, while real models take advantage\\nof representations in high dimensions, which, in\\nparticular, facilitates the optimization by provid-\\ning many degrees of freedom.\\nEmpirical evidence accumulated over twenty\\nyears demonstrates that state-of-the-art perfor-\\nmance across application domains necessitates\\nmodels with tens of layers, such as resid ualnet-\\nworks (see Â§ 5.2) or Trans form ers (see Â§ 5.3).\\nTheoretical results show that, for a fixed com-\\nputational budget or number of parameters, in-\\ncreasing the depth leads to a greater complexity\\nof the resulting mapping [Telgarsky, 2016].\\n47', metadata={'source': 'rag/deep learning.pdf', 'page': 46}),\n",
       " Document(page_content='3.6 Training protocols\\nTraining a deep network requires defining a pro-\\ntocol to make the most of computation and data,\\nand to ensure that performance will be good on\\nnew data.\\nAs we saw in Â§ 1.3, the performance on the train-\\ning samples may be misleading, so in the sim-\\nplest setup one needs at least two sets of samples:\\none is a train ingset, used to optimize the model\\nparameters, and the other is a testset, to evaluate\\nthe performance of the trained model.\\nAdditionally, there are usually hyper-parame-\\nters to adapt, in particular, those related to the\\nmodel architecture, the learning rate, and the\\nregularization terms in the loss. In that case,\\none needs a validation set that is disjoint from\\nboth the training and test sets to assess the best\\nconfiguration.\\nThe full training is usually decomposed into\\nepochs, each of which corresponds to going\\nthrough all the training examples once. The\\nusual dynamic of the losses is that the training', metadata={'source': 'rag/deep learning.pdf', 'page': 47}),\n",
       " Document(page_content='loss decreases as long as the optimization runs,\\nwhile the validation loss may reach a minimum\\nafter a certain number of epochs and then start\\nto increase, reflecting an overfitting regime, as\\n48', metadata={'source': 'rag/deep learning.pdf', 'page': 47}),\n",
       " Document(page_content='Loss\\nNumber of epochsOverfitting\\nTrainingValidation\\nFigure 3.5: As training progresses, a modelâ€™s perfor-\\nmance is usually monitored through losses. The train-\\ning loss is the one driving the optimization process and\\ngoes down, while the validation loss is estimated on\\nan other set of examples to assess the overfitting of\\nthe model. Overfitting appears when the model starts\\nto take into account random structures specific to the\\ntraining set at hand, resulting in the validation loss\\nstarting to increase.\\nintroduced in Â§ 1.3 and illustrated in Figure 3.5.\\nParadoxically, although they should suffer from\\nsevere overfitting due to their capacity, large\\nmodels usually continue to improve as training\\nprogresses. This may be due to the inductive\\nbias of the model becoming the main driver of\\noptimization when performance is near perfect\\n49', metadata={'source': 'rag/deep learning.pdf', 'page': 48}),\n",
       " Document(page_content='on the training set [Belkin et al., 2018].\\nAn important design choice is the learn ingrate\\nsched ule during training, that is, the specifica-\\ntion of the value of the learn ingrate at each iter-\\nation of the gradient descent. The general policy\\nis that the learning rate should be initially large\\nto avoid having the optimization being trapped\\nin a bad local minimum early, and that it should\\nget smaller so that the optimized parameter val-\\nues do not bounce around and reach a good min-\\nimum in a narrow valley of the loss landscape.\\nThe training of very large models may take\\nmonths on thousands of powerful GPUs and\\nhave a financial cost of several million dollars. At\\nthis scale, the training may involve many man-\\nual interventions, informed, in particular, by the\\ndynamics of the loss evolution.\\nFine-tuning\\nIt is often beneficial to adapt an already trained\\nmodel to a new task, referred to as a down stream\\ntask.\\nIt can be because the amount of data for the', metadata={'source': 'rag/deep learning.pdf', 'page': 49}),\n",
       " Document(page_content='original task is plentiful, while they are lim-\\nited for the downstream task, and the two tasks\\nshare enough similarities that statistical struc-\\n50', metadata={'source': 'rag/deep learning.pdf', 'page': 49}),\n",
       " Document(page_content='tures learned for the first provide a good induc-\\ntive bias for the second. It can also be to limit the\\ntraining cost by reusing the patterns encoded in\\nan existing model.\\nAdapting a pre-trained model to a specific task\\nis achieved with fine-tuning, which is a standard\\ntraining procedure for the downstream task, but\\nwhich starts from the pre-trained model instead\\nof using a random initialization.\\nThis is the main strategy for most computer vi-\\nsion applications which generally use a model\\npre-trained for classification on ImageNet [Deng\\net al., 2009] (see Â§ 6.3 and Â§ 6.4), and it is also how\\npurely generative pre-trained Large Language\\nModels are re-purposed as assistant-like models,\\nable to produce interactive dialogues (see Â§ 7.1).\\nWe come back to techniques to cope with lim-\\nited resources in inference and for fine-tuning\\nin Chapter 8.\\n51', metadata={'source': 'rag/deep learning.pdf', 'page': 50}),\n",
       " Document(page_content='3.7 The benefits of scale\\nThere is an accumulation of empirical results\\nshowing that performance, for instance, esti-\\nmated through the loss on test data, improves\\nwith the amount of data according to remarkable\\nscalinglaws, as long as the model size increases\\ncorrespondingly [Kaplan et al., 2020] (see Figure\\n3.6).\\nBenefiting from these scaling laws in the multi-\\nbillion sample regime is possible in part thanks to\\nthe structure of deep models which can be scaled\\nup arbitrarily, as we will see, by increasing the\\nnumber of layers or feature dimensions. But it\\nis also made possible by the distributed nature\\nof the computation they implement, and by the\\nstochas ticgradientdescent, which requires only\\na fraction of the data at a time and can operate\\nwith datasets whose size is orders of magnitude\\ngreater than that of the computing deviceâ€™s mem-\\nory. This has resulted in an exponential growth\\nof the models, as illustrated in Figure 3.7.\\nTypical vision models have 10â€“100million train -', metadata={'source': 'rag/deep learning.pdf', 'page': 51}),\n",
       " Document(page_content='able parameters and require 1018â€“1019FLOPs\\nfor training [He et al., 2015; Sevilla et al., 2022].\\nLanguage models have from 100million to hun-\\ndreds of billions of trainable parameters and re-\\n52', metadata={'source': 'rag/deep learning.pdf', 'page': 51}),\n",
       " Document(page_content='Test loss Test loss Test lossCompute (peta-FLOP/s-day)\\nDataset size (tokens)\\nNumber of parameters\\nFigure 3.6: Test loss of a language model vs. the amount\\nof computation in petaflop/s-day, the dataset size in\\ntokens, that is fragments of words, and the model size\\nin parameters [Kaplan et al., 2020].\\n53', metadata={'source': 'rag/deep learning.pdf', 'page': 52}),\n",
       " Document(page_content='Dataset Year Nb. of images Size\\nImageNet 2012 1.2M 150Gb\\nCityscape 2016 25K 60Gb\\nLAION-5B 2022 5.8B 240Tb\\nDataset Year Nb. of books Size\\nWMT-18-de-en 2018 14M 8Gb\\nThe Pile 2020 1.6B 825Gb\\nOSCAR 2020 12B 6Tb\\nTable 3.1: Some examples of publicly available datasets.\\nThe equivalent number of books is an indicative esti-\\nmate for 250 pages of 2000 characters per book.\\nquire 1020â€“1023FLOPs for training [Devlin et al.,\\n2018; Brown et al., 2020; Chowdhery et al., 2022;\\nSevilla et al., 2022]. These latter models require\\nmachines with multiple high-end GPUs.\\nTraining these large models is impossible using\\ndatasets with a detailed ground-truth costly to\\nproduce, which can only be of moderate size.\\nInstead, it is done with datasets automatically\\nproduced by combining data available on the\\ninternet with minimal curation, if any. These\\nsets may combine multiple modalities, such as\\ntext and images from web pages, or sound and\\nimages from videos, which can be used for large-\\nscale supervised training.', metadata={'source': 'rag/deep learning.pdf', 'page': 53}),\n",
       " Document(page_content='As of 2024, the most powerful models are the so-\\n54', metadata={'source': 'rag/deep learning.pdf', 'page': 53}),\n",
       " Document(page_content='2015 2020101810211024\\n1KWh1MWh1GWh\\nAlexNetResNetAlphaGoAlphaZero\\nTransformer\\nGPTBERTGPT-2GPT-3\\nViTPaLM\\nLaMDA\\nWhisper\\nVGG16\\nGoogLeNetCLIP-ViT\\nYearTraining cost (FLOP)\\nFigure 3.7: Training costs in number of FLOP of some\\nlandmark models [Sevilla et al., 2023]. The colors in-\\ndicate the domains of application: Computer Vision\\n(blue), Natural Language Processing (red), or other\\n(black). The dashed lines correspond to the energy con-\\nsumption using A100s SXM in 16-bit precision. For\\nreference, the total electricity consumption in the US in\\n2021 was 3920 TWh.\\n55', metadata={'source': 'rag/deep learning.pdf', 'page': 54}),\n",
       " Document(page_content='called Large Language Models (LLMs), which we\\nwill see in Â§ 5.3 and Â§ 7.1, trained on extremely\\nlarge text datasets (see Table 3.1).\\n56', metadata={'source': 'rag/deep learning.pdf', 'page': 55}),\n",
       " Document(page_content='Part II\\nDeep Models\\n57', metadata={'source': 'rag/deep learning.pdf', 'page': 56}),\n",
       " Document(page_content='Chapter 4\\nModel Components\\nA deep model is nothing more than a complex\\ntensorial computation that can ultimately be\\ndecomposed into standard mathematical oper-\\nations from linear algebra and analysis. Over\\nthe years, the field has developed a large collec-\\ntion of high-level modules with a clear semantic,\\nand complex models combining these modules,\\nwhich have proven to be effective in specific ap-\\nplication domains.\\nEmpirical evidence and theoretical results show\\nthat greater performance is achieved with deeper\\narchitectures, that is, long compositions of map-\\npings. As we saw in section Â§ 3.4, training such\\na model is challenging due to the vanishinggra-\\ndient, and multiple important technical contri-\\nbutions have mitigated this issue.\\n58', metadata={'source': 'rag/deep learning.pdf', 'page': 57}),\n",
       " Document(page_content='4.1 The notion of layer\\nWe call layers standard complex compounded\\ntensor operations that have been designed and\\nempirically identified as being generic and effi-\\ncient. They often incorporate trainable param-\\neters and correspond to a convenient level of\\ngranularity for designing and describing large\\ndeep models. The term is inherited from sim-\\nple multi-layer neural networks, even though\\nmodern models may take the form of a complex\\ngraph of such modules, incorporating multiple\\nparallel pathways.\\nÃ—K\\nXfgn=4Y\\n32Ã—324Ã—4\\nIn the following pages, I try to stick to the con-\\nvention for model depiction illustrated above:\\nâ€¢operators / layers are depicted as boxes,\\nâ€¢darker coloring indicates that they embed\\ntrainable parameters,\\nâ€¢non-default valued hyper-parameters are\\n59', metadata={'source': 'rag/deep learning.pdf', 'page': 58}),\n",
       " Document(page_content='added in blue on their right,\\nâ€¢a dashed outer frame with a multiplicative\\nfactor indicates that a group of layers is repli-\\ncated in series, each with its own set of trainable\\nparameters, if any, and\\nâ€¢in some cases, the dimension of their output is\\nspecified on the right when it differs from their\\ninput.\\nAdditionally, layers that have a complex internal\\nstructure are depicted with a greater height.\\n60', metadata={'source': 'rag/deep learning.pdf', 'page': 59}),\n",
       " Document(page_content='4.2 Linear layers\\nThe most important modules in terms of compu-\\ntation and number of parameters are the Linear\\nlayers. They benefit from decades of research\\nand engineering in algorithmic and chip design\\nfor matrix operations.\\nNote that the term â€œlinearâ€ in deep learning gen-\\nerally refers improperly to an affine operation,\\nwhich is the sum of a linear expression and a\\nconstant bias.\\nFully connected layers\\nThe most basic linear layer is the fully connected\\nlayer, parameterized by a trainable weight ma-\\ntrixWof size Dâ€²Ã—Dandbiasvectorbof dimen-\\nsionDâ€². It implements an affine transformation\\ngeneralized to arbitrary tensor shapes, where\\nthe supplementary dimensions are interpreted\\nas vector indexes. Formally, given an input X\\nof dimension D1Ã—Â·Â·Â·Ã— DKÃ—D, it computes an\\noutput Yof dimension D1Ã—Â·Â·Â·Ã— DKÃ—Dâ€²with\\nâˆ€d1,...,d K,\\nY[d1,...,d K] =WX[d1,...,d K]+b.\\nWhile at first sight such an affine operation\\n61', metadata={'source': 'rag/deep learning.pdf', 'page': 60}),\n",
       " Document(page_content='seems limited to geometric transformations such\\nas rotations, symmetries, and translations, it can\\nin fact do more than that. In particular, projec-\\ntions for dimension reduction or signal filtering,\\nbut also, from the perspective of the dot product\\nbeing a measure of similarity, a matrix-vector\\nproduct can be interpreted as computing match-\\ning scores between the queries, as encoded by\\nthe input vectors, and keys, as encoded by the\\nmatrix rows.\\nAs we saw in Â§ 3.3, the gradient descent starts\\nwith the parametersâ€™ random initialization. If\\nthis is done too naively, as seen in Â§ 3.4, the net-\\nwork may suffer from exploding or vanishing\\nactivations and gradients [Glorot and Bengio,\\n2010]. Deep learning frameworks implement ini-\\ntialization methods that in particular scale the\\nrandom parameters according to the dimension\\nof the input to keep the variance of the activa-\\ntions constant and prevent pathological behav-\\niors.\\nConvolutional layers\\nA linear layer can take as input an arbitrarily-', metadata={'source': 'rag/deep learning.pdf', 'page': 61}),\n",
       " Document(page_content='shaped tensor by reshaping it into a vector, as\\nlong as it has the correct number of coefficients.\\nHowever, such a layer is poorly adapted to deal-\\n62', metadata={'source': 'rag/deep learning.pdf', 'page': 61}),\n",
       " Document(page_content='Ï•\\nXY\\nÏˆY\\nXÏ•\\nXY\\nÏˆY\\nXÏ•\\nXY\\nÏˆY\\nX\\n...\\n1D convolution...\\n1D transposed\\nconvolution\\nFigure 4.1: A 1D convolution (left) takes as input\\naDÃ—Ttensor X, applies the same affine mapping\\nÏ•(Â·;w)to every sub-tensor of shape DÃ—K, and stores\\nthe resulting Dâ€²Ã—1tensors into Y. A 1D transposed\\nconvolution (right) takes as input a DÃ—Ttensor, ap-\\nplies the same affine mapping Ïˆ(Â·;w)to every sub-\\ntensor of shape DÃ—1, and sums the shifted resulting\\nDâ€²Ã—Ktensors. Both can process inputs of different\\nsizes.\\n63', metadata={'source': 'rag/deep learning.pdf', 'page': 62}),\n",
       " Document(page_content='XYÏ•\\n2D convolutionHWD\\nYXÏˆ\\n2D transposed\\nconvolution\\nFigure 4.2: A 2D convolution (left) takes as input a\\nDÃ—HÃ—Wtensor X, applies the same affine mapping\\nÏ•(Â·;w)to every sub-tensor of shape DÃ—KÃ—L, and\\nstores the resulting Dâ€²Ã—1Ã—1tensors into Y. A 2D\\ntransposed convolution (right) takes as input a DÃ—\\nHÃ—Wtensor, applies the same affine mapping Ïˆ(Â·;w)\\nto every DÃ—1Ã—1sub-tensor, and sums the shifted\\nresulting Dâ€²Ã—KÃ—Ltensors into Y.\\ning with large tensors, since the number of pa-\\nrameters and number of operations are propor-\\ntional to the product of the input and output\\ndimensions. For instance, to process an RGB\\nimage of size 256Ã—256as input and compute a\\nresult of the same size, it would require approxi-\\nmately 4Ã—1010parameters and multiplications.\\nBesides these practical issues, most of the high-\\ndimension signals are strongly structured. For\\n64', metadata={'source': 'rag/deep learning.pdf', 'page': 63}),\n",
       " Document(page_content='Ï•Y\\nXÏ•Y\\nX\\ns= 2...\\nStrideÏ•Y\\nX\\nd= 2\\nDilationÏ•Y\\nX\\np= 2\\nPadding\\nFigure 4.3: Beside its kernel size and number of input\\n/ output channels, a convolution admits three hyper-\\nparameters: the stride s(left) modulates the step size\\nwhen going through the input tensor, the padding p\\n(top right) specifies how many zero entries are added\\naround the input tensor before processing it, and the\\ndilation d(bottom right) parameterizes the index count\\nbetween coefficients of the filter.\\n65', metadata={'source': 'rag/deep learning.pdf', 'page': 64}),\n",
       " Document(page_content='instance, images exhibit short-term correlations\\nand statistical stationarity with respect to trans-\\nlation, scaling, and certain symmetries. This\\nis not reflected in the inductivebias of a fully\\nconnected layer, which completely ignores the\\nsignal structure.\\nTo leverage these regularities, the tool of choice\\nisconvolutional layers, which are also affine, but\\nprocess time-series or 2D signals locally, with\\nthe same operator everywhere.\\nA1Dconvolution is mainly defined by three hy-\\nper-parameters: its kernelsizeK, its number of\\ninput channels D, its number of output chan-\\nnelsDâ€², and by the trainable parameters wof an\\naffine mapping Ï•(Â·;w) :RDÃ—Kâ†’RDâ€²Ã—1.\\nIt can process any tensor Xof size DÃ—Twith\\nTâ‰¥K, and applies Ï•(Â·;w)to every sub-tensor\\nof size DÃ—KofX, storing the results in a tensor\\nYof size Dâ€²Ã—(Tâˆ’K+1), as pictured in Figure\\n4.1 (left).\\nA2Dconvolution is similar but has a KÃ—Lker-\\nnel and takes as input a DÃ—HÃ—Wtensor (see\\nFigure 4.2, left).\\nBoth operators have for trainable parameters', metadata={'source': 'rag/deep learning.pdf', 'page': 65}),\n",
       " Document(page_content='those of Ï•that can be envisioned as Dâ€²filters\\n66', metadata={'source': 'rag/deep learning.pdf', 'page': 65}),\n",
       " Document(page_content='of size DÃ—KorDÃ—KÃ—Lrespectively, and a\\nbiasvector of dimension Dâ€².\\nSuch a layer is equiv ariant to translation, mean-\\ning that if the input signal is translated, the out-\\nput is similarly transformed. This property re-\\nsults in a desirable inductivebias when dealing\\nwith a signal whose distribution is invariant to\\ntranslation.\\nThey also admit three additional hyper-parame-\\nters, illustrated on Figure 4.3:\\nâ€¢Thepadding specifies how many zero coeffi-\\ncients should be added around the input tensor\\nbefore processing it, particularly to maintain the\\ntensor size when the kernel size is greater than\\none. Its default value is 0.\\nâ€¢Thestride specifies the step size used when go-\\ning through the input, allowing one to reduce the\\noutput size geometrically by using large steps.\\nIts default value is 1.\\nâ€¢Thedilation specifies the index count between\\nthe filter coefficients of the local affine opera-\\ntor. Its default value is 1, and greater values\\ncorrespond to inserting zeros between the coef-', metadata={'source': 'rag/deep learning.pdf', 'page': 66}),\n",
       " Document(page_content='ficients, which increases the filter / kernel size\\nwhile keeping the number of trainable parame-\\n67', metadata={'source': 'rag/deep learning.pdf', 'page': 66}),\n",
       " Document(page_content='HW\\nModel depth\\nFigure 4.4: Given an activation in a series of convolu-\\ntion layers, here in red, its receptivefield is the area in\\nthe input signal, in blue, that modulates its value. Each\\nintermediate convolutional layer increases the width\\nand height of that area by roughly those of the kernel.\\nters unchanged.\\nExcept for the number of channels, a convo-\\nlutionâ€™s output is usually smaller than its in-\\nput. In the 1D case without padding nor di-\\nlation, if the input is of size T, the kernel of\\nsizeK, and the stride is S, the output is of size\\nTâ€²= (Tâˆ’K)/S+1.\\nGiven an activation computed by a convolutional\\nlayer, or the vector of values for all the channels\\nat a certain location, the portion of the input\\n68', metadata={'source': 'rag/deep learning.pdf', 'page': 67}),\n",
       " Document(page_content='signal that it depends on is called its receptive\\nfield (see Figure 4.4). One of the HÃ—Wsub-\\ntensors corresponding to a single channel of a\\nDÃ—HÃ—Wactivation tensor is called an activa-\\ntion map.\\nConvolutions are used to recombine information,\\ngenerally to reduce the spatial size of the rep-\\nresentation, in exchange for a greater number\\nof channels, which translates into a richer local\\nrepresentation. They can implement differential\\noperators such as edge-detectors, or template\\nmatching mechanisms. A succession of such lay-\\ners can also be envisioned as a compositional and\\nhierarchical representation [Zeiler and Fergus,\\n2014], or as a diffusion process in which infor-\\nmation can be transported by half the kernel size\\nwhen passing through a layer.\\nA converse operation is the trans posed convo-\\nlution that also consists of a localized affine op-\\nerator, defined by similar hyper and trainable\\nparameters as the convolution, but which, for\\ninstance, in the 1D case, applies an affine map-', metadata={'source': 'rag/deep learning.pdf', 'page': 68}),\n",
       " Document(page_content='pingÏˆ(Â·;w) :RDÃ—1â†’RDâ€²Ã—K,to every DÃ—1\\nsub-tensor of the input, and sums the shifted\\nDâ€²Ã—Kresulting tensors to compute its output.\\nSuch an operator increases the size of the signal\\nand can be understood intuitively as a synthe-\\n69', metadata={'source': 'rag/deep learning.pdf', 'page': 68}),\n",
       " Document(page_content='sis process (see Figure 4.1, right, and Figure 4.2,\\nright).\\nA series of convolutional layers is the usual ar-\\nchitecture for mapping a large-dimension signal,\\nsuch as an image or a sound sample, to a low-\\ndimension tensor. This can be used, for instance,\\nto get class scores for classification or a com-\\npressed representation. Transposed convolution\\nlayers are used the opposite way to build a large-\\ndimension signal from a compressed representa-\\ntion, either to assess that the compressed repre-\\nsentation contains enough information to recon-\\nstruct the signal or for synthesis, as it is easier\\nto learn a density model over a low-dimension\\nrepresentation. We will revisit this in Â§ 5.2.\\n70', metadata={'source': 'rag/deep learning.pdf', 'page': 69}),\n",
       " Document(page_content='4.3 Activation functions\\nIf a network were combining only linear com-\\nponents, it would itself be a linear operator,\\nso it is essential to have non-linearoperations.\\nThese are implemented in particular with activa-\\ntion functions, which are layers that transform\\neach component of the input tensor individually\\nthrough a mapping, resulting in a tensor of the\\nsame shape.\\nThere are many different activation functions,\\nbut the most used is the RectifiedLinearUnit\\n(ReLU) [Glorot et al., 2011], which sets nega-\\ntive values to zero and keeps positive values un-\\nchanged (see Figure 4.5, top right):\\nrelu(x) =(\\n0ifx <0,\\nxotherwise .\\nGiven that the core training strategy of deep-\\nlearning relies on the gradient, it may seem prob-\\nlematic to have a mapping that is not differen-\\ntiable at zero and constant on half the real line.\\nHowever, the main property gradient descent\\nrequires is that the gradient is informative on\\naverage. Parameter initialization and data nor-', metadata={'source': 'rag/deep learning.pdf', 'page': 70}),\n",
       " Document(page_content='malization make half of the activations positive\\n71', metadata={'source': 'rag/deep learning.pdf', 'page': 70}),\n",
       " Document(page_content='Tanh ReLU\\nLeaky ReLU GELU\\nFigure 4.5: Activation functions.\\nwhen the training starts, ensuring that this is the\\ncase.\\nBefore the generalization of ReLU, the standard\\nactivation function was the hyperbolic tangent\\n(Tanh, see Figure 4.5, top left) which saturates\\nexponentially fast on both the negative and pos-\\nitive sides, aggravating the vanishing gradient.\\nOther popular activation functions follow the\\nsame idea of keeping positive values unchanged\\nand squashing the negative values. Leaky ReLU\\n[Maas et al., 2013] applies a small positive multi-\\n72', metadata={'source': 'rag/deep learning.pdf', 'page': 71}),\n",
       " Document(page_content='plying factor to the negative values (see Figure\\n4.5, bottom left):\\nleakyrelu( x) =(\\naxifx <0,\\nxotherwise .\\nAnd GELU [Hendrycks and Gimpel, 2016] is de-\\nfined using the cumulative distribution function\\nof the Gaussian distribution, that is:\\ngelu(x) =xP(Zâ‰¤x),\\nwhere Zâˆ¼ð’©(0,1). It roughly behaves like a\\nsmooth ReLU (see Figure 4.5, bottom right).\\nThe choice of an activation function, in partic-\\nular among the variants of ReLU, is generally\\ndriven by empirical performance.\\n73', metadata={'source': 'rag/deep learning.pdf', 'page': 72}),\n",
       " Document(page_content='4.4 Pooling\\nA classical strategy to reduce the signal size is to\\nuse a pooling operation that combines multiple\\nactivations into one that ideally summarizes the\\ninformation. The most standard operation of this\\nclass is the max pooling layer, which, similarly\\nto convolution, can operate in 1D and 2D and is\\ndefined by a kernelsize.\\nIn its standard form, this layer computes the\\nmaximum activation per channel, over non-\\noverlapping sub-tensors of spatial size equal to\\nthe kernel size. These values are stored in a re-\\nsult tensor with the same number of channels\\nas the input, and whose spatial size is divided\\nby the kernel size. As with the convolution, this\\noperator has three hyper-parameters: padding,\\nstride, and dilation, with the stride being equal\\nto the kernel size by default. A smaller stride\\nresults in a larger resulting tensor, following the\\nsame formula as for convolutions (see Â§ 4.2).\\nThe max operation can be intuitively interpreted\\nas a logical disjunction, or, when it follows a', metadata={'source': 'rag/deep learning.pdf', 'page': 73}),\n",
       " Document(page_content='series of convolutional layers that compute lo-\\ncal scores for the presence of parts, as a way\\nof encoding that at least one instance of a part\\nis present. It loses precise location, making it\\n74', metadata={'source': 'rag/deep learning.pdf', 'page': 73}),\n",
       " Document(page_content='Y\\nXmaxY\\nXmaxY\\nXmax\\n...\\n1D max pooling\\nFigure 4.6: A 1D max pooling takes as input a DÃ—T\\ntensor X, computes the max over non-overlapping 1Ã—\\nLsub-tensors (in blue) and stores the resulting values\\n(in red) in a DÃ—(T/L)tensor Y.\\n75', metadata={'source': 'rag/deep learning.pdf', 'page': 74}),\n",
       " Document(page_content='invariant to local deformations.\\nA standard alternative is the averagepooling\\nlayer that computes the average instead of the\\nmaximum over the sub-tensors. This is a linear\\noperation, whereas max pooling is not.\\n76', metadata={'source': 'rag/deep learning.pdf', 'page': 75}),\n",
       " Document(page_content='4.5 Dropout\\nSome layers have been designed to explicitly\\nfacilitate training or improve the learned repre-\\nsentations.\\nOne of the main contributions of that sort was\\ndropout [Srivastava et al., 2014]. Such a layer\\nhas no trainable parameters, but one hyper-\\nparameter, p, and takes as input a tensor of arbi-\\ntrary shape.\\nIt is usually switched off during testing, in which\\ncase its output is equal to its input. When it is ac-\\ntive, it has a probability pof setting to zero each\\nactivation of the input tensor independently, and\\nit re-scales all the activations by a factor of1\\n1âˆ’p\\nto maintain the expected value unchanged (see\\nFigure 4.7).\\nThe motivation behind dropout is to favor\\nmeaningful individual activation and discourage\\ngroup representation. Since the probability that\\na group of kactivations remains intact through\\na dropout layer is (1âˆ’p)k, joint representations\\nbecome unreliable, making the training proce-\\ndure avoid them. It can also be seen as a noise', metadata={'source': 'rag/deep learning.pdf', 'page': 76}),\n",
       " Document(page_content='injection that makes the training more robust.\\nWhen dealing with images and 2D tensors, the\\n77', metadata={'source': 'rag/deep learning.pdf', 'page': 76}),\n",
       " Document(page_content='11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111\\n11111 0\\n0\\n00 0\\n00 00\\n0\\n0Ã— Ã—1\\n1âˆ’p\\nTrain TestY\\nXY\\nX\\nFigure 4.7: Dropout can process a tensor of arbitrary\\nshape. During training (left), it sets activations at ran-\\ndom to zero with probability pand applies a multiply-\\ning factor to keep the expected values unchanged. Dur-\\ning test (right), it keeps all the activations unchanged.\\nshort-term correlation of the signals and the re-\\nsulting redundancy negate the effect of dropout,\\nsince activations set to zero can be inferred from\\ntheir neighbors. Hence, dropout for 2D tensors\\nsets entire channels to zero instead of individual\\nactivations (see Figure 4.8).\\nAlthough dropout is generally used to improve\\ntraining and is inactive during inference, it can\\nbe used in certain setups as a randomization\\nstrategy, for instance, to estimate empirically\\nconfidence scores [Gal and Ghahramani, 2015].\\n78', metadata={'source': 'rag/deep learning.pdf', 'page': 77}),\n",
       " Document(page_content='BD\\nH,W\\n1101001 Ã— Ã—1\\n1âˆ’p\\nTrain Test\\nFigure 4.8: 2D signals such as images generally exhibit\\nstrong short-term correlation and individual activa-\\ntions can be inferred from their neighbors. This redun-\\ndancy nullifies the effect of the standard unstructured\\ndropout, so the usual dropout layer for 2D tensors drops\\nentire channels instead of individual values.\\n79', metadata={'source': 'rag/deep learning.pdf', 'page': 78}),\n",
       " Document(page_content='4.6 Normalizing layers\\nAn important class of operators to facilitate the\\ntraining of deep architectures are the normaliz-\\ninglayers, which force the empirical mean and\\nvariance of groups of activations.\\nThe main layer in that family is batch normal-\\nization [Ioffe and Szegedy, 2015], which is the\\nonly standard layer to process batches instead\\nof individual samples. It is parameterized by a\\nhyper-parameter Dand two series of trainable\\nscalar parameters Î²1,...,Î² DandÎ³1,...,Î³ D.\\nGiven a batch of Bsamples x1,...,x Bof dimen-\\nsionD, it first computes for each of the Dcom-\\nponents an empirical mean Ë†mdand variance Ë†vd\\nacross the batch:\\nË†md=1\\nBBX\\nb=1xb,d\\nË†vd=1\\nBBX\\nb=1(xb,dâˆ’Ë†md)2,\\nfrom which it computes for every component\\nxb,da normalized value zb,d, with empirical\\nmean 0and variance 1, and from it the final\\nresult value yb,dwith mean Î²dand standard de-\\n80', metadata={'source': 'rag/deep learning.pdf', 'page': 79}),\n",
       " Document(page_content='BD\\nH,W\\n(Â· âˆ’Ë†md)/âˆšË†vd+ÏµÎ³dÂ·+Î²d\\n(Â· âˆ’Ë†mb)/âˆšË†vb+ÏµÎ³d,h,wÂ·+Î²d,h,w\\nbatchnorm layernorm\\nFigure 4.9: Batch normalization (left) normalizes in\\nmean and variance each group of activations for a\\ngiven d, and scales/shifts that same group of activation\\nwith learned parameters for each d. Layer normaliza-\\ntion (right) normalizes each group of activations for a\\ncertain b, and scales/shifts each group of activations\\nfor a given d,h,w with learned parameters indexed by\\nthe same.\\n81', metadata={'source': 'rag/deep learning.pdf', 'page': 80}),\n",
       " Document(page_content='viation Î³d:\\nâˆ€b, z b,d=xb,dâˆ’Ë†mdâˆšË†vd+Ïµ\\nyb,d=Î³dzb,d+Î²d.\\nBecause this normalization is defined across a\\nbatch, it is done only during training. During\\ntesting, the layer transforms individual samples\\naccording to the Ë†mds and Ë†vds estimated with a\\nmoving average over the full training set, which\\nboils down to a fixed affine transformation per\\ncomponent.\\nThe motivation behind batch normalization was\\nto avoid that a change in scaling in an early layer\\nof the network during training impacts all the\\nlayers that follow, which then have to adapt their\\ntrainable parameters accordingly. Although the\\nactual mode of action may be more complicated\\nthan this initial motivation, this layer consider-\\nably facilitates the training of deep models.\\nIn the case of 2D tensors, to follow the prin-\\nciple of convolutional layers of processing all\\nlocations similarly, the normalization is done\\nper-channel across all 2D positions, and Î²and\\nÎ³remain vectors of dimension Dso that the', metadata={'source': 'rag/deep learning.pdf', 'page': 81}),\n",
       " Document(page_content='scaling/shift does not depend on the 2D posi-\\ntion. Hence, if the tensor to be processed is\\n82', metadata={'source': 'rag/deep learning.pdf', 'page': 81}),\n",
       " Document(page_content='of shape BÃ—DÃ—HÃ—W, the layer computes\\n( Ë†md,Ë†vd), ford= 1,...,D from the correspond-\\ningBÃ—HÃ—Wslice, normalizes it accordingly,\\nand finally scales and shifts its components with\\nthe trainable parameters Î²dandÎ³d.\\nSo, given a BÃ—Dtensor, batch normalization\\nnormalizes it across band scales/shifts it ac-\\ncording to d, which can be implemented as a\\ncomponent-wise product by Î³and a sum with\\nÎ². Given a BÃ—DÃ—HÃ—Wtensor, it normal-\\nizes across b,h,w and scales/shifts according to\\nd(see Figure 4.9, left).\\nThis can be generalized depending on these di-\\nmensions. For instance, layer normalization [Ba\\net al., 2016] computes moments and normalizes\\nacross all components of individual samples, and\\nscales and shifts components individually (see\\nFigure 4.9, right). So, given a BÃ—Dtensor, it\\nnormalizes across dand scales/shifts also accord-\\ning to the same. Given a BÃ—DÃ—HÃ—Wtensor,\\nit normalizes it across d,h,w and scales/shifts\\naccording to the same.\\nContrary to batch normalization, since it pro-', metadata={'source': 'rag/deep learning.pdf', 'page': 82}),\n",
       " Document(page_content='cesses samples individually, layer normalization\\nbehaves the same during training and testing.\\n83', metadata={'source': 'rag/deep learning.pdf', 'page': 82}),\n",
       " Document(page_content='4.7 Skip connections\\nAnother technique that mitigates the vanishing\\ngradient and allows the training of deep archi-\\ntectures are skip connections [Long et al., 2014;\\nRonneberger et al., 2015]. They are not layers\\nper se, but an architectural design in which out-\\nputs of some layers are transported as-is to other\\nlayers further in the model, bypassing process-\\ning in between. This unmodified signal can be\\nconcatenated or added to the input of the layer\\nthe connection branches into (see Figure 4.10). A\\nparticular type of skip connections are the resid -\\nualconnections which combine the signal with\\na sum, and usually skip only a few layers (see\\nFigure 4.10, right).\\nThe most desirable property of this design is to\\nensure that, even in the case of gradient-killing\\nprocessing at a certain stage, the gradient will\\nstill propagate through the skip connections.\\nResidual connections, in particular, allow for the\\nbuilding of deep models with up to several hun-', metadata={'source': 'rag/deep learning.pdf', 'page': 83}),\n",
       " Document(page_content='dred layers, and key models, such as the resid ual\\nnetworks [He et al., 2015] in computer vision\\n(see Â§ 5.2), and the Trans form ers [Vaswani et al.,\\n2017] in natural language processing (see Â§ 5.3),\\nare entirely composed of blocks of layers with\\nresidual connections.\\n84', metadata={'source': 'rag/deep learning.pdf', 'page': 83}),\n",
       " Document(page_content='Â·Â·Â·f(1)f(2)f(3)f(4)f(5)f(6)f(7)f(8)Â·Â·Â·\\nÂ·Â·Â·f(1)f(2)f(3)f(4)f(5)f(6)Â·Â·Â·\\nÂ·Â·Â·f(1)f(2)+f(3)f(4)+Â·Â·Â·\\nFigure 4.10: Skip connections, highlighted in red on this\\nfigure, transport the signal unchanged across multiple\\nlayers. Some architectures (center) that downscale and\\nre-upscale the representation size to operate at multiple\\nscales, have skip connections to feed outputs from the\\nearly parts of the network to later layers operating at\\nthe same scales [Long et al., 2014; Ronneberger et al.,\\n2015]. The residual connections (right) are a special\\ntype of skip connections that sum the original signal\\nto the transformed one, and usually bypass at most a\\nhandful of layers [He et al., 2015].\\n85', metadata={'source': 'rag/deep learning.pdf', 'page': 84}),\n",
       " Document(page_content='Their role can also be to facilitate multi-scale rea-\\nsoning in models that reduce the signal size be-\\nfore re-expanding it, by connecting layers with\\ncompatible sizes, for instance for semanticseg-\\nmentation (see Â§ 6.4). In the case of residual\\nconnections, they may also facilitate learning\\nby simplifying the task to finding a differential\\nimprovement instead of a full update.\\n86', metadata={'source': 'rag/deep learning.pdf', 'page': 85}),\n",
       " Document(page_content='4.8 Attention layers\\nIn many applications, there is a need for an op-\\neration able to combine local information at lo-\\ncations far apart in a tensor. For instance, this\\ncould be distant details for coherent and realistic\\nimagesynthesis, or words at different positions\\nin a paragraph to make a grammatical or seman-\\ntic decision in NaturalLanguage Processing.\\nFully connected layers cannot process large-\\ndimension signals, nor signals of variable size,\\nandconvolutional layers are not able to prop-\\nagate information quickly. Strategies that ag-\\ngregate the results of convolutions, for instance,\\nby averaging them over large spatial areas, suf-\\nfer from mixing multiple signals into a limited\\nnumber of dimensions.\\nAttention layers specifically address this prob-\\nlem by computing an attention score for each\\ncomponent of the resulting tensor to each com-\\nponent of the input tensor, without locality con-\\nstraints, and averaging the features across the', metadata={'source': 'rag/deep learning.pdf', 'page': 86}),\n",
       " Document(page_content='full tensor accordingly [Vaswani et al., 2017].\\nEven though they are substantially more com-\\nplicated than other layers, they have become a\\nstandard element in many recent models. They\\nare, in particular, the key building block of Trans -\\n87', metadata={'source': 'rag/deep learning.pdf', 'page': 86}),\n",
       " Document(page_content='KQ\\nVYA A\\nComputes Aq,1,...,A q,NKV Computes Yq\\nFigure 4.11: The attention operator can be inter-\\npreted as matching every query Qqwith all the\\nkeysK1,...,K NKVto get normalized attention scores\\nAq,1,...,A q,NKV(left, and Equation 4.1), and then av-\\neraging the values V1,...,V NKVwith these scores to\\ncompute the resulting Yq(right, and Equation 4.2).\\nform ers, the dominant architecture for Large\\nLanguage Models. See Â§ 5.3 and Â§ 7.1.\\nAttention operator\\nGiven\\nâ€¢a tensor Qofqueries of size NQÃ—DQK,\\nâ€¢a tensor Kofkeys of size NKVÃ—DQK, and\\nâ€¢a tensor Vofvalues of size NKVÃ—DV,\\ntheattention operator computes a tensor\\nY= att( Q,K,V )\\nof dimension NQÃ—DV. To do so, it first com-\\nputes for every query index qand every key in-\\n88', metadata={'source': 'rag/deep learning.pdf', 'page': 87}),\n",
       " Document(page_content='dexkan attention score Aq,kas the softargmax\\nof the dot products between the query Qqand\\nthe keys:\\nAq,k=exp\\x10\\n1âˆš\\nDQKQqÂ·Kk\\x11\\nP\\nlexp\\x10\\n1âˆš\\nDQKQqÂ·Kl\\x11, (4.1)\\nwhere the scaling factor1âˆš\\nDQKkeeps the range\\nof values roughly unchanged even for large DQK.\\nThen a retrieved value is computed for each\\nquery by averaging the values according to the\\nattention scores (see Figure 4.11):\\nYq=X\\nkAq,kVk. (4.2)\\nSo if a query Qnmatches one key Kmfar more\\nthan all the others, the corresponding attention\\nscore An,mwill be close to one, and the retrieved\\nvalue Ynwill be the value Vmassociated to that\\nkey. But, if it matches several keys equally, then\\nYnwill be the average of the associated values.\\nThis can be implemented as\\natt(Q,K,V ) = softargmax\\x12QKT\\nâˆš\\nDQK\\x13\\n| {z }\\nAV.\\n89', metadata={'source': 'rag/deep learning.pdf', 'page': 88}),\n",
       " Document(page_content='Q K VTÃ—expâŠ™1/Î£kdropoutA\\nMÃ—Y\\nMasked\\nsoftargmax\\nFigure 4.12: The attention operator Y= att( Q,K,V )\\ncomputes first an attention matrix Aas the per-query\\nsoftargmax of QKT, which may be masked by a con-\\nstant matrix Mbefore the normalization. This atten-\\ntion matrix goes through a dropout layer before being\\nmultiplied by Vto get the resulting Y. This operator\\ncan be made causal by taking Mfull of 1s below the\\ndiagonal and zeros above.\\n90', metadata={'source': 'rag/deep learning.pdf', 'page': 89}),\n",
       " Document(page_content='This operator is usually extended in two ways,\\nas depicted in Figure 4.12. First, the attention\\nmatrix can be masked by multiplying it before\\nthe softargmax normalization by a Boolean ma-\\ntrixM. This allows, for instance, to make the\\noperator causal by taking Mfull of 1s below the\\ndiagonal and zero above, preventing Yqfrom de-\\npending on keys and values of indices kgreater\\nthanq. Second, the attention matrix is processed\\nby a dropout layer (see Â§ 4.5) before being multi-\\nplied by V, providing the usual benefits during\\ntraining.\\nSince a dot product is computed for every\\nquery/key pair, the computational cost of the at-\\ntention operator is quadratic with the sequence\\nlength. This happens to be problematic, as some\\nof the applications of these methods require to\\nprocess sequences of tens of thousands, or more\\ntokens. Multiple attempts have been made at\\nreducing this cost, for instance by combining a\\ndense attention to a local window with a long-', metadata={'source': 'rag/deep learning.pdf', 'page': 90}),\n",
       " Document(page_content='range sparse attention [Beltagy et al., 2020], or\\nlinearizing the operator to benefit from the asso-\\nciativity of the matrix product and compute the\\nkey-value product before multiplying with the\\nqueries [Katharopoulos et al., 2020].\\n91', metadata={'source': 'rag/deep learning.pdf', 'page': 90}),\n",
       " Document(page_content='Ã—WQ\\n1Ã—WK\\n1Ã—WV\\n1att\\nÃ—WQ\\n2Ã—WK\\n2Ã—WV\\n2att\\nÃ—WQ\\n3Ã—WK\\n3Ã—WV\\n3att\\nÃ—WQ\\n4Ã—WK\\n4Ã—WV\\n4att\\nÃ—WQ\\nHÃ—WK\\nHÃ—WV\\nHatt(Y1| Â·Â·Â· |YH)\\nXQXKXVÃ—WOY\\nÃ—H\\nFigure 4.13: The Multi-head Attention layer applies\\nfor each of its h= 1,...,H heads a parametrized lin-\\near transformation to individual elements of the input\\nsequences XQ,XK,XVto get sequences Q,K,V that\\nare processed by the attention operator to compute Yh.\\nThese Hsequences are concatenated along features,\\nand individual elements are passed through one last\\nlinear operator to get the final result sequence Y.\\n92', metadata={'source': 'rag/deep learning.pdf', 'page': 91}),\n",
       " Document(page_content='Multi-head Attention Layer\\nThis parameterless attention operator is the key\\nelement in the Multi -Head Attention layer de-\\npicted in Figure 4.13. The structure of this layer\\nis defined by several hyper-parameters: a num-\\nberHof heads, and the shapes of three series of\\nHtrainable weight matrices\\nâ€¢WQof size HÃ—DÃ—DQK,\\nâ€¢WKof size HÃ—DÃ—DQK, and\\nâ€¢WVof size HÃ—DÃ—DV,\\nto compute respectively the queries, the keys,\\nand the values from the input, and a final weight\\nmatrix WOof size HDVÃ—Dto aggregate the\\nper-head results.\\nIt takes as input three sequences\\nâ€¢XQof size NQÃ—D,\\nâ€¢XKof size NKVÃ—D, and\\nâ€¢XVof size NKVÃ—D,\\nfrom which it computes, for h= 1,...,H ,\\nYh= att\\x00\\nXQWQ\\nh,XKWK\\nh,XVWV\\nh\\x01\\n.\\nThese sequences Y1,...,Y Hare concatenated\\nalong the feature dimension and each individual\\nelement of the resulting sequence is multiplied\\n93', metadata={'source': 'rag/deep learning.pdf', 'page': 92}),\n",
       " Document(page_content='byWOto get the final result:\\nY= (Y1| Â·Â·Â· |YH)WO.\\nAs we will see in Â§ 5.3 and in Figure 5.6, this\\nlayer is used to build two model sub-structures:\\nself-attention blocks, in which the three input\\nsequences XQ,XK, and XVare the same, and\\ncross -attention blocks, where XKandXVare\\nthe same.\\nIt is noteworthy that the attention operator,\\nand consequently the multi-head attention layer\\nwhen there is no masking, is invariant to a per-\\nmutation of the keys and values, and equiv ariant\\nto a permutation of the queries, as it would per-\\nmute the resulting tensor similarly.\\n94', metadata={'source': 'rag/deep learning.pdf', 'page': 93}),\n",
       " Document(page_content='4.9 Token embedding\\nIn many situations, we need to convert discrete\\ntokens into vectors. This can be done with an em-\\nbedding layer, which consists of a lookup table\\nthat directly maps integers to vectors.\\nSuch a layer is defined by two hyper-parame-\\nters: the number Nof possible token values,\\nand the dimension Dof the output vectors, and\\none trainable NÃ—Dweight matrix M.\\nGiven as input an integer tensor Xof dimen-\\nsionD1Ã—Â·Â·Â·Ã— DKand values in {0,...,N âˆ’1}\\nsuch a layer returns a real-valued tensor Yof\\ndimension D1Ã—Â·Â·Â·Ã— DKÃ—Dwith\\nâˆ€d1,...,d K,\\nY[d1,...,d K] =M[X[d1,...,d K]].\\n95', metadata={'source': 'rag/deep learning.pdf', 'page': 94}),\n",
       " Document(page_content='4.10 Positional encoding\\nWhile the processing of a fully connected layer\\nis specific to both the positions of the features\\nin the input tensor and to the positions of the\\nresulting activations in the output tensor, con-\\nvolutional layers and Multi -Head Attention lay-\\ners are oblivious to the absolute position in the\\ntensor. This is key to their strong invariance and\\ninductivebias, which is beneficial for dealing\\nwith a stationary signal.\\nHowever, this can be an issue in certain situ-\\nations where proper processing has to access\\nthe absolute positioning. This is the case, for\\ninstance, for image synthesis, where the statis-\\ntics of a scene are not totally stationary, or in\\nnatural language processing, where the relative\\npositions of words strongly modulate the mean-\\ning of a sentence.\\nThe standard way of coping with this problem\\nis to add or concatenate to the feature represen-\\ntation, at every position, a positional encoding,\\nwhich is a feature vector that depends on the po-', metadata={'source': 'rag/deep learning.pdf', 'page': 95}),\n",
       " Document(page_content='sition in the tensor. This positional encoding can\\nbe learned as other layer parameters, or defined\\nanalytically.\\nFor instance, in the original Trans former model,\\n96', metadata={'source': 'rag/deep learning.pdf', 'page': 95}),\n",
       " Document(page_content='for a series of vectors of dimension D, Vaswani\\net al. [2017] add an encoding of the sequence\\nindex as a series of sines and cosines at various\\nfrequencies:\\npos-enc[ t,d] =\\uf8f1\\n\\uf8f2\\n\\uf8f3sin\\x10\\nt\\nTd/D\\x11\\nifdâˆˆ2N\\ncos\\x10\\nt\\nT(dâˆ’1)/D\\x11\\notherwise,\\nwithT= 104.\\n97', metadata={'source': 'rag/deep learning.pdf', 'page': 96}),\n",
       " Document(page_content='Chapter 5\\nArchitectures\\nThe field of deep learning has developed over\\nthe years for each application domain multiple\\ndeep architectures that exhibit good trade-offs\\nwith respect to multiple criteria of interest: e.g.\\nease of training, accuracy of prediction, memory\\nfootprint, computational cost, scalability.\\n98', metadata={'source': 'rag/deep learning.pdf', 'page': 97}),\n",
       " Document(page_content='5.1 Multi-Layer Perceptrons\\nThe simplest deep architecture is the Multi -Layer\\nPerceptron ( MLP), which takes the form of a\\nsuccession of fully connected layers separated\\nbyactivationfunctions. See an example in Figure\\n5.1. For historical reasons, in such a model, the\\nnumber of hiddenlayers refers to the number of\\nlinear layers, excluding the last one.\\nA key theoretical result is the universalapproxi-\\nmation theorem [Cybenko, 1989] which states\\nthat, if the activation function Ïƒis continuous\\nXfully-connrelufully-connrelufully-connY\\n5025102\\nHidden\\nlayers\\nFigure 5.1: This multi-layer perceptron takes as input\\na one-dimensional tensor of size 50, is composed of\\nthree fully connected layers with outputs of dimensions\\nrespectively 25,10, and 2, the two first followed by\\nReLU layers.\\n99', metadata={'source': 'rag/deep learning.pdf', 'page': 98}),\n",
       " Document(page_content='and not polynomial, any continuous function f\\ncan be approximated arbitrarily well uniformly\\non a compact domain, which is bounded and\\ncontains its boundary, by a model of the form\\nl2â—¦Ïƒâ—¦l1where l1andl2are affine. Such a model\\nis aMLP with a single hidden layer, and this\\nresult implies that it can approximate anything\\nof practical value. However, this approximation\\nholds if the dimension of the first linear layerâ€™s\\noutput can be arbitrarily large.\\nIn spite of their simplicity, MLPs remain an im-\\nportant tool when the dimension of the signal\\nto be processed is not too large.\\n100', metadata={'source': 'rag/deep learning.pdf', 'page': 99}),\n",
       " Document(page_content='5.2 Convolutional networks\\nThe standard architecture for processingimages\\nis aconvolutional network, or convnet, that com-\\nbines multiple convolutional layers, either to re-\\nduce the signal size before it can be processed by\\nfully connected layers, or to output a 2D signal\\nalso of large size.\\nLeNet-like\\nThe original LeNet model for image classifica-\\ntion [LeCun et al., 1998] combines a series of 2D\\nconvolutional layers and max pooling layers that\\nplay the role of feature extractor, with a series of\\nfully connected layers which act as a MLP and\\nperform the classification per se (see Figure 5.2).\\nThis architecture was the blueprint for many\\nmodels that share its structure and are simply\\nlarger, such as AlexNet [Krizhevsky et al., 2012]\\nor the VGG family [Simonyan and Zisserman,\\n2014].\\nResidual networks\\nStandard convolutional neural networks that fol-\\nlow the architecture of the LeNet family are not\\neasily extended to deep architectures and suffer', metadata={'source': 'rag/deep learning.pdf', 'page': 100}),\n",
       " Document(page_content='from the vanishing gradient problem. The resid -\\n101', metadata={'source': 'rag/deep learning.pdf', 'page': 100}),\n",
       " Document(page_content='Xconv-2dk=5maxpoolk=3reluconv-2dk=5maxpoolk=2relureshapefully-connrelufully-connË†P(Y)\\n1Ã—28Ã—2832Ã—24Ã—2432Ã—8Ã—864Ã—4Ã—464Ã—2Ã—225620010\\nFeature\\nextractorClassifier\\nFigure 5.2: Example of a small LeNet-like network for\\nclassifying 28Ã—28grayscale images of handwritten\\ndigits [LeCun et al., 1998]. Its first half is convolutional,\\nand alternates convolutional layers per se and max\\npooling layers, reducing the signal dimension from\\n28Ã—28scalars to 256. Its second half processes this\\n256-dimensional feature vector through a one hidden\\nlayer perceptron to compute 10logit scores correspond-\\ning to the ten possible digits.\\n102', metadata={'source': 'rag/deep learning.pdf', 'page': 101}),\n",
       " Document(page_content='Xconv-2dk=1batchnormreluconv-2dk=3p=1batchnormreluconv-2dk=1batchnorm+reluY\\nCÃ—HÃ—WC\\n2Ã—HÃ—WCÃ—HÃ—WCÃ—HÃ—W\\nFigure 5.3: A residual block.\\nualnetworks, or ResNets, proposed by He et al.\\n[2015] explicitly address the issue of the vanish-\\ning gradient with resid ualconnections (see Â§ 4.7),\\nwhich allow hundreds of layers. They have be-\\ncome standard architectures for computer vision\\napplications, and exist in multiple versions de-\\npending on the number of layers. We are going\\nto look in detail at the architecture of the ResNet -\\n50 for classification.\\nAs other ResNets, it is composed of a series of\\n103', metadata={'source': 'rag/deep learning.pdf', 'page': 102}),\n",
       " Document(page_content='Xconv-2dk=1batchnormreluconv-2dk=3s=S p=1batchnormreluconv-2dk=1batchnorm+reluY\\nconv-2dk=1s=Sbatchnorm\\nCÃ—HÃ—WC\\nSÃ—HÃ—WC\\nSÃ—H\\nSÃ—W\\nS4C\\nSÃ—H\\nSÃ—W\\nS4C\\nSÃ—H\\nSÃ—W\\nS\\nFigure 5.4: A downscaling residual block. It admits a\\nhyper-parameter S, the stride of the first convolution\\nlayer, which modulates the reduction of the tensor size.\\nresid ualblocks, each combining several convolu-\\ntional layers,batch norm layers, and ReLU layers,\\nwrapped in a residual connection. Such a block\\nis pictured in Figure 5.3.\\nA key requirement for high performance with\\nreal images is to propagate a signal with a large\\nnumber of channels, to allow for a rich repre-\\nsentation. However, the parameter count of a\\n104', metadata={'source': 'rag/deep learning.pdf', 'page': 103}),\n",
       " Document(page_content='Ã—2Ã—3Ã—5Ã—2\\nXconv-2dk=7s=2p=3batchnormrelumaxpoolk=3s=2p=1dresblock\\nS=1resblockdresblock\\nS=2resblockdresblock\\nS=2resblockdresblock\\nS=2resblockavgpoolk=7reshapefully-connË†P(Y)\\n3Ã—224Ã—22464Ã—112Ã—11264Ã—56Ã—56256Ã—56Ã—56512Ã—28Ã—281024Ã—14Ã—142048Ã—7Ã—72048Ã—1Ã—120481000\\nFigure 5.5: Structure of the ResNet-50 [He et al., 2015].\\n105', metadata={'source': 'rag/deep learning.pdf', 'page': 104}),\n",
       " Document(page_content='convolutional layer, and its computational cost,\\nare quadratic with the number of channels. This\\nresidual block mitigates this problem by first re-\\nducing the number of channels with a 1Ã—1con-\\nvolution, then operating spatially with a 3Ã—3\\nconvolution on this reduced number of chan-\\nnels, and then upscaling the number of channels,\\nagain with a 1Ã—1convolution.\\nThe network reduces the dimensionality of the\\nsignal to finally compute the logits for the clas-\\nsification. This is done thanks to an architec-\\nture composed of several sections, each starting\\nwith a down scalingresid ualblock that halves\\nthe height and width of the signal, and doubles\\nthe number of channels, followed by a series\\nof residual blocks. Such a downscaling resid-\\nual block has a structure similar to a standard\\nresidual block, except that it requires a residual\\nconnection that changes the tensor shape. This\\nis achieved with a 1Ã—1convolution with a stride\\nof two (see Figure 5.4).', metadata={'source': 'rag/deep learning.pdf', 'page': 105}),\n",
       " Document(page_content='The overall structure of the ResNet-50 is pre-\\nsented in Figure 5.5. It starts with a 7Ã—7convo-\\nlutional layer that converts the three-channel in-\\nput image to a 64-channel image of half the size,\\nfollowed by four sections of residual blocks. Sur-\\nprisingly, in the first section, there is no down-\\n106', metadata={'source': 'rag/deep learning.pdf', 'page': 105}),\n",
       " Document(page_content='scaling, only an increase of the number of chan-\\nnels by a factor of 4. The output of the last resid-\\nual block is 2048Ã—7Ã—7, which is converted to a\\nvector of dimension 2048 by an average pooling\\nof kernel size 7Ã—7, and then processed through\\na fully-connected layer to get the final logits,\\nhere for 1000 classes.\\n107', metadata={'source': 'rag/deep learning.pdf', 'page': 106}),\n",
       " Document(page_content='5.3 Attention models\\nAs stated in Â§ 4.8, many applications, particu-\\nlarly from natural language processing, benefit\\ngreatly from models that include attention mech-\\nanisms. The architecture of choice for such tasks,\\nwhich has been instrumental in recent advances\\nin deep learning, is the Trans former proposed\\nby Vaswani et al. [2017].\\nTransformer\\nThe original Transformer, pictured in Figure 5.7,\\nwas designed for sequence-to-sequence transla-\\ntion. It combines an encoder that processes the\\ninput sequence to get a refined representation,\\nand an autoregressive decoder that generates\\neach token of the result sequence, given the en-\\ncoderâ€™s representation of the input sequence and\\nthe output tokens generated so far.\\nAs the residual convolutional networks of Â§ 5.2,\\nboth the encoder and the decoder of the Trans-\\nformer are sequences of compounded blocks\\nbuilt with residual connections.\\nâ€¢Thefeed-forward block, pictured at the top of\\nFigure 5.6 is a one hidden layer MLP, preceded', metadata={'source': 'rag/deep learning.pdf', 'page': 107}),\n",
       " Document(page_content='by a layer normalization. It can update represen-\\ntations at every position separately.\\n108', metadata={'source': 'rag/deep learning.pdf', 'page': 107}),\n",
       " Document(page_content='XQKVlayernormfully-conngelufully-conndropout+Y\\nXQKVlayernormQ K Vmha+Y\\nXQlayernormQ K Vmha+Y\\nXKV\\nFigure 5.6: Feed-forward block (top), self-attention\\nblock (bottom left) and cross -attention block (bottom\\nright). These specific structures proposed by Radford\\net al. [2018] differ slightly from the original architec-\\nture of Vaswani et al. [2017], in particular by having\\nthe layer normalization first in the residual blocks.\\n109', metadata={'source': 'rag/deep learning.pdf', 'page': 108}),\n",
       " Document(page_content='Ã—NÃ—N\\nX1,...,X Tembed+self-attffwZ1,...,Z T\\npos-enc0,Y1,...,Y Sâˆ’1embed+causal\\nself-attQ KVcross-attffwfully-connË†P(Y1),...,Ë†P(YS|Ys<S)\\npos-enc\\nEncoderDecoder\\nTTÃ—DTÃ—DSSÃ—DSÃ—DSÃ—V\\nFigure 5.7: Original encoder-decoder Trans former\\nmodel for sequence-to-sequence translation [Vaswani\\net al., 2017].\\n110', metadata={'source': 'rag/deep learning.pdf', 'page': 109}),\n",
       " Document(page_content='â€¢Theself-attention block, pictured on the bot-\\ntom left of Figure 5.6, is a Multi -Head Attention\\nlayer (see Â§ 4.8), that recombines information\\nglobally, allowing any position to collect infor-\\nmation from any other positions, preceded by\\nalayer normalization. This block can be made\\ncausal by using an adequate mask in the atten-\\ntion layer, as described in Â§ 4.8\\nâ€¢Thecross -attentionblock, pictured on the bot-\\ntom right of Figure 5.6, is similar except that it\\ntakes as input two sequences, one to compute\\nthe queries and one to compute the keys and\\nvalues.\\nThe encoder of the Transformer (see Figure\\n5.7, bottom), recodes the input sequence of dis-\\ncrete tokens X1,...X Twith an embedding layer\\n(see Â§ 4.9), and adds a positional encoding (see\\nÂ§ 4.10), before processing it with several self-\\nattention blocks to generate a refined represen-\\ntation Z1,...,Z T.\\nThe decoder (see Figure 5.7, top), takes as in-\\nput the sequence Y1,...,Y Sâˆ’1of result tokens', metadata={'source': 'rag/deep learning.pdf', 'page': 110}),\n",
       " Document(page_content='produced so far, similarly recodes them through\\nan embedding layer, adds a positional encoding,\\nand processes it through alternating causal self-\\nattention blocks and cross-attention blocks to\\n111', metadata={'source': 'rag/deep learning.pdf', 'page': 110}),\n",
       " Document(page_content='Ã—N\\n0,X1,...,X Tâˆ’1embed+causal\\nself-attffwfully-connË†P(X1),...,Ë†P(XT|Xt<T)\\npos-enc\\nTTÃ—DTÃ—DTÃ—V\\nFigure 5.8: GPT model [Radford et al., 2018].\\nproduce the logits predicting the next tokens.\\nThese cross-attention blocks compute their keys\\nand values from the encoderâ€™s result represen-\\ntation Z1,...,Z T, which allows the resulting se-\\nquence to be a function of the original sequence\\nX1,...,X T.\\nAs we saw in Â§ 3.2 being causal ensures that\\nsuch a model can be trained by minimizing the\\ncross-entropy summed across the full sequence.\\nGenerative Pre-trained Transformer\\nTheGenerativePre-trained Trans former ( GPT)\\n[Radford et al., 2018, 2019], pictured in Figure 5.8\\n112', metadata={'source': 'rag/deep learning.pdf', 'page': 111}),\n",
       " Document(page_content='is a pure autoregressive model that consists of a\\nsuccession of causal self-attention blocks, hence\\na causal version of the original Transformer en-\\ncoder.\\nThis class of models scales extremely well, up\\nto hundreds of billions of trainable parameters\\n[Brown et al., 2020]. We will come back to their\\nuse for text generation in Â§ 7.1.\\nVision Transformer\\nTransformers have been put to use for image\\nclassification with the Vision Trans former ( ViT)\\nmodel [Dosovitskiy et al., 2020] (see Figure 5.9).\\nIt splits the three-channel input image into M\\npatches of resolution PÃ—P, which are then flat-\\ntened to create a sequence of vectors X1,...,X M\\nof shape MÃ—3P2. This sequence is multiplied\\nby a trainable matrix WEof shape 3P2Ã—Dto\\nmap it to an MÃ—Dsequence, to which is con-\\ncatenated one trainable vector E0. The resulting\\n(M+1)Ã—Dsequence E0,...,E Mis then pro-\\ncessed through multiple self-attention blocks.\\nSee Â§ 5.3 and Figure 5.6.\\nThe first element Z0in the resultant sequence,', metadata={'source': 'rag/deep learning.pdf', 'page': 112}),\n",
       " Document(page_content='which corresponds to E0and is not associated\\nwith any part of the image, is finally processed\\n113', metadata={'source': 'rag/deep learning.pdf', 'page': 112}),\n",
       " Document(page_content='Ã—N\\nE0,E1,...,E M+self-attffwZ0,Z1,...,Z Mfully-conngelufully-conngelufully-connË†P(Y)\\nÃ—WE\\nX1,...,X ME0pos-encMLP\\nreadout\\nImage\\nencoderMÃ—3P2(M+1)Ã—D(M+1)Ã—DDC\\nFigure 5.9: Vision Transformer model [Dosovitskiy\\net al., 2020].\\n114', metadata={'source': 'rag/deep learning.pdf', 'page': 113}),\n",
       " Document(page_content='by a two-hidden-layer MLP to get the final C\\nlogits. Such a token, added for a readout of a\\nclass prediction, was introduced by Devlin et al.\\n[2018] in the BERT model and is referred to as a\\nCLS token.\\n115', metadata={'source': 'rag/deep learning.pdf', 'page': 114}),\n",
       " Document(page_content='Part III\\nApplications\\n116', metadata={'source': 'rag/deep learning.pdf', 'page': 115}),\n",
       " Document(page_content='Chapter 6\\nPrediction\\nA first category of applications, such as face\\nrecognition, sentiment analysis, object detection,\\nor speech recognition, requires predicting an un-\\nknown value from an available signal.\\n117', metadata={'source': 'rag/deep learning.pdf', 'page': 116}),\n",
       " Document(page_content='6.1 Image denoising\\nA direct application of deep models to image\\nprocessing is to recover from degradation by\\nutilizing the redundancy in the statistical struc-\\nture of images. The petals of a sunflower in a\\ngrayscale picture can be colored with high confi-\\ndence, and the texture of a geometric shape such\\nas a table on a low-light, grainy picture can be\\ncorrected by averaging it over a large area likely\\nto be uniform.\\nAdenoisingautoencoder is a model that takes\\na degraded signal ËœXas input and computes an\\nestimate of the original signal X. For images, it\\nis a convolutional network that may integrate\\nskip-connections, in particular to combine repre-\\nsentations at the same resolution obtained early\\nand late in the model, as well as attention layers\\nto facilitate taking into account elements that\\nare far away from each other.\\nSuch a model is trained by collecting a large num-\\nber of clean samples paired with their degraded\\ninputs. The latter can be captured in degraded', metadata={'source': 'rag/deep learning.pdf', 'page': 117}),\n",
       " Document(page_content='conditions, such as low-light or inadequate fo-\\ncus, or generated algorithmically, for instance,\\nby converting the clean sample to grayscale, re-\\nducing its size, or aggressively compressing it\\n118', metadata={'source': 'rag/deep learning.pdf', 'page': 117}),\n",
       " Document(page_content='with a lossy compression method.\\nThe standard training procedure for denoising\\nautoencoders uses the MSE loss summed across\\nall pixels, in which case the model aims at com-\\nputing the best average clean picture, given the\\ndegraded one, that is E[X|ËœX]. This quantity\\nmay be problematic when Xis not completely\\ndetermined by ËœX, in which case some parts\\nof the generated signal may be an unrealistic,\\nblurry average.\\n119', metadata={'source': 'rag/deep learning.pdf', 'page': 118}),\n",
       " Document(page_content='6.2 Image classification\\nImage classification is the simplest strategy for\\nextracting semantics from an image and consists\\nof predicting a class from a finite, predefined\\nnumber of classes, given an input image.\\nThe standard models for this task are convolu-\\ntional networks, such as ResNets (see Â§ 5.2), and\\nattention-based models such as ViT (see Â§ 5.3).\\nThese models generate a vector of logits with as\\nmany dimensions as there are classes.\\nThe training procedure simply minimizes the\\ncross-entropy loss (see Â§ 3.1). Usually, perfor-\\nmance can be improved with data augmenta-\\ntion, which consists of modifying the training\\nsamples with hand-designed random transfor-\\nmations that do not change the semantic content\\nof the image, such as cropping, scaling, mirror-\\ning, or color changes.\\n120', metadata={'source': 'rag/deep learning.pdf', 'page': 119}),\n",
       " Document(page_content='6.3 Object detection\\nA more complex task for image understanding is\\nobjectdetection, in which the objective is, given\\nan input image, to predict the classes and posi-\\ntions of objects of interest.\\nAn object position is formalized as the four co-\\nordinates (x1,y1,x2,y2)of a rectangular bound-\\ning box, and the ground truth associated with\\neach training image is a list of such bounding\\nboxes, each labeled with the class of the object\\ncontained therein.\\nThe standard approach to solve this task, for in-\\nstance, by the SingleShot Detector (SSD) [Liu\\net al., 2015]), is to use a convolutional neural\\nnetwork that produces a sequence of image\\nrepresentations Zsof size DsÃ—HsÃ—Ws, s=\\n1,...,S , with decreasing spatial resolution HsÃ—\\nWsdown to 1Ã—1fors=S(see Figure 6.1). Each\\nof these tensors covers the input image in full, so\\ntheh,w indices correspond to a partitioning of\\nthe image lattice into regular squares that gets\\ncoarser when sincreases.\\nAs seen in Â§ 4.2, and illustrated in Figure 4.4,', metadata={'source': 'rag/deep learning.pdf', 'page': 120}),\n",
       " Document(page_content='due to the succession of convolutional layers, a\\nfeature vector (Zs[0,h,w],...,Z s[Dsâˆ’1,h,w])\\nis a descriptor of an area of the image, called its\\n121', metadata={'source': 'rag/deep learning.pdf', 'page': 120}),\n",
       " Document(page_content='X\\nZ1\\nZ2ZSâˆ’1ZS\\n......\\nFigure 6.1: A convolutional object detector processes the\\ninput image to generate a sequence of representations\\nof decreasing resolutions. It computes for every h,w, at\\nevery scale s, a pre-defined number of bounding boxes\\nwhose centers are in the image area corresponding to\\nthat cell, and whose sizes are such that they fit in its\\nreceptive field. Each prediction takes the form of the\\nestimates (Ë†x1,Ë†x2,Ë†y1,Ë†y2), represented by the red boxes\\nabove, and a vector of C+1logits for the Cclasses of\\ninterest, and an additional â€œno objectâ€ class.\\n122', metadata={'source': 'rag/deep learning.pdf', 'page': 121}),\n",
       " Document(page_content='Figure 6.2: Examples of object detection with the Single-\\nShot Detector [Liu et al., 2015].\\n123', metadata={'source': 'rag/deep learning.pdf', 'page': 122}),\n",
       " Document(page_content='receptivefield, that is larger than this square but\\ncentered on it. This results in a non-ambiguous\\nmatching of any bounding box (x1,x2,y1,y2)to\\nas,h,w , determined respectively by max( x2âˆ’\\nx1,y2âˆ’y1),y1+y2\\n2, andx1+x2\\n2.\\nDetection is achieved by adding Sconvolutional\\nlayers, each processing a Zsand computing, for\\nevery tensor indices h,w, the coordinates of a\\nbounding box and the associated logits. If there\\nareCobject classes, there are C+1logits, the\\nadditional one standing for â€œno object.â€ Hence,\\neach additional convolution layer has 4+C+1\\noutput channels. The SSD algorithm in particu-\\nlar generates several bounding boxes per s,h,w ,\\neach dedicated to a hard-coded range of aspect\\nratios.\\nTraining sets for object detection are costly to\\ncreate, since the labeling with bounding boxes\\nrequires a slow human intervention. To mitigate\\nthis issue, the standard approach is to fine-tune\\na convolutional model that has been pre-trained\\non a large classification dataset such as VGG-16', metadata={'source': 'rag/deep learning.pdf', 'page': 123}),\n",
       " Document(page_content='for the original SSD, and to replace its final fully-\\nconnected layers with additional convolutional\\nones. Surprisingly, models trained for classifica-\\ntion only learn feature representations that can\\nbe repurposed for object detection, even though\\n124', metadata={'source': 'rag/deep learning.pdf', 'page': 123}),\n",
       " Document(page_content='that task involves the regression of geometric\\nquantities.\\nDuring training, every ground-truth bounding\\nbox is associated with its s,h,w , and induces a\\nloss term composed of a cross-entropy loss for\\nthe logits, and a regression loss such as MSE\\nfor the bounding box coordinates. Every other\\ns,h,w free of bounding-box match induces a\\ncross-entropy only penalty to predict the class\\nâ€œno objectâ€.\\n125', metadata={'source': 'rag/deep learning.pdf', 'page': 124}),\n",
       " Document(page_content='6.4 Semantic segmentation\\nThe finest-grain prediction task for image under-\\nstanding is semanticsegmentation, which con-\\nsists of predicting, for each pixel, the class of the\\nobject to which it belongs. This can be achieved\\nwith a standard convolutional neural network\\nthat outputs a convolutional map with as many\\nchannels as classes, carrying the estimated logits\\nfor every pixel.\\nWhile a standard residual network, for instance,\\ncan generate a dense output of the same reso-\\nlution as its input, as for object detection, this\\ntask requires operating at multiple scales. This\\nis necessary so that any object, or sufficiently\\ninformative sub-part, regardless of its size, is\\ncaptured somewhere in the model by the feature\\nrepresentation at a single tensor position. Hence,\\nstandard architectures for this task downscale\\nthe image with a series of convolutional layers\\nto increase the receptive field of the activations,\\nand re-upscale it with a series of trans posed con-', metadata={'source': 'rag/deep learning.pdf', 'page': 125}),\n",
       " Document(page_content='volutional layers, or other upscaling methods\\nsuch as bilinear interpolation, to make the pre-\\ndiction at high resolution.\\nHowever, a strict downscaling-upscaling archi-\\ntecture does not allow for operating at a fine\\n126', metadata={'source': 'rag/deep learning.pdf', 'page': 125}),\n",
       " Document(page_content='Figure 6.3: Semantic segmentation results with the\\nPyramid Scene Parsing Network [Zhao et al., 2016].\\ngrain when making the final prediction, since all\\nthe signal has been transmitted through a low-\\nresolution representation at some point. Models\\nthat apply such downscaling-upscaling serially\\nmitigate these issues with skip connections from\\nlayers at a certain resolution, before downscal-\\ning, to layers at the same resolution, after upscal-\\ning [Long et al., 2014; Ronneberger et al., 2015].\\nModels that do it in parallel, after a convolutional\\n127', metadata={'source': 'rag/deep learning.pdf', 'page': 126}),\n",
       " Document(page_content='backbone, concatenate the resulting multi-scale\\nrepresentation after upscaling, before making\\nthe final per-pixel prediction [Zhao et al., 2016].\\nTraining is achieved with a standard cross-\\nentropy summed over all the pixels. As for ob-\\nject detection, training can start from a network\\npre-trained on a large-scale image classification\\ndataset to compensate for the limited availability\\nof segmentation ground truth.\\n128', metadata={'source': 'rag/deep learning.pdf', 'page': 127}),\n",
       " Document(page_content='6.5 Speech recognition\\nSpeech recog nition consists of converting a\\nsound sample into a sequence of words. There\\nhave been plenty of approaches to this problem\\nhistorically, but a conceptually simple and recent\\none proposed by Radford et al. [2022] consists of\\ncasting it as a sequence-to-sequence translation\\nand then solving it with a standard attention-\\nbased Trans former, as described in Â§ 5.3.\\nTheir model first converts the sound signal into a\\nspectrogram, which is a one-dimensional series\\nTÃ—D, that encodes at every time step a vector\\nof energies in Dfrequency bands. The associ-\\nated text is encoded with the BPE tokenizer (see\\nÂ§ 3.2).\\nThe spectrogram is processed through a few 1D\\nconvolutional layers, and the resulting repre-\\nsentation is fed into the encoder of the Trans-\\nformer. The decoder directly generates a discrete\\nsequence of tokens, that correspond to one of\\nthe possible tasks considered during training.\\nMultiple objectives are considered: transcription', metadata={'source': 'rag/deep learning.pdf', 'page': 128}),\n",
       " Document(page_content='of English or non-English text, translation from\\nany language to English, or detection of non-\\nspeech sequences, such as background music or\\nambient noise.\\n129', metadata={'source': 'rag/deep learning.pdf', 'page': 128}),\n",
       " Document(page_content='This approach allows leveraging extremely large\\ndatasets that combine multiple types of sound\\nsources with diverse ground truths.\\nIt is noteworthy that even though the ultimate\\ngoal of this approach is to produce a transla-\\ntion as deterministic as possible given the input\\nsignal, it is formally the sampling of a text dis-\\ntribution conditioned on a sound sample, hence\\na synthesis process. The decoder is, in fact, ex-\\ntremely similar to the generative model of Â§ 7.1.\\n130', metadata={'source': 'rag/deep learning.pdf', 'page': 129}),\n",
       " Document(page_content='6.6 Text-image representations\\nA powerful approach to image understanding\\nconsists of learning consistent image and text\\nrepresentations, such that an image, or a textual\\ndescription of it, would be mapped to the same\\nfeature vector.\\nThe Contrastive Language -ImagePre-train ing\\n(CLIP) proposed by Radford et al. [2021] com-\\nbines an image encoder f, which is a ViT, and\\na text encoder g, which is a GPT. See Â§ 5.3 for\\nboth.\\nTo repurpose a GPT as a text encoder, instead of a\\nstandard autoregressive model, they add an â€œend\\nof sentenceâ€ token to the input sequence, and use\\nthe representation of this token in the last layer\\nas the embedding. Its dimension is between 512\\nand1024 , depending on the configuration.\\nThose two models are trained from scratch using\\na dataset of 400 million image-text pairs (ik,tk)\\ncollected from the internet. The training proce-\\ndure follows the standard mini-batch stochastic\\ngradient descent approach but relies on a con-', metadata={'source': 'rag/deep learning.pdf', 'page': 130}),\n",
       " Document(page_content='trastive loss. The embeddings are computed for\\nevery image and every text of the Npairs in the\\nmini-batch, and a cosine similarity measure is\\ncomputed not only between text and image em-\\n131', metadata={'source': 'rag/deep learning.pdf', 'page': 130}),\n",
       " Document(page_content='beddings from each pair, but also across pairs, re-\\nsulting in an NÃ—Nmatrix of similarity scores:\\nlm,n=f(im)Â·g(tn), m= 1,...,N,n = 1,...,N.\\nThe model is trained with cross-entropy so that,\\nâˆ€nthe values l1,n,...,l N,ninterpreted as logit\\nscores predict n, and similarly for ln,1,...,l n,N.\\nThis means that âˆ€n,m, s.t.nÌ¸=mthe similarity\\nln,nis unambiguously greater than both ln,mand\\nlm,n.\\nWhen it has been trained, this model can be used\\nto do zero-shot prediction, that is, classifying a\\nsignal in the absence of training examples by\\ndefining a series of candidate classes with text\\ndescriptions, and computing the similarity of the\\nembedding of an image with the embedding of\\neach of those descriptions (see Figure 6.4).\\nAdditionally, since the textual descriptions are\\noften detailed, such a model has to capture a\\nricher representation of images and pick up cues\\nbeyond what is necessary for instance for classifi-\\ncation. This translates to excellent performance', metadata={'source': 'rag/deep learning.pdf', 'page': 131}),\n",
       " Document(page_content='on challenging datasets such as ImageNet Adver-\\nsarial [Hendrycks et al., 2019] which was specifi-\\ncally designed to degrade or erase cues on which\\nstandard predictors rely.\\n132', metadata={'source': 'rag/deep learning.pdf', 'page': 131}),\n",
       " Document(page_content='Figure 6.4: The CLIP text-image embedding [Radford\\net al., 2021] allows for zero-shot prediction by predicting\\nwhich class description embedding is the most consis-\\ntent with the image embedding.\\n133', metadata={'source': 'rag/deep learning.pdf', 'page': 132}),\n",
       " Document(page_content='6.7 Reinforcement learning\\nMany problems, such as strategy games or\\nrobotic control, can be formalized with a discrete-\\ntime state process Stand reward process Rtthat\\ncan be modulated by choosing actions At. If\\nStisMarko vian, meaning that it carries alone\\nas much information about the future as all the\\npast states until that instant, such an object is a\\nMarko vian Decision Process ( MDP).\\nGiven an MDP, the objective is classically to find\\napolicyÏ€such that At=Ï€(St)maximizes the\\nexpectation of the return, which is an accumu-\\nlated discounted reward:\\nE\\uf8ee\\n\\uf8f0X\\ntâ‰¥0Î³tRt\\uf8f9\\n\\uf8fb,\\nfor a discount factor 0< Î³ < 1.\\nThis is the standard setup of Reinforce ment\\nLearn ing (RL), and it can be worked out by intro-\\nducing the optimal state-action value function\\nQ(s,a)which is the expected return if we exe-\\ncute action ain state s, and then follow the opti-\\nmalpolicy. It provides a means to compute the\\noptimal policy as Ï€(s) = argmaxaQ(s,a), and,\\nthanks to the Markovian assumption, it verifies\\n134', metadata={'source': 'rag/deep learning.pdf', 'page': 133}),\n",
       " Document(page_content='theBellman equa tion:\\nQ(s,a) = (6.1)\\nE\\x14\\nRt+Î³max\\naâ€²Q(St+1,aâ€²)\\x0c\\x0c\\x0c\\x0cSt=s,At=a\\x15\\n,\\nfrom which we can design a procedure to train\\na parametric model Q(Â·,Â·;w).\\nTo apply this framework to play classical Atari\\nvideo games, Mnih et al. [2015] use for Stthe con-\\ncatenation of the frame at time tand the three\\nthat precede, so that the Markovian assumption\\nis reasonable, and use for Qa model dubbed the\\nDeep Q-Network ( DQN), composed of two con-\\nvolutional layers and one fully connected layer\\nwith one output value per action, following the\\nclassical structure of a LeNet (see Â§ 5.2).\\nTraining is achieved by alternatively playing and\\nrecording episodes, and building mini-batches of\\ntuples (sn,an,rn,sâ€²n)âˆ¼(St,At,Rt,St+1)taken\\nacross stored episodes and time steps, and mini-\\nmizing\\nâ„’(w) =1\\nNNX\\nn=1(Q(sn,an;w)âˆ’yn)2(6.2)\\nwith one iteration of SGD, where yn=rnif this\\ntuple is the end of the episode, and yn=rn+\\nÎ³max aQ(sâ€²n,a; Â¯w)otherwise.\\n135', metadata={'source': 'rag/deep learning.pdf', 'page': 134}),\n",
       " Document(page_content='Frame numberValue\\nFigure 6.5: This graph shows the evolution of the state\\nvalue V(St) = max aQ(St,a)during a game of Break-\\nout. The spikes at time points (1) and (2) correspond to\\nclearing a brick, at time point (3) it is about to break\\nthrough to the top line, and at (4) it does, which ensures\\na high future reward [Mnih et al., 2015].\\nHere Â¯wis a constant copy of w, i.e. the gradient\\ndoes not propagate through it to w. This is nec-\\nessary since the target value in Equation 6.1 is\\nthe expectation of yn, while it is ynitself which\\nis used in Equation 6.2. Fixing winynresults in\\na better approximation of the desirable gradient.\\nA key issue is the policy used to collect episodes.\\nMnih et al. [2015] simply use the Ïµ-greedy strat-\\negy, which consists of taking an action com-\\npletely at random with probability Ïµ, and the\\noptimal action argmaxaQ(s,a)otherwise. In-\\njecting a bit of randomness is necessary to favor\\n136', metadata={'source': 'rag/deep learning.pdf', 'page': 135}),\n",
       " Document(page_content='exploration.\\nTraining is done with ten million frames corre-\\nsponding to a bit less than eight days of game-\\nplay. The trained network computes accurate\\nestimates of the state values (see Figure 6.5), and\\nreaches human performance on a majority of the\\n49 games used in the experimental validation.\\n137', metadata={'source': 'rag/deep learning.pdf', 'page': 136}),\n",
       " Document(page_content='Chapter 7\\nSynthesis\\nA second category of applications distinct from\\nprediction is synthesis. It consists of fitting a\\ndensity model to training samples and providing\\nmeans to sample from this model.\\n138', metadata={'source': 'rag/deep learning.pdf', 'page': 137}),\n",
       " Document(page_content='7.1 Text generation\\nThe standard approach to text synthesis is to\\nuse an attention-based, autoregressivemodel. A\\nvery successful model proposed by Radford et al.\\n[2018], is the GPT which we described in Â§ 5.3.\\nThis architecture has been used for very large\\nmodels, such as OpenAIâ€™s 175-billion-parameter\\nGPT-3 [Brown et al., 2020]. It is composed of 96\\nself-attention blocks, each with 96 heads, and\\nprocesses tokens of dimension 12,288, with a\\nhidden dimension of 49,512 in the MLPs of the\\nattention blocks.\\nWhen such a model is trained on a very large\\ndataset, it results in a Large Language Model\\n(LLM), which exhibits extremely powerful prop-\\nerties. Besides the syntactic and grammatical\\nstructure of the language, it has to integrate\\nvery diverse knowledge, e.g. to predict the word\\nfollowing â€œThe capital of Japan isâ€, â€œif water is\\nheated to 100 Celsius degrees it turns intoâ€, or\\nâ€œbecause her puppy was sick, Jane wasâ€.\\nThis results in particular in the ability to solve', metadata={'source': 'rag/deep learning.pdf', 'page': 138}),\n",
       " Document(page_content='few-shot prediction, where only a handful of\\ntraining examples are available, as illustrated\\nin Figure 7.1. More surprisingly, when given a\\ncarefully crafted prompt, it can exhibit abilities\\n139', metadata={'source': 'rag/deep learning.pdf', 'page': 138}),\n",
       " Document(page_content='I: I love apples, O: positive, I: music is my passion, O:\\npositive, I: my job is boring, O: negative, I: frozen pizzas\\nare awesome, O: positive,\\nI: I love apples, O: positive, I: music is my passion, O:\\npositive, I: my job is boring, O: negative, I: frozen pizzas\\ntaste like cardboard, O: negative,\\nI: water boils at 100 degrees, O: physics, I: the square\\nroot of two is irrational, O: mathematics, I: the set of\\nprime numbers is infinite, O: mathematics, I: gravity is\\nproportional to the mass, O: physics,\\nI: water boils at 100 degrees, O: physics, I: the square\\nroot of two is irrational, O: mathematics, I: the set of\\nprime numbers is infinite, O: mathematics, I: squares\\nare rectangles, O: mathematics,\\nFigure 7.1: Examples of few-shot prediction with a 120\\nmillion parameter GPT model from Hugging Face. In\\neach example, the beginning of the sentence was given\\nas a prompt, and the model generated the part in bold.\\nfor question answering, problem solving, and', metadata={'source': 'rag/deep learning.pdf', 'page': 139}),\n",
       " Document(page_content='chain -of-thought that appear eerily close to high-\\nlevel reasoning [Chowdhery et al., 2022; Bubeck\\net al., 2023].\\nDue to these remarkable capabilities, these mod-\\nels are sometimes called foun dation models\\n[Bommasani et al., 2021].\\nHowever, even though it integrates a very large\\nbody of knowledge, such a model may be inad-\\n140', metadata={'source': 'rag/deep learning.pdf', 'page': 139}),\n",
       " Document(page_content='equate for practical applications, in particular\\nwhen interacting with human users. In many\\nsituations, one needs responses that follow the\\nstatistics of a helpful dialog with an assistant.\\nThis differs from the statistics of available large\\ntraining sets, which combine novels, encyclope-\\ndias, forum messages, and blog posts.\\nThis discrepancy is addressed by fine-tuning\\nsuch a language model (see Â§ 3.6). The current\\ndominant strategy is Reinforce ment Learn ing\\nfrom Human Feed back ( RLHF) [Ouyang et al.,\\n2022], which consists of creating small labeled\\ntraining sets by asking users to either write\\nresponses or provide ratings of generated re-\\nsponses. The former can be used as-is to fine-\\ntune the language model, and the latter can be\\nused to train a reward network that predicts\\nthe rating and use it as a target to fine-tune the\\nlanguage model with a standard Reinforce ment\\nLearn ing approach.\\n141', metadata={'source': 'rag/deep learning.pdf', 'page': 140}),\n",
       " Document(page_content='7.2 Image generation\\nMultiple deep methods have been developed to\\nmodel and sample from a high-dimensional den-\\nsity. A powerful approach for imagesynthesis\\nrelies on inverting a diffusion process. Such a\\ngenerative model is referred to, somehow incor-\\nrectly, as a diffusion model.\\nThe principle consists of defining analytically\\na process that gradually degrades any sample,\\nand consequently transforms the complex and\\nunknown density of the data into a simple and\\nwell-known density such as a normal, and train-\\ning a deep architecture to invert this degradation\\nprocess [Ho et al., 2020].\\nGiven a fixed T, the diffusion process defines a\\nprobability distribution over series of T+1im-\\nages as follows: sample x0uniformly from the\\ndataset, and then sequentially sample xt+1âˆ¼\\np(xt+1|xt),t= 0,...,Tâˆ’1, where the condi-\\ntional distribution pis defined analytically and\\nsuch that it gradually erases the structure that\\nwas in x0. The setup should degrade the signal', metadata={'source': 'rag/deep learning.pdf', 'page': 141}),\n",
       " Document(page_content='so much that the distribution p(xT)has a known\\nanalytical form which can be sampled.\\nFor instance, Ho et al. [2020] normalize the data\\nto have a mean of 0and a variance of 1, and their\\n142', metadata={'source': 'rag/deep learning.pdf', 'page': 141}),\n",
       " Document(page_content='xT\\nx0\\nFigure 7.2: Image synthesis with denoising diffusion\\n[Ho et al., 2020]. Each sample starts as a white noise\\nxT(top), and is gradually de-noised by sampling iter-\\natively xtâˆ’1|xtâˆ¼ð’©(xt+f(xt,t;w),Ïƒt).\\n143', metadata={'source': 'rag/deep learning.pdf', 'page': 142}),\n",
       " Document(page_content='diffusion process consists of adding a bit of white\\nnoise and re-normalizing the variance to 1. This\\nprocess exponentially reduces the importance of\\nx0, andxtâ€™s density can rapidly be approximated\\nwith a normal.\\nThe denoiser fis a deep architecture that\\nshould model and allow sampling from\\nf(xtâˆ’1,xt,t;w)â‰ƒp(xtâˆ’1|xt). It can be shown,\\nthanks to a variational bound, that if this\\none-step reverse process is accurate enough,\\nsampling xTâˆ¼p(xT)and denoising Tsteps\\nwithfresults in x0that follows p(x0).\\nTraining fcan be achieved by generating a large\\nnumber of sequences x(n)\\n0,...,x(n)\\nT, picking a tn\\nin each, and maximizing\\nX\\nnlogf\\x10\\nx(n)\\ntnâˆ’1,x(n)\\ntn,tn;w\\x11\\n.\\nGiven their diffusion process, Ho et al. [2020]\\nhave a denoising of the form:\\nxtâˆ’1|xtâˆ¼ð’©(xt+f(xt,t;w);Ïƒt), (7.1)\\nwhere Ïƒtis defined analytically.\\nIn practice, such a model initially hallucinates\\nstructures by pure luck in the random noise, and\\n144', metadata={'source': 'rag/deep learning.pdf', 'page': 143}),\n",
       " Document(page_content='then gradually builds more elements that emerge\\nfrom the noise by reinforcing the most likely\\ncontinuation of the image obtained thus far.\\nThis approach can be extended to text-\\nconditioned synthesis, to generate images\\nthat match a description. For instance, Nichol\\net al. [2021] add to the mean of the denoising\\ndistribution of Equation 7.1 a bias that goes in\\nthe direction of increasing the CLIP matching\\nscore (see Â§ 6.6) between the produced image\\nand the conditioning text description.\\n145', metadata={'source': 'rag/deep learning.pdf', 'page': 144}),\n",
       " Document(page_content='Chapter 8\\nThe Compute Schism\\nThe scale of deep architectures is critical to their\\nperformance and, as we saw in Â§ 3.7, Large Lan-\\nguage Models in particular may require amounts\\nof memory and computation that greatly exceed\\nthose of consumer hardware.\\nWhile training such a model from scratch re-\\nquires resources available only to large corpora-\\ntions or public bodies, techniques have been de-\\nveloped to allow inference and adaptation to spe-\\ncific tasks under strong resource constraints. Al-\\nlowing to run models locally instead of through\\na provider may be highly desirable for cost or\\nconfidentiality reasons.\\n146', metadata={'source': 'rag/deep learning.pdf', 'page': 145}),\n",
       " Document(page_content='8.1 Prompt Engineering\\nThe simplest strategy to specialize or improve a\\nLarge Language Model with a limited computa-\\ntional budget is to use prompt engineering, that\\nis, to carefully craft the beginning of the text se-\\nquence to bias the autoregressive process [Sahoo\\net al., 2024]. This approach moves a part of the\\ninformation traditionally encoded in the modelâ€™s\\nparameters to the input.\\nWe saw in Â§ 7.1 a simple example of few-shot\\nprediction, to use an LLM for a text classification\\ntask without fine-tuning. A long and sophisti-\\ncated prompt allows generalizing this strategy\\nto complex tasks.\\nSince the promptâ€™s role is to leverage the â€œgoodâ€\\nbiases that were present in the training set, it\\nbenefits from surprising strategies such as stat-\\ning that the response is generated by a skilled\\nprofessional [Xu et al., 2023].\\nThe context size of a language model, that is,\\nthe number of tokens it can operate on, directly\\nmodulates the quantity of information that can', metadata={'source': 'rag/deep learning.pdf', 'page': 146}),\n",
       " Document(page_content='be provided in the prompt. This is mostly con-\\nstrained by the computational cost of standard\\nattention models, which is quadratic with the\\ncontext size (see Â§ 4.8).\\n147', metadata={'source': 'rag/deep learning.pdf', 'page': 146}),\n",
       " Document(page_content='Q: Gina has 105 beans, she gives 23 beans to Bob, and\\nprepares a soup with 53 beans. How many beans are\\nleft? A: There are 29 beans left.\\nQ: I prepare 53 pancakes, eat 5 of them and give 7 to\\nGina. I then prepare 26 more. How many pancakes are\\nleft? A: 27 pancakes are left.\\nQ: Gina has 105 beans, she gives 23 beans to Bob, and\\nprepares a soup with 53 beans. How many beans are\\nleft? A: Letâ€™s proceed step by step: Gina has 105 beans,\\nshe gives 23 beans to Bob (82 left), and prepares a soup\\nwith 53 beans (29 left). So there are 29 beans left.\\nQ: I prepare 53 pancakes, eat 5 of them and give 7 to\\nGina. I then prepare 26 more. How many pancakes are\\nleft? A: Letâ€™s proceed step by step: 53 pancakes, eat 5\\nof them (48 left), give 7 to Gina (41 left), prepare\\n26 more (67 left). So there are 67 pancakes left.\\nFigure 8.1: Example of a chain-of-thought to improve\\nthe response of the Llama-3-8B base model. In the\\ntwo examples, the beginning of the text in normal font', metadata={'source': 'rag/deep learning.pdf', 'page': 147}),\n",
       " Document(page_content='is the prompt, and the generated part is indicated in\\nbold. The generation without chain-of-thought (top)\\nleads to an incorrect answer, while the generation with\\nit (bottom) generates a correct answer, by explicitly\\nproducing multiple simple arithmetic operations.\\n148', metadata={'source': 'rag/deep learning.pdf', 'page': 147}),\n",
       " Document(page_content='Chain of Thought\\nA remarkable type of prompting aims at making\\nthe model generate intermediate steps before\\ngenerating the response itself.\\nSuch a chain -of-thought is composed of succes-\\nsive steps that are simpler, hence have been bet-\\nter modeled during training, and are predicted\\nmore deterministically [Wei et al., 2022; Kojima\\net al., 2022]. See Figure 8.1 for an example.\\nRetrieval-Augmented Generation\\nPrompt engineering can also be put to use to\\nconnect a language model to an external knowl-\\nedge base. It plays the role of a smart interface\\nthat allows the end user to formulate questions\\nin natural language and get back a response that\\ncombines information that is not encoded in the\\nmodelâ€™s parameters [Lewis et al., 2020].\\nFor such Retrieval -Augmented Generation\\n(RAG), an embedding model is used to retrieve\\ndocuments whose embedding is correlated to\\nthat of the userâ€™s query. Then, a prompt is con-\\nstructed by joining these retrieved documents', metadata={'source': 'rag/deep learning.pdf', 'page': 148}),\n",
       " Document(page_content='with instructions to combine them, and the\\ngenerative model produces the response to the\\nuser.\\n149', metadata={'source': 'rag/deep learning.pdf', 'page': 148}),\n",
       " Document(page_content='8.2 Quantization\\nAlthough training or generating multiple\\nstreams can benefit from high-end parallel\\ncomputing devices, deployment of a Large\\nLanguage Model for individual use requires\\ngenerally single-stream inference, which is\\nbounded by memory size and speed far more\\nthan by computation.\\nAs stated in Â§ 2.1, parameters, activations, and\\ngradients are usually encoded with 32or16bits.\\nThe precision it provides is necessary for train-\\ning, to allow gradual changes to accumulate.\\nHowever, since activations are the sums of many\\nterms, quan tization during inference is mitigated\\nby an averaging effect. This is even more true\\nwith large architectures, and models quantized\\ndown to 6or4bits per parameter exhibit remark-\\nable performance. Additionally to reducing the\\nmemory footprint, quantization also improves\\ninference speed significantly.\\nThis has motivated the development of soft-\\nware to quantize existing models with Post-\\nTrain ingQuan tization, and run them in single-', metadata={'source': 'rag/deep learning.pdf', 'page': 149}),\n",
       " Document(page_content='stream inference on consumer hardware, such\\nas llama.cpp [Llama.cpp, 2023]. This framework\\nimplements multiple formats, that apply specific\\n150', metadata={'source': 'rag/deep learning.pdf', 'page': 149}),\n",
       " Document(page_content='2 4 8 16 325.566.5\\nSize (Gigabytes)Perplexity\\nFigure 8.2: Perplexity of quantized versions of the lan-\\nguage models Llama-7B (blue) and 13B (red) [Touvron\\net al., 2023] on the wikitext corpus, as a function of\\nthe parametersâ€™ memory footprint. The crosses are the\\noriginal FP16 models and the dots correspond to differ-\\nent levels of quantization with llama.cpp [Llama.cpp,\\n2023].\\nquantization levels for the different weight matri-\\nces of a language model. For instance the quan-\\ntization may use more bits for the WVweights\\nof the attention blocks, and for the weights of\\nthe feed-forward blocks.\\nAn example of llama.cppâ€™s quantization is Q4_1.\\n151', metadata={'source': 'rag/deep learning.pdf', 'page': 150}),\n",
       " Document(page_content='It quantizes individually sub-blocks of 32entries\\nof the original weight matrix by storing for each\\na scaling factor dand a bias min the original\\nFP16 encoding, and encoding each entry xwith4\\nbits as a value qâˆˆ {0,...,24âˆ’1}. The resulting\\nde-quantized value being Ëœx=dq+m.\\nSuch a block was encoded originally as 32values\\nin FP16, hence 64bytes, while the quantized\\nversion needs 4bytes for qandmand32Â·4bits\\n=16bytes for the entries, hence a total of 20\\nbytes.\\nSuch an aggressive quantization surprisingly de-\\ngrades only marginally the performance of the\\nmodels, as illustrated on Figure 8.2.\\nAn alternative to Post-Training Quantization is\\nQuan tization-Aware Train ing that applies quan-\\ntization during the forward pass but keeps high-\\nprecision encoding of parameters and gradients,\\nand propagates the gradients during the back-\\nward pass as if there was no quantization [Ma\\net al., 2024].\\n152', metadata={'source': 'rag/deep learning.pdf', 'page': 151}),\n",
       " Document(page_content='8.3 Adapters\\nAs we saw in Â§ 3.6, fine-tuning is a key strat-\\negy to reuse pre-trained models. Since it aims\\nat making only minor changes to an existing\\nmodel, techniques have been developed that add\\ncomponents with few parameters, referred to\\nasadapters, to the pre-trained architecture, and\\nfreeze all the original parameters [Houlsby et al.,\\n2019].\\nThe current dominant method is the Low-Rank\\nAdap tation ( LoRA), which adds low-rank cor-\\nrections to some of the modelâ€™s weight matrices\\n[Hu et al., 2021].\\nFormally, given a linear operation of the form\\nXWT, where Xis aNÃ—Dtensor of activations\\nfor a batch of Nsamples, and Wis aCÃ—D\\nweight matrix, the LoRA adapter replaces this\\noperation with X(W+BA)T, where AandB\\nare two trainable matrices of size RÃ—Dand\\nCÃ—Rrespectively, with Râ‰ªmin(C,D), and\\nthe matrix Wis removed from the trainable pa-\\nrameters. The matrix Ais initialized with ran-\\ndom Gaussian values, and Bis set to zero, so\\nthat the fine-tuning starts with a model that com-', metadata={'source': 'rag/deep learning.pdf', 'page': 152}),\n",
       " Document(page_content='putes an output identical to that of the original\\none.\\n153', metadata={'source': 'rag/deep learning.pdf', 'page': 152}),\n",
       " Document(page_content='The total number of parameters to optimize with\\nthis approach is generally a few percent of the\\nnumber of parameters in the original model.\\nThe standard procedure to fine-tune a trans -\\nformer with such adapters is to change only the\\nweight matrices in the attention blocks, and to\\nkeep the MLP of the feed-forward blocks un-\\nchanged. The same strategy has been used suc-\\ncessfully to tune diffusion denoising models by\\nfine-tuning the attention blocks responsible for\\nthe text-based conditioning.\\nSince fine-tuning with LoRA adapters drastically\\nreduces the number of trainable parameters, it\\nreduces the memory footprint required by op-\\ntimizers such as Adam, which generally store\\ntwo running average per parameter to optimize.\\nAlso, it reduces slightly the computation during\\ntheback ward pass.\\nFor commercial applications that require a large\\nnumber of fine-tuned models, the ABpairs can\\nbe stored separately from the original model,\\nwhich has to be stored only once. And finally,', metadata={'source': 'rag/deep learning.pdf', 'page': 153}),\n",
       " Document(page_content='contrary to other type of adapters, the modifica-\\ntions can be integrated into the original architec-\\nture, simply by adding ABtoW, resulting in an\\narchitecture and parameter count for inference\\n154', metadata={'source': 'rag/deep learning.pdf', 'page': 153}),\n",
       " Document(page_content='strictly identical to that of the base model.\\nWe saw that quantization degrade modelsâ€™ ac-\\ncuracy only marginally. However, gradient de-\\nscent requires high precision in both the gra-\\ndient and the trained parameters, to allow the\\naccumulation of small changes. The QLoRA ap-\\nproach combines a quantized base model and un-\\nquantized Low-Rank Adap tation to reduce the\\nmemory requirement even more [Dettmers et al.,\\n2023].\\n155', metadata={'source': 'rag/deep learning.pdf', 'page': 154}),\n",
       " Document(page_content='8.4 Model merging\\nAn alternative to the fine-tuning and prompting\\nmethods seen in the previous sections consists\\nof combining multiple models with diverse ca-\\npabilities into a single one, without additional\\ntraining.\\nModel merg ing relies on the compatibility be-\\ntween multiple fine-tuned versions of a base\\nmodel.\\nIlharco et al. [2022] showed that models obtained\\nby fine-tuning a CLIP base model on several im-\\nage classification data-sets can be combined in\\nthe parameter space, where they exhibit Task\\nArith metic properties.\\nFormally, let Î¸be the parameter vector of a pre-\\ntrained model, and for t= 1,...,T , letÎ¸tand\\nÏ„t=Î¸tâˆ’Î¸be respectively the parameters af-\\nter fine-tuning on task tand the corresponding\\nresidual. Experiments show that the model with\\nparameters Î¸+Ï„1+Â·Â·Â·+Ï„Texhibits multi-task\\ncapabilities. Similarly, subtracting a Ï„tdegrades\\nthe performance on the corresponding task.\\nMethods have been developed to reduce the in-\\nterference between the different residuals and', metadata={'source': 'rag/deep learning.pdf', 'page': 155}),\n",
       " Document(page_content='improve the performance when the number of\\n156', metadata={'source': 'rag/deep learning.pdf', 'page': 155}),\n",
       " Document(page_content='tasks increases [Yadav et al., 2023; Yu et al., 2023].\\nAn alternative to merging models in parame-\\nter space is to recombine their layers. Akiba\\net al. [2024] combine merging the parameters\\nand re-combining layers, and rely on a stochas-\\ntic optimization to deal with the combinatorial\\nexplosion. Experiments with three fine-tuned\\nversions of Mistral-7B [Jiang et al., 2023] show\\nthat combining these two merging strategies out-\\nperforms both of them.\\n157', metadata={'source': 'rag/deep learning.pdf', 'page': 156}),\n",
       " Document(page_content='The Missing Bits\\nFor the sake of concision, this volume skips many\\nimportant topics, in particular:\\nRecurrent Neural Networks\\nBefore attention models showed greater perfor-\\nmance, Recurrent NeuralNetworks ( RNN) were\\nthe standard approach for dealing with temporal\\nsequences such as text or sound samples. These\\narchitectures possess an internal hiddenstate\\nthat gets updated each time a component of the\\nsequence is processed. Their main components\\nare layers such as LSTM [Hochreiter and Schmid-\\nhuber, 1997] or GRU [Cho et al., 2014].\\nTraining a recurrent architecture amounts to\\nunfolding it in time, which results in a long\\ncomposition of operators. This has historically\\nprompted the design of key techniques now used\\nfor deep architectures such as rectifiers and gat-\\ning, a form of skip connections which are modu-\\n158', metadata={'source': 'rag/deep learning.pdf', 'page': 157}),\n",
       " Document(page_content='lated dynamically.\\nOne of the key drawbacks of traditional recur-\\nrent architectures is that the structure of the\\ncomputation xt+1=f(xt)imposes to process\\nthe input sequence serially, which takes a time\\nproportional to T. In contrast, transformers, for\\ninstance, can take advantage of parallel compu-\\ntation, resulting in a constant time if enough\\ncomputing units are available.\\nThis is addressed by architectures such as QRNN\\n[Bradbury et al., 2016], S4 [Gu et al., 2021], or\\nMamba [Gu and Dao, 2023], whose recurrent op-\\nerations are affine so that the ftthemselves, and\\nconsequently the xt=ft(x0), can be computed\\nin parallel, resulting in a constant time if fdoes\\nnot depend on tandlogTotherwise, again if\\nenough parallel computing units are available.\\nAutoencoder\\nAnautoencoder is a model that maps an input\\nsignal, possibly of high dimension, to a low-\\ndimension latent representation, and then maps\\nit back to the original signal, ensuring that infor-', metadata={'source': 'rag/deep learning.pdf', 'page': 158}),\n",
       " Document(page_content='mation has been preserved. We saw it in Â§ 6.1\\nfor denoising, but it can also be used to auto-\\nmatically discover a meaningful low-dimension\\n159', metadata={'source': 'rag/deep learning.pdf', 'page': 158}),\n",
       " Document(page_content='parameterization of the data manifold.\\nTheVariational Autoencoder ( VAE) proposed by\\nKingma and Welling [2013] is a generative model\\nwith a similar structure. It imposes, through\\nthe loss, a pre-defined distribution on the latent\\nrepresentation. This allows, after training, the\\ngeneration of new samples by sampling the la-\\ntent representation according to this imposed\\ndistribution and then mapping back through the\\ndecoder.\\nGenerative Adversarial Networks\\nAnother approach to density modeling is the\\nGenerativeAdversarialNetworks ( GAN) intro-\\nduced by Goodfellow et al. [2014]. This method\\ncombines a generator, which takes a random in-\\nput following a fixed distribution as input and\\nproduces a structured signal such as an image,\\nand a discrim inator, which takes a sample as\\ninput and predicts whether it comes from the\\ntraining set or if it was generated by the genera-\\ntor.\\nTraining optimizes the discriminator to mini-\\nmize a standard cross-entropy loss, and the gen-', metadata={'source': 'rag/deep learning.pdf', 'page': 159}),\n",
       " Document(page_content='erator to maximize the discriminatorâ€™s loss. It\\ncan be shown that, at equilibrium, the gener-\\n160', metadata={'source': 'rag/deep learning.pdf', 'page': 159}),\n",
       " Document(page_content='ator produces samples indistinguishable from\\nreal data. In practice, when the gradient flows\\nthrough the discriminator to the generator, it\\ninforms the latter about the cues that the dis-\\ncriminator uses that need to be addressed.\\nGraph Neural Networks\\nMany applications require processing signals\\nwhich are not organized regularly on a grid. For\\ninstance, proteins, 3D meshes, geographic loca-\\ntions, or social interactions are more naturally\\nstructured as graphs. Standard convolutional\\nnetworks or even attention models are poorly\\nadapted to process such data, and the tool of\\nchoice for such a task is Graph NeuralNetworks\\n(GNN) [Scarselli et al., 2009].\\nThese models are composed of layers that com-\\npute activations at each vertex by combining\\nlinearly the activations located at its immediate\\nneighboring vertices. This operation is very sim-\\nilar to a standard convolution, except that the\\ndata structure does not reflect any geometrical\\ninformation associated with the feature vectors', metadata={'source': 'rag/deep learning.pdf', 'page': 160}),\n",
       " Document(page_content='they carry.\\n161', metadata={'source': 'rag/deep learning.pdf', 'page': 160}),\n",
       " Document(page_content='Self-supervised training\\nAs stated in Â§ 7.1, even though they are trained\\nonly to predict the next word, Large Language\\nModels trained on large unlabeled datasets such\\nasGPT (see Â§ 5.3) are able to solve various tasks,\\nsuch as identifying the grammatical role of a\\nword, answering questions, or even translating\\nfrom one language to another [Radford et al.,\\n2019].\\nSuch models constitute one category of a larger\\nclass of methods that fall under the name of self-\\nsupervised learn ing, and try to take advantage\\nof unlabeled datasets [Balestriero et al., 2023].\\nThe key principle of these methods is to define a\\ntask that does not require labels but necessitates\\nfeature representations which are useful for the\\nreal task of interest, for which a small labeled\\ndataset exists. In computer vision, for instance,\\nimage features can be optimized so that they are\\ninvariant to data transformations that do not\\nchange the semantic content of the image, while', metadata={'source': 'rag/deep learning.pdf', 'page': 161}),\n",
       " Document(page_content='being statistically uncorrelated [Zbontar et al.,\\n2021].\\nIn both NLP and computer vision, a powerful\\ngeneric strategy is to train a model to recover\\nparts of the signal that have been masked [Devlin\\n162', metadata={'source': 'rag/deep learning.pdf', 'page': 161}),\n",
       " Document(page_content='et al., 2018; Zhou et al., 2021].\\n163', metadata={'source': 'rag/deep learning.pdf', 'page': 162}),\n",
       " Document(page_content='Bibliography\\nT. Akiba, M. Shing, Y. Tang, et al. Evolution-\\nary Optimization of Model Merging Recipes.\\nCoRR , abs/2403.13187, 2024. [pdf]. 157\\nJ. L. Ba, J. R. Kiros, and G. E. Hinton. Layer\\nNormalization. CoRR , abs/1607.06450, 2016.\\n[pdf]. 83\\nR. Balestriero, M. Ibrahim, V. Sobal, et al. A\\nCookbook of Self-Supervised Learning. CoRR ,\\nabs/2304.12210, 2023. [pdf]. 162\\nA. Baydin, B. Pearlmutter, A. Radul, and\\nJ. Siskind. Automatic differentiation in\\nmachine learning: a survey. CoRR ,\\nabs/1502.05767, 2015. [pdf]. 42\\nM. Belkin, D. Hsu, S. Ma, and S. Mandal. Rec-\\nonciling modern machine learning and the\\nbias-variance trade-off. CoRR , abs/1812.11118,\\n2018. [pdf]. 50\\n164', metadata={'source': 'rag/deep learning.pdf', 'page': 163}),\n",
       " Document(page_content='I. Beltagy, M. Peters, and A. Cohan. Long-\\nformer: The Long-Document Transformer.\\nCoRR , abs/2004.05150, 2020. [pdf]. 91\\nR. Bommasani, D. Hudson, E. Adeli, et al. On\\nthe Opportunities and Risks of Foundation\\nModels. CoRR , abs/2108.07258, 2021. [pdf].\\n140\\nJ. Bradbury, S. Merity, C. Xiong, and R. Socher.\\nQuasi-Recurrent Neural Networks. CoRR ,\\nabs/1611.01576, 2016. [pdf]. 159\\nT. Brown, B. Mann, N. Ryder, et al. Lan-\\nguage Models are Few-Shot Learners. CoRR ,\\nabs/2005.14165, 2020. [pdf]. 54, 113, 139\\nS. Bubeck, V. Chandrasekaran, R. Eldan, et al.\\nSparks of Artificial General Intelligence:\\nEarly experiments with GPT-4. CoRR ,\\nabs/2303.12712, 2023. [pdf]. 140\\nT. Chen, B. Xu, C. Zhang, and C. Guestrin. Train-\\ning Deep Nets with Sublinear Memory Cost.\\nCoRR , abs/1604.06174, 2016. [pdf]. 43\\nK. Cho, B. van Merrienboer, Ã‡. GÃ¼lÃ§ehre,\\net al. Learning Phrase Representations using\\nRNN Encoder-Decoder for Statistical Machine\\nTranslation. CoRR , abs/1406.1078, 2014. [pdf].\\n158\\n165', metadata={'source': 'rag/deep learning.pdf', 'page': 164}),\n",
       " Document(page_content='A. Chowdhery, S. Narang, J. Devlin, et al. PaLM:\\nScaling Language Modeling with Pathways.\\nCoRR , abs/2204.02311, 2022. [pdf]. 9, 54, 140\\nG. Cybenko. Approximation by superpositions\\nof a sigmoidal function. Mathematics of Con-\\ntrol, Signals, and Systems , 2(4):303â€“314, De-\\ncember 1989. [pdf]. 99\\nJ. Deng, W. Dong, R. Socher, et al. ImageNet:\\nA Large-Scale Hierarchical Image Database.\\nInConference on Computer Vision and Pattern\\nRecognition (CVPR) , 2009. [pdf]. 51\\nT. Dettmers, A. Pagnoni, A. Holtzman, and\\nL. Zettlemoyer. QLoRA: Efficient Finetuning\\nof Quantized LLMs. CoRR , abs/2305.14314,\\n2023. [pdf]. 155\\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova.\\nBERT: Pre-training of Deep Bidirectional\\nTransformers for Language Understanding.\\nCoRR , abs/1810.04805, 2018. [pdf]. 54, 115,\\n162\\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, et al.\\nAn Image is Worth 16x16 Words: Transform-\\ners for Image Recognition at Scale. CoRR ,\\nabs/2010.11929, 2020. [pdf]. 113, 114\\n166', metadata={'source': 'rag/deep learning.pdf', 'page': 165}),\n",
       " Document(page_content='K. Fukushima. Neocognitron: A self-organizing\\nneural network model for a mechanism of\\npattern recognition unaffected by shift in po-\\nsition. Biological Cybernetics , 36(4):193â€“202,\\nApril 1980. [pdf]. 2\\nY. Gal and Z. Ghahramani. Dropout as\\na Bayesian Approximation: Representing\\nModel Uncertainty in Deep Learning. CoRR ,\\nabs/1506.02142, 2015. [pdf]. 78\\nX. Glorot and Y. Bengio. Understanding the dif-\\nficulty of training deep feedforward neural\\nnetworks. In International Conference on Arti-\\nficial Intelligence and Statistics (AISTATS) , 2010.\\n[pdf]. 44, 62\\nX. Glorot, A. Bordes, and Y. Bengio. Deep Sparse\\nRectifier Neural Networks. In International\\nConference on Artificial Intelligence and Statis-\\ntics (AISTATS) , 2011. [pdf]. 71\\nA. Gomez, M. Ren, R. Urtasun, and R. Grosse.\\nThe Reversible Residual Network: Backprop-\\nagation Without Storing Activations. CoRR ,\\nabs/1707.04585, 2017. [pdf]. 43\\nI. J. Goodfellow, J. Pouget-Abadie, M. Mirza,\\net al. Generative Adversarial Networks. CoRR ,', metadata={'source': 'rag/deep learning.pdf', 'page': 166}),\n",
       " Document(page_content='abs/1406.2661, 2014. [pdf]. 160\\n167', metadata={'source': 'rag/deep learning.pdf', 'page': 166}),\n",
       " Document(page_content='A. Gu and T. Dao. Mamba: Linear-Time Se-\\nquence Modeling with Selective State Spaces.\\nCoRR , abs/2312.00752, 2023. [pdf]. 159\\nA. Gu, K. Goel, and C. RÃ©. Efficiently Modeling\\nLong Sequences with Structured State Spaces.\\nCoRR , abs/2111.00396, 2021. [pdf]. 159\\nK. He, X. Zhang, S. Ren, and J. Sun. Deep Resid-\\nual Learning for Image Recognition. CoRR ,\\nabs/1512.03385, 2015. [pdf]. 52, 84, 85, 103,\\n105\\nD. Hendrycks and K. Gimpel. Gaussian Error\\nLinear Units (GELUs). CoRR , abs/1606.08415,\\n2016. [pdf]. 73\\nD. Hendrycks, K. Zhao, S. Basart, et al. Natural\\nAdversarial Examples. CoRR , abs/1907.07174,\\n2019. [pdf]. 132\\nJ. Ho, A. Jain, and P. Abbeel. Denoising Diffusion\\nProbabilistic Models. CoRR , abs/2006.11239,\\n2020. [pdf]. 142, 143, 144\\nS. Hochreiter and J. Schmidhuber. Long Short-\\nTerm Memory. Neural Computation , 9(8):1735â€“\\n1780, 1997. [pdf]. 158\\nN. Houlsby, A. Giurgiu, S. Jastrzebski, et al.\\nParameter-Efficient Transfer Learning for\\nNLP. CoRR , abs/1902.00751, 2019. [pdf]. 153\\n168', metadata={'source': 'rag/deep learning.pdf', 'page': 167}),\n",
       " Document(page_content='E. Hu, Y. Shen, P. Wallis, et al. LoRA: Low-Rank\\nAdaptation of Large Language Models. CoRR ,\\nabs/2106.09685, 2021. [pdf]. 153\\nG. Ilharco, M. Ribeiro, M. Wortsman, et al. Edit-\\ning Models with Task Arithmetic. CoRR ,\\nabs/2212.04089, 2022. [pdf]. 156\\nS. Ioffe and C. Szegedy. Batch Normalization: Ac-\\ncelerating Deep Network Training by Reduc-\\ning Internal Covariate Shift. In International\\nConference on Machine Learning (ICML) , 2015.\\n[pdf]. 80\\nA. Jiang, A. Sablayrolles, A. Mensch, et al. Mistral\\n7B.CoRR , abs/2310.06825, 2023. [pdf]. 157\\nJ. Kaplan, S. McCandlish, T. Henighan, et al. Scal-\\ning Laws for Neural Language Models. CoRR ,\\nabs/2001.08361, 2020. [pdf]. 52, 53\\nA. Katharopoulos, A. Vyas, N. Pappas, and\\nF. Fleuret. Transformers are RNNs: Fast Au-\\ntoregressive Transformers with Linear Atten-\\ntion. In Proceedings of the International Confer-\\nence on Machine Learning (ICML) , pages 5294â€“\\n5303, 2020. [pdf]. 91\\nD. Kingma and J. Ba. Adam: A Method for', metadata={'source': 'rag/deep learning.pdf', 'page': 168}),\n",
       " Document(page_content='Stochastic Optimization. CoRR , abs/1412.6980,\\n2014. [pdf]. 39\\n169', metadata={'source': 'rag/deep learning.pdf', 'page': 168}),\n",
       " Document(page_content='D. P. Kingma and M. Welling. Auto-Encoding\\nVariational Bayes. CoRR , abs/1312.6114, 2013.\\n[pdf]. 160\\nT. Kojima, S. Gu, M. Reid, et al. Large Lan-\\nguage Models are Zero-Shot Reasoners. CoRR ,\\nabs/2205.11916, 2022. [pdf]. 149\\nA. Krizhevsky, I. Sutskever, and G. Hinton. Ima-\\ngeNet Classification with Deep Convolutional\\nNeural Networks. In Neural Information Pro-\\ncessing Systems (NIPS) , 2012. [pdf]. 8, 101\\nY. LeCun, B. Boser, J. S. Denker, et al. Back-\\npropagation applied to handwritten zip code\\nrecognition. Neural Computation , 1(4):541â€“\\n551, 1989. [pdf]. 8\\nY. LeCun, L. Bottou, Y. Bengio, and P. Haffner.\\nGradient-based learning applied to document\\nrecognition. Proceedings of the IEEE , 86(11):\\n2278â€“2324, 1998. [pdf]. 101, 102\\nP. Lewis, E. Perez, A. Piktus, et al. Retrieval-\\nAugmented Generation for Knowledge-\\nIntensive NLP Tasks. CoRR , abs/2005.11401,\\n2020. [pdf]. 149\\nW. Liu, D. Anguelov, D. Erhan, et al. SSD: Single\\nShot MultiBox Detector. CoRR , abs/1512.02325,\\n2015. [pdf]. 121, 123', metadata={'source': 'rag/deep learning.pdf', 'page': 169}),\n",
       " Document(page_content='170', metadata={'source': 'rag/deep learning.pdf', 'page': 169}),\n",
       " Document(page_content='Llama.cpp. Llama.cpp git repository, June 2023.\\n[web]. 150, 151\\nJ. Long, E. Shelhamer, and T. Darrell. Fully Con-\\nvolutional Networks for Semantic Segmenta-\\ntion. CoRR , abs/1411.4038, 2014. [pdf]. 84, 85,\\n127\\nS. Ma, H. Wang, L. Ma, et al. The Era of 1-bit\\nLLMs: All Large Language Models are in 1.58\\nBits. CoRR , abs/2402.17764, 2024. [pdf]. 152\\nA. L. Maas, A. Y. Hannun, and A. Y. Ng. Rec-\\ntifier nonlinearities improve neural network\\nacoustic models. In proceedings of the ICML\\nWorkshop on Deep Learning for Audio, Speech\\nand Language Processing , 2013. [pdf]. 72\\nV. Mnih, K. Kavukcuoglu, D. Silver, et al. Human-\\nlevel control through deep reinforcement\\nlearning. Nature , 518(7540):529â€“533, February\\n2015. [pdf]. 135, 136\\nA. Nichol, P. Dhariwal, A. Ramesh, et al. GLIDE:\\nTowards Photorealistic Image Generation and\\nEditing with Text-Guided Diffusion Models.\\nCoRR , abs/2112.10741, 2021. [pdf]. 145\\nL. Ouyang, J. Wu, X. Jiang, et al. Training lan-\\nguage models to follow instructions with hu-\\n171', metadata={'source': 'rag/deep learning.pdf', 'page': 170}),\n",
       " Document(page_content='man feedback. CoRR , abs/2203.02155, 2022.\\n[pdf]. 141\\nR. Pascanu, T. Mikolov, and Y. Bengio. On the dif-\\nficulty of training recurrent neural networks.\\nInInternational Conference on Machine Learn-\\ning (ICML) , 2013. [pdf]. 44\\nA. Radford, J. Kim, C. Hallacy, et al. Learn-\\ning Transferable Visual Models From Natural\\nLanguage Supervision. CoRR , abs/2103.00020,\\n2021. [pdf]. 131, 133\\nA. Radford, J. Kim, T. Xu, et al. Robust Speech\\nRecognition via Large-Scale Weak Supervi-\\nsion. CoRR , abs/2212.04356, 2022. [pdf]. 129\\nA. Radford, K. Narasimhan, T. Salimans, and\\nI. Sutskever. Improving Language Understand-\\ning by Generative Pre-Training, 2018. [pdf].\\n109, 112, 139\\nA. Radford, J. Wu, R. Child, et al. Language\\nModels are Unsupervised Multitask Learners,\\n2019. [pdf]. 112, 162\\nO. Ronneberger, P. Fischer, and T. Brox. U-Net:\\nConvolutional Networks for Biomedical Im-\\nage Segmentation. In Medical Image Comput-\\ning and Computer-Assisted Intervention , 2015.\\n[pdf]. 84, 85, 127\\n172', metadata={'source': 'rag/deep learning.pdf', 'page': 171}),\n",
       " Document(page_content='P. Sahoo, A. Singh, S. Saha, et al. A Systematic\\nSurvey of Prompt Engineering in Large Lan-\\nguage Models: Techniques and Applications.\\nCoRR , abs/2402.07927, 2024. [pdf]. 147\\nF. Scarselli, M. Gori, A. C. Tsoi, et al. The Graph\\nNeural Network Model. IEEE Transactions\\non Neural Networks (TNN) , 20(1):61â€“80, 2009.\\n[pdf]. 161\\nR. Sennrich, B. Haddow, and A. Birch. Neural\\nMachine Translation of Rare Words with Sub-\\nword Units. CoRR , abs/1508.07909, 2015. [pdf].\\n34\\nJ. Sevilla, L. Heim, A. Ho, et al. Compute Trends\\nAcross Three Eras of Machine Learning. CoRR ,\\nabs/2202.05924, 2022. [pdf]. 8, 52, 54\\nJ. Sevilla, P. Villalobos, J. F. CerÃ³n, et al. Param-\\neter, Compute and Data Trends in Machine\\nLearning, May 2023. [web]. 55\\nK. Simonyan and A. Zisserman. Very Deep Con-\\nvolutional Networks for Large-Scale Image\\nRecognition. CoRR , abs/1409.1556, 2014. [pdf].\\n101\\nN. Srivastava, G. Hinton, A. Krizhevsky, et al.\\nDropout: A Simple Way to Prevent Neural\\n173', metadata={'source': 'rag/deep learning.pdf', 'page': 172}),\n",
       " Document(page_content='Networks from Overfitting. Journal of Ma-\\nchine Learning Research (JMLR) , 15:1929â€“1958,\\n2014. [pdf]. 77\\nM. Telgarsky. Benefits of depth in neural net-\\nworks. CoRR , abs/1602.04485, 2016. [pdf]. 47\\nH. Touvron, T. Lavril, G. Izacard, et al. LLaMA:\\nOpen and Efficient Foundation Language Mod-\\nels.CoRR , abs/2302.13971, 2023. [pdf]. 151\\nA. Vaswani, N. Shazeer, N. Parmar, et al. Atten-\\ntion Is All You Need. CoRR , abs/1706.03762,\\n2017. [pdf]. 84, 87, 97, 108, 109, 110\\nJ. Wei, X. Wang, D. Schuurmans, et al. Chain of\\nThought Prompting Elicits Reasoning in Large\\nLanguage Models. CoRR , abs/2201.11903, 2022.\\n[pdf]. 149\\nB. Xu, A. Yang, J. Lin, et al. ExpertPrompting: In-\\nstructing Large Language Models to be Distin-\\nguished Experts. CoRR , abs/2305.14688, 2023.\\n[pdf]. 147\\nP. Yadav, D. Tam, L. Choshen, et al. TIES-\\nMerging: Resolving Interference When Merg-\\ning Models. CoRR , abs/2306.01708, 2023. [pdf].\\n157\\nL. Yu, B. Yu, H. Yu, et al. Language Models\\nare Super Mario: Absorbing Abilities from\\n174', metadata={'source': 'rag/deep learning.pdf', 'page': 173}),\n",
       " Document(page_content='Homologous Models as a Free Lunch. CoRR ,\\nabs/2311.03099, 2023. [pdf]. 157\\nJ. Zbontar, L. Jing, I. Misra, et al. Barlow Twins:\\nSelf-Supervised Learning via Redundancy Re-\\nduction. CoRR , abs/2103.03230, 2021. [pdf].\\n162\\nM. D. Zeiler and R. Fergus. Visualizing and Un-\\nderstanding Convolutional Networks. In Eu-\\nropean Conference on Computer Vision (ECCV) ,\\n2014. [pdf]. 69\\nH. Zhao, J. Shi, X. Qi, et al. Pyramid Scene\\nParsing Network. CoRR , abs/1612.01105, 2016.\\n[pdf]. 127, 128\\nJ. Zhou, C. Wei, H. Wang, et al. iBOT: Im-\\nage BERT Pre-Training with Online Tokenizer.\\nCoRR , abs/2111.07832, 2021. [pdf]. 163\\n175', metadata={'source': 'rag/deep learning.pdf', 'page': 174}),\n",
       " Document(page_content='Index\\n1D convolution, 66\\n2D convolution, 66\\nactivation, 23, 41\\nfunction, 71, 99\\nmap, 69\\nAdam, 39, 154\\nadapter, 153\\naffine operation, 61\\nartificial neural network, 8, 11\\nattention operator, 88\\nautoencoder, 159\\ndenoising, 118\\nAutograd, 42\\nautoregressive model, seemodel, autoregressive\\naverage pooling, 76\\nbackpropagation, 42\\nbackward pass, 42, 154\\nbasis function regression, 14\\nbatch, 21, 38\\nbatch normalization, 80, 104\\n176', metadata={'source': 'rag/deep learning.pdf', 'page': 175}),\n",
       " Document(page_content='Bellman equation, 135\\nbias vector, 61, 67\\nBPE, seeByte Pair Encoding\\nByte Pair Encoding, 34, 129\\ncache memory, 21\\ncapacity, 16\\ncausal, 32, 90, 111\\nmodel, seemodel, causal\\nchain rule (derivative), 40\\nchain rule (probability), 30\\nchain-of-thought, 140, 149\\nchannel, 23\\ncheckpointing, 43\\nclassification, 18, 26, 101, 120\\nCLIP, seeContrastive Language-Image\\nPre-training\\nCLS token, 115\\ncomputational cost, 43, 91\\ncontext size, 147\\nContrastive Language-Image Pre-training, 131,\\n156\\ncontrastive loss, 27, 131\\nconvnet, seeconvolutional network\\nconvolution, 66\\nconvolutional layer, seelayer, convolutional\\nconvolutional network, 101\\ncross-attention block, 94, 109, 111\\ncross-entropy, 27, 31, 45\\n177', metadata={'source': 'rag/deep learning.pdf', 'page': 176}),\n",
       " Document(page_content='data augmentation, 120\\ndeep learning, 8, 11\\nDeep Q-Network, 135\\ndenoising autoencoder, seeautoencoder,\\ndenoising\\ndensity modeling, 18\\ndepth, 41\\ndiffusion model, 142\\ndilation, 67, 74\\ndiscriminator, 160\\ndownscaling residual block, 106\\ndownstream task, 50\\nDQN, seeDeep Q-Network\\ndropout, 77, 91\\nembedding layer, seelayer, embedding\\nepoch, 48\\nequivariance, 67, 94\\nfeed-forward block, 108, 109\\nfew-shot prediction, 139\\nfilter, 66\\nfine-tune, 124\\nfine-tuning, 51, 141\\nflops, 22\\nforward pass, 41\\nfoundation model, 140\\nFP32, 22\\nframework, 23\\n178', metadata={'source': 'rag/deep learning.pdf', 'page': 177}),\n",
       " Document(page_content='GAN, seeGenerative Adversarial Networks\\nGELU, 73\\nGenerative Adversarial Networks, 160\\nGenerative Pre-trained Transformer, 112, 131,\\n139, 162\\ngenerator, 160\\nGNN, seeGraph Neural Network\\nGPT, seeGenerative Pre-trained Transformer\\nGPU, seeGraphical Processing Unit\\ngradient descent, 35, 37, 40, 45\\ngradient norm clipping, 44\\ngradient step, 35\\nGraph Neural Network, 161\\nGraphical Processing Unit, 8, 20\\nground truth, 18\\nhidden layer, seelayer, hidden\\nhidden state, 158\\nhyper parameter, seeparameter, hyper\\nhyperbolic tangent, 72\\nimage processing, 101\\nimage synthesis, 87, 142\\ninductive bias, 17, 49, 66, 67, 96\\ninvariance, 76, 94, 96, 162\\nkernel size, 66, 74\\nkey, 88\\nLarge Language Model, 51, 56, 88, 139, 146, 162\\n179', metadata={'source': 'rag/deep learning.pdf', 'page': 178}),\n",
       " Document(page_content='layer, 41, 59\\nattention, 87\\nconvolutional, 66, 74, 87, 96, 101, 104, 121,\\n126, 129\\nembedding, 95, 111\\nfully connected, 61, 87, 96, 99, 101\\nhidden, 99\\nlinear, 61\\nMulti-Head Attention, 93, 96, 111\\nnormalizing, 80\\nreversible, 43\\nlayer normalization, 83, 108, 111\\nLeaky ReLU, 72\\nlearning rate, 35, 50\\nlearning rate schedule, 50\\nLeNet, 101, 102\\nlinear layer, seelayer, linear\\nLLM, seeLarge Language Model\\nlocal minimum, 35\\nlogit, 26, 31\\nLoRA, seeLow-Rank Adaptation\\nloss, 12\\nLow-Rank Adaptation, 153, 155\\nmachine learning, 11, 17, 18\\nMarkovian Decision Process, 134\\nMarkovian property, 134\\nmax pooling, 74, 101\\nMDP, seeMarkovian, Decision Process\\n180', metadata={'source': 'rag/deep learning.pdf', 'page': 179}),\n",
       " Document(page_content='mean squared error, 14, 26\\nmemory requirement, 43\\nmemory speed, 21\\nmetric learning, 27\\nMLP, seemulti-layer perceptron, 154\\nmodel, 12\\nautoregressive, 30, 31, 139\\ncausal, 33, 91, 111, 112\\nparametric, 12\\npre-trained, 51, 124, 128\\nmodel merging, 156\\nmulti-layer perceptron, 45, 99â€“101, 108\\nNatural Language Processing, 87\\nNLP, seeNatural Language Processing\\nnon-linearity, 71\\nnormalizing layer, seelayer, normalizing\\nobject detection, 121\\noverfitting, 17, 48\\npadding, 67, 74\\nparameter, 12\\nhyper, 13, 35, 48, 66, 67, 74, 93, 95\\nparametric model, seemodel, parametric\\npeak performance, 22\\nPerplexity, 151\\nperplexity, 31\\npolicy, 134\\noptimal, 134\\n181', metadata={'source': 'rag/deep learning.pdf', 'page': 180}),\n",
       " Document(page_content='pooling, 74\\npositional encoding, 96, 111\\nPost-Training Quantization, 150\\nposterior probability, 26\\npre-trained model, seemodel, pre-trained\\nprompt, 139, 140\\nengineering, 147\\nquantization, 150\\nQuantization-Aware Training, 152\\nquery, 88\\nRAG, seeRetrieval-Augmented Generation\\nrandom initialization, 62\\nreceptive field, 68, 69, 124\\nrectified linear unit, 71, 158\\nrecurrent neural network, 158\\nregression, 18\\nReinforcement Learning, 134, 141\\nReinforcement Learning from Human Feedback,\\n141\\nReLU, seerectified linear unit\\nresidual\\nblock, 104\\nconnection, 84, 103\\nnetwork, 47, 84, 103\\nResNet-50, 103\\nRetrieval-Augmented Generation, 149\\nreturn, 134\\n182', metadata={'source': 'rag/deep learning.pdf', 'page': 181}),\n",
       " Document(page_content='reversible layer, seelayer, reversible\\nRL,seeReinforcement Learning\\nRLHF, seeReinforcement Learning from Human\\nFeeback\\nRNN, seerecurrent neural network\\nscaling laws, 52\\nself-attention block, 94, 109, 111\\nself-supervised learning, 162\\nsemantic segmentation, 86, 126\\nSGD, seestochastic gradient descent\\nSingle Shot Detector, 121\\nskip connection, 84, 127, 158\\nsoftargmax, 26, 89\\nsoftmax, 26\\nspeech recognition, 129\\nSSD, seeSingle Shot Detector\\nstochastic gradient descent, 38, 45, 52\\nstride, 67, 74\\nsupervised learning, 19\\nTanh, seehyperbolic tangent\\nTask Arithmetic, 156\\ntensor, 23\\ntensor cores, 21\\nTensor Processing Unit, 21\\ntest set, 48\\ntext synthesis, 139\\ntoken, 30\\n183', metadata={'source': 'rag/deep learning.pdf', 'page': 182}),\n",
       " Document(page_content='tokenizer, 34, 129\\nTPU, seeTensor Processing Unit\\ntrainable parameter, 12, 23, 52\\ntraining, 12\\ntraining set, 12, 25, 48\\nTransformer, 47, 84, 88, 96, 108, 110, 129\\ntransformer, 154\\ntransposed convolution, 69, 126\\nunderfitting, 16\\nuniversal approximation theorem, 99\\nunsupervised learning, 19\\nVAE, seevariational, autoencoder\\nvalidation set, 48\\nvalue, 88\\nvanishing gradient, 44, 58\\nvariational\\nautoencoder, 160\\nbound, 144\\nVision Transformer, 113, 131\\nViT, seeVision Transformer\\nvocabulary, 30\\nweight, 13\\ndecay, 28\\nmatrix, 61\\nzero-shot prediction, 132\\n184', metadata={'source': 'rag/deep learning.pdf', 'page': 183}),\n",
       " Document(page_content='This book is licensed under the Creative Com-\\nmons BY-NC-SA 4.0 International License.\\nV1.2â€“May 19, 2024\\n185', metadata={'source': 'rag/deep learning.pdf', 'page': 184})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Divyanshu Shukla\\Desktop\\projects\\groq\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(documents[:30],OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1cad0e10050>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\nMachine Learning\\nDeep learn ing belongs historically to the larger\\nfield of statistical machine learn ing, as it funda-\\nmentally concerns methods that are able to learn\\nrepresentations from data. The techniques in-\\nvolved come originally from artificialneuralnet-\\nworks, and the â€œdeepâ€ qualifier highlights that\\nmodels are long compositions of mappings, now\\nknown to achieve greater performance.\\nThe modularity, versatility, and scalability of\\ndeep models have resulted in a plethora of spe-\\ncific mathematical methods and software devel-\\nopment tools, establishing deep learning as a\\ndistinct and vast technical field.\\n11'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"The techniques involved come originally from\"\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.llms.huggingface_hub import HuggingFaceHub\n",
    "load_dotenv()\n",
    "\n",
    "huggingfacehub_api_token = os.environ['HUGGINGFACE_API_KEY']\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"gpt2\", huggingfacehub_api_token=huggingfacehub_api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design chatprompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "I will tip you $1000 if the user finds the answer helpful. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain introduction\n",
    "# create stuff document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001CAD0E10050>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrievers: A retriever is an interface that returns documents given\n",
    " an unstructured query. It is more general than a vector store.\n",
    " A retriever does not need to be able to store documents, only to \n",
    " return (or retrieve) them. Vector stores can be used as the backbone\n",
    " of a retriever, but there are other types of retrievers as well. \n",
    " https://python.langchain.com/docs/modules/data_connection/retrievers/   \n",
    "\"\"\"\n",
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## retreiver chain\n",
    "\"\"\"\n",
    "Retrieval chain:This chain takes in a user inquiry, which is then\n",
    "passed to the retriever to fetch relevant documents. Those documents \n",
    "(and original inputs) are then passed to an LLM to generate a response\n",
    "https://python.langchain.com/docs/modules/chains/\n",
    "\"\"\"\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\":\"The techniques involved come originally from\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nAnswer the following question based only on the provided context. \\nThink step by step before providing a detailed answer. \\nI will tip you $1000 if the user finds the answer helpful. \\n<context>\\nChapter 1\\nMachine Learning\\nDeep learn ing belongs historically to the larger\\nfield of statistical machine learn ing, as it funda-\\nmentally concerns methods that are able to learn\\nrepresentations from data. The techniques in-\\nvolved come originally from artificialneuralnet-\\nworks, and the â€œdeepâ€ qualifier highlights that\\nmodels are long compositions of mappings, now\\nknown to achieve greater performance.\\nThe modularity, versatility, and scalability of\\ndeep models have resulted in a plethora of spe-\\ncific mathematical methods and software devel-\\nopment tools, establishing deep learning as a\\ndistinct and vast technical field.\\n11\\n\\nFranÃ§ois Fleuret is a professor of computer sci-\\nence at the University of Geneva, Switzerland.\\nThe cover illustration is a schematic of the\\nNeocognitron by Fukushima [1980], a key an-\\ncestor of deep neural networks.\\nThis ebook is formatted to fit on a phone screen.\\n\\nThe Little Book\\nof\\nDeep Learning\\nFranÃ§ois Fleuret\\n\\nForeword\\nThe current period of progress in artificial in-\\ntelligence was triggered when Krizhevsky et al.\\n[2012] demonstrated that an artificialneuralnet-\\nwork designed twenty years earlier [LeCun et al.,\\n1989] could outperform complex state-of-the-\\nart image recognition methods by a huge mar-\\ngin, simply by being a hundred times larger and\\ntrained on a dataset similarly scaled up.\\nThis breakthrough was made possible thanks\\ntoGraph icalProcessingUnits ( GPUs), highly\\nparallel consumer-grade computing devices de-\\nveloped for real-time image synthesis and repur-\\nposed for artificial neural networks.\\nSince then, under the umbrella term of â€œ deep\\nlearn ing,â€ innovations in the structures of these\\nnetworks, the strategies to train them, and ded-\\nicated hardware have allowed for an exponen-\\ntial increase in both their size and the quantity\\nof training data they take advantage of [Sevilla\\n8\\n</context>\\nQuestion: The techniques involved come originally from\\nmachine learning, and the â€œdeepâ€ qualifier highlights that\\n\\nmodels are long compositions of mappings, now known to achieve greater performance.\\n\\nThe modularity, versatility, and scalability of\\n\\ndeep models have resulted in a plethora of spe-\\ncific mathematical methods and software devel-opment tools, establishing deep learning as a\\n\\ndistinct and vast technical field.\\n\\nThe Little Book\\n\\nof\\n\\nDeep Learning\\n\\nFranÃ§ois Fle'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
